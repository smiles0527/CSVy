{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eeeaec",
   "metadata": {},
   "source": [
    "# ELO Model Training - Hyperparameter Grid Search\n",
    "\n",
    "This notebook:\n",
    "1. Loads the 648 hyperparameter configs from Ruby\n",
    "2. Trains ELO model for each config\n",
    "3. Tracks RMSE, MAE, R¬≤ for each\n",
    "4. Saves results and identifies best config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory for utils imports\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EloModel Class (self-contained for portability)\n",
    "class EloModel:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.ratings = {}\n",
    "        self.rating_history = []\n",
    "    \n",
    "    def initialize_ratings(self, teams, divisions=None):\n",
    "        \"\"\"Initialize team ratings based on division tier.\"\"\"\n",
    "        initial = self.params.get('initial_rating', 1500)\n",
    "        division_ratings = {'D1': initial + 100, 'D2': initial, 'D3': initial - 100}\n",
    "        \n",
    "        for i, team in enumerate(teams):\n",
    "            if divisions is not None and i < len(divisions):\n",
    "                div = divisions.iloc[i] if hasattr(divisions, 'iloc') else divisions[i]\n",
    "                self.ratings[team] = division_ratings.get(div, initial)\n",
    "            else:\n",
    "                self.ratings[team] = initial\n",
    "    \n",
    "    def calculate_expected_score(self, team_elo, opponent_elo):\n",
    "        return 1 / (1 + 10 ** ((opponent_elo - team_elo) / 400))\n",
    "    \n",
    "    def calculate_mov_multiplier(self, goal_diff):\n",
    "        mov = self.params.get('mov_multiplier', 0)\n",
    "        if mov == 0:\n",
    "            return 1.0\n",
    "        if self.params.get('mov_method', 'logarithmic') == 'linear':\n",
    "            return 1 + (abs(goal_diff) * mov)\n",
    "        return 1 + (np.log(abs(goal_diff) + 1) * mov)\n",
    "    \n",
    "    def get_actual_score(self, outcome):\n",
    "        if outcome in ['RW', 'W', 1]:\n",
    "            return 1.0\n",
    "        elif outcome == 'OTW':\n",
    "            return self.params.get('ot_win_multiplier', 0.75)\n",
    "        elif outcome == 'OTL':\n",
    "            return 1 - self.params.get('ot_win_multiplier', 0.75)\n",
    "        return 0.0\n",
    "    \n",
    "    def adjust_for_context(self, team_elo, is_home, rest_time, travel_dist, injuries):\n",
    "        adjusted = team_elo\n",
    "        if is_home:\n",
    "            adjusted += self.params.get('home_advantage', 0)\n",
    "        if rest_time <= 1:\n",
    "            adjusted -= self.params.get('b2b_penalty', 0)\n",
    "        if not is_home and travel_dist > 0:\n",
    "            adjusted -= (travel_dist / 1000) * 15\n",
    "        adjusted -= injuries * 25\n",
    "        return adjusted\n",
    "    \n",
    "    def update_ratings(self, game):\n",
    "        home_team = game['home_team']\n",
    "        away_team = game['away_team']\n",
    "        \n",
    "        home_elo = self.ratings.get(home_team, 1500)\n",
    "        away_elo = self.ratings.get(away_team, 1500)\n",
    "        \n",
    "        # Get context values with defaults\n",
    "        home_rest = game.get('home_rest', 2)\n",
    "        away_rest = game.get('away_rest', 2)\n",
    "        away_travel = game.get('away_travel_dist', game.get('travel_distance', 0))\n",
    "        home_injuries = game.get('home_injuries', game.get('injuries', 0))\n",
    "        away_injuries = game.get('away_injuries', game.get('injuries', 0))\n",
    "        \n",
    "        home_adj = self.adjust_for_context(home_elo, True, home_rest, 0, home_injuries)\n",
    "        away_adj = self.adjust_for_context(away_elo, False, away_rest, away_travel, away_injuries)\n",
    "        \n",
    "        rest_diff = home_rest - away_rest\n",
    "        home_adj += rest_diff * self.params.get('rest_advantage_per_day', 0)\n",
    "        \n",
    "        home_expected = self.calculate_expected_score(home_adj, away_adj)\n",
    "        \n",
    "        # Handle different outcome column names\n",
    "        if 'home_outcome' in game:\n",
    "            home_actual = self.get_actual_score(game['home_outcome'])\n",
    "        elif 'home_win' in game:\n",
    "            home_actual = 1.0 if game['home_win'] else 0.0\n",
    "        else:\n",
    "            home_actual = 1.0 if game['home_goals'] > game['away_goals'] else 0.0\n",
    "        \n",
    "        goal_diff = game['home_goals'] - game['away_goals']\n",
    "        mov_mult = self.calculate_mov_multiplier(goal_diff)\n",
    "        \n",
    "        k = self.params.get('k_factor', 32) * mov_mult\n",
    "        self.ratings[home_team] = home_elo + k * (home_actual - home_expected)\n",
    "        self.ratings[away_team] = away_elo + k * ((1 - home_actual) - (1 - home_expected))\n",
    "        \n",
    "        self.rating_history.append({\n",
    "            'home_team': home_team, 'away_team': away_team,\n",
    "            'home_rating': self.ratings[home_team],\n",
    "            'away_rating': self.ratings[away_team]\n",
    "        })\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        home_team = game['home_team']\n",
    "        away_team = game['away_team']\n",
    "        \n",
    "        home_elo = self.ratings.get(home_team, 1500)\n",
    "        away_elo = self.ratings.get(away_team, 1500)\n",
    "        \n",
    "        home_rest = game.get('home_rest', 2)\n",
    "        away_rest = game.get('away_rest', 2)\n",
    "        away_travel = game.get('away_travel_dist', game.get('travel_distance', 0))\n",
    "        home_injuries = game.get('home_injuries', game.get('injuries', 0))\n",
    "        away_injuries = game.get('away_injuries', game.get('injuries', 0))\n",
    "        \n",
    "        home_adj = self.adjust_for_context(home_elo, True, home_rest, 0, home_injuries)\n",
    "        away_adj = self.adjust_for_context(away_elo, False, away_rest, away_travel, away_injuries)\n",
    "        \n",
    "        rest_diff = home_rest - away_rest\n",
    "        home_adj += rest_diff * self.params.get('rest_advantage_per_day', 0)\n",
    "        \n",
    "        home_win_prob = self.calculate_expected_score(home_adj, away_adj)\n",
    "        expected_diff = (home_win_prob - 0.5) * 12\n",
    "        \n",
    "        home_goals = 3.0 + (expected_diff / 2)\n",
    "        away_goals = 3.0 - (expected_diff / 2)\n",
    "        return home_goals, away_goals\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        teams = pd.concat([games_df['home_team'], games_df['away_team']]).unique()\n",
    "        if 'division' in games_df.columns:\n",
    "            divisions = games_df.groupby('home_team')['division'].first()\n",
    "            self.initialize_ratings(teams, divisions)\n",
    "        else:\n",
    "            self.initialize_ratings(teams)\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            self.update_ratings(game)\n",
    "    \n",
    "    def evaluate(self, games_df):\n",
    "        predictions, actuals = [], []\n",
    "        for _, game in games_df.iterrows():\n",
    "            home_pred, _ = self.predict_goals(game)\n",
    "            predictions.append(home_pred)\n",
    "            actuals.append(game['home_goals'])\n",
    "        \n",
    "        rmse = mean_squared_error(actuals, predictions, squared=False)\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        r2 = r2_score(actuals, predictions) if len(set(actuals)) > 1 else 0.0\n",
    "        return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "print(\"EloModel class loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b34c4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameter grid (generated by Ruby)\n",
    "# Try multiple possible paths\n",
    "config_paths = [\n",
    "    '../data/model3_elo_grid.csv',           # DeepNote: uploaded to python/data/\n",
    "    '../../output/hyperparams/model3_elo_grid.csv',  # Local: from training/ folder\n",
    "    'model3_elo_grid.csv',                    # Current directory\n",
    "]\n",
    "\n",
    "configs_df = None\n",
    "for path in config_paths:\n",
    "    if os.path.exists(path):\n",
    "        configs_df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Loaded configs from: {path}\")\n",
    "        break\n",
    "\n",
    "if configs_df is None:\n",
    "    print(\"‚ö†Ô∏è No config file found. Generating default grid...\")\n",
    "    # Generate default grid if no file found\n",
    "    from itertools import product\n",
    "    k_factors = [20, 32, 40]\n",
    "    home_advantages = [50, 100, 150]\n",
    "    mov_multipliers = [0, 1.0, 1.5]\n",
    "    rest_advantages = [0, 10]\n",
    "    b2b_penalties = [0, 50]\n",
    "    \n",
    "    configs = []\n",
    "    for i, (k, h, m, r, b) in enumerate(product(k_factors, home_advantages, mov_multipliers, rest_advantages, b2b_penalties)):\n",
    "        configs.append({\n",
    "            'experiment_id': f'elo_{i+1:03d}',\n",
    "            'k_factor': k, 'home_advantage': h, 'mov_multiplier': m,\n",
    "            'mov_method': 'logarithmic', 'rest_advantage_per_day': r, 'b2b_penalty': b,\n",
    "            'initial_rating': 1500, 'ot_win_multiplier': 0.75\n",
    "        })\n",
    "    configs_df = pd.DataFrame(configs)\n",
    "    print(f\"Generated {len(configs_df)} default configurations\")\n",
    "\n",
    "print(f\"\\nüìä Total configurations: {len(configs_df)}\")\n",
    "configs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hockey game data\n",
    "# Try multiple possible paths\n",
    "data_paths = [\n",
    "    '../data/hockey_data.csv',      # DeepNote: uploaded to python/data/\n",
    "    '../../data/hockey_data.csv',   # Local: from training/ folder\n",
    "    'hockey_data.csv',              # Current directory\n",
    "]\n",
    "\n",
    "games_df = None\n",
    "for path in data_paths:\n",
    "    if os.path.exists(path):\n",
    "        games_df = pd.read_csv(path)\n",
    "        print(f\"‚úÖ Loaded games from: {path}\")\n",
    "        break\n",
    "\n",
    "if games_df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"‚ùå Hockey data not found! Upload hockey_data.csv to python/data/ folder.\\n\"\n",
    "        \"Expected columns: home_team, away_team, home_goals, away_goals, game_date, division\"\n",
    "    )\n",
    "\n",
    "# CRITICAL: Sort by game date (ELO requires chronological order)\n",
    "date_col = None\n",
    "for col in ['game_date', 'date', 'Date', 'game_datetime']:\n",
    "    if col in games_df.columns:\n",
    "        date_col = col\n",
    "        break\n",
    "\n",
    "if date_col:\n",
    "    games_df = games_df.sort_values(date_col).reset_index(drop=True)\n",
    "    print(f\"‚úÖ Sorted by: {date_col}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No date column found - assuming data is already chronological\")\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(games_df)} games\")\n",
    "print(f\"üìã Columns: {list(games_df.columns)}\")\n",
    "games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c33f81",
   "metadata": {},
   "source": [
    "## Time Series Split for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83184f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80/20 train/test split (chronological)\n",
    "split_idx = int(len(games_df) * 0.8)\n",
    "train_df = games_df[:split_idx]\n",
    "test_df = games_df[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(train_df)} games\")\n",
    "print(f\"Test: {len(test_df)} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bcd81",
   "metadata": {},
   "source": [
    "## Grid Search Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Loop through all configs (this will take a while - 648 iterations)\n",
    "for idx, row in tqdm(configs_df.iterrows(), total=len(configs_df), desc=\"Training ELO models\"):\n",
    "    try:\n",
    "        # Convert row to parameters dict\n",
    "        params = row.to_dict()\n",
    "        experiment_id = params.pop('experiment_id')\n",
    "        \n",
    "        # Initialize model\n",
    "        model = EloModel(params)\n",
    "        \n",
    "        # Train on training set\n",
    "        model.fit(train_df)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        metrics = model.evaluate(test_df)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'experiment_id': experiment_id,\n",
    "            'rmse': metrics['rmse'],\n",
    "            'mae': metrics['mae'],\n",
    "            'r2': metrics['r2'],\n",
    "            'status': 'completed',\n",
    "            **params\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment {experiment_id}: {e}\")\n",
    "        results.append({\n",
    "            'experiment_id': experiment_id,\n",
    "            'rmse': np.nan,\n",
    "            'mae': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'status': 'failed',\n",
    "            **params\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted {len(results_df)} experiments\")\n",
    "print(f\"Failed: {results_df['status'].value_counts().get('failed', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5f6ff",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results with metrics\n",
    "output_path = '../data/model3_elo_results.csv'  # Save to python/data/ for portability\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Saved results to: {output_path}\")\n",
    "\n",
    "# Display summary\n",
    "completed = results_df[results_df['status'] == 'completed']\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Completed: {len(completed)}\")\n",
    "print(f\"   Best RMSE: {completed['rmse'].min():.3f}\")\n",
    "print(f\"   Mean RMSE: {completed['rmse'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8d285",
   "metadata": {},
   "source": [
    "## Analyze Best Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best configs by RMSE\n",
    "best_configs = results_df.nsmallest(10, 'rmse')\n",
    "print(\"\\nTop 10 Configurations by RMSE:\")\n",
    "print(best_configs[['experiment_id', 'rmse', 'mae', 'r2', 'k_factor', 'home_advantage', \n",
    "                     'mov_multiplier', 'rest_advantage_per_day', 'b2b_penalty']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56158d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best overall config\n",
    "best = results_df.loc[results_df['rmse'].idxmin()]\n",
    "print(f\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "print(f\"   Experiment ID: {best['experiment_id']}\")\n",
    "print(f\"   RMSE: {best['rmse']:.3f}\")\n",
    "print(f\"   MAE: {best['mae']:.3f}\")\n",
    "print(f\"   R¬≤: {best['r2']:.3f}\")\n",
    "print(f\"\\n   Parameters:\")\n",
    "print(f\"   - k_factor: {best['k_factor']}\")\n",
    "print(f\"   - home_advantage: {best['home_advantage']}\")\n",
    "print(f\"   - mov_multiplier: {best['mov_multiplier']}\")\n",
    "print(f\"   - mov_method: {best['mov_method']}\")\n",
    "print(f\"   - rest_advantage_per_day: {best['rest_advantage_per_day']}\")\n",
    "print(f\"   - b2b_penalty: {best['b2b_penalty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98ef64",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of RMSE scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(results_df['rmse'].dropna(), bins=50, edgecolor='black')\n",
    "axes[0].axvline(best['rmse'], color='red', linestyle='--', linewidth=2, label=f\"Best: {best['rmse']:.3f}\")\n",
    "axes[0].set_xlabel('RMSE')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of RMSE Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(results_df['r2'].dropna(), bins=50, edgecolor='black', color='green', alpha=0.7)\n",
    "axes[1].axvline(best['r2'], color='red', linestyle='--', linewidth=2, label=f\"Best: {best['r2']:.3f}\")\n",
    "axes[1].set_xlabel('R¬≤ Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of R¬≤ Scores')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter importance heatmap\n",
    "param_cols = [col for col in ['k_factor', 'home_advantage', 'mov_multiplier', 'rest_advantage_per_day', 'b2b_penalty'] \n",
    "              if col in results_df.columns]\n",
    "\n",
    "if param_cols:\n",
    "    corr = results_df[param_cols + ['rmse']].corr()['rmse'].drop('rmse')\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    corr.abs().sort_values().plot(kind='barh', color='steelblue')\n",
    "    plt.xlabel('Correlation with RMSE (absolute)')\n",
    "    plt.title('Hyperparameter Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No parameter columns found for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a7dca",
   "metadata": {},
   "source": [
    "## Train Final Model with Best Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full dataset with best parameters\n",
    "best_params = best.drop(['experiment_id', 'rmse', 'mae', 'r2', 'status']).to_dict()\n",
    "final_model = EloModel(best_params)\n",
    "final_model.fit(games_df)\n",
    "\n",
    "print(\"Final model trained on full dataset\")\n",
    "print(f\"Final team ratings:\")\n",
    "sorted_ratings = sorted(final_model.ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "for team, rating in sorted_ratings[:10]:\n",
    "    print(f\"  {team}: {rating:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3cc7c0",
   "metadata": {},
   "source": [
    "## Generate Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f080f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test/submission games\n",
    "def generate_predictions(model, test_games_df, output_path=None):\n",
    "    \"\"\"Generate goal predictions for a set of games.\"\"\"\n",
    "    predictions = []\n",
    "    for _, game in test_games_df.iterrows():\n",
    "        home_pred, away_pred = model.predict_goals(game)\n",
    "        predictions.append({\n",
    "            'game_id': game.get('game_id', _),\n",
    "            'home_team': game['home_team'],\n",
    "            'away_team': game['away_team'],\n",
    "            'home_goals_pred': round(home_pred, 2),\n",
    "            'away_goals_pred': round(away_pred, 2),\n",
    "        })\n",
    "    \n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    if output_path:\n",
    "        predictions_df.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ Predictions saved to: {output_path}\")\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Example: Generate predictions on test set\n",
    "test_predictions = generate_predictions(final_model, test_df)\n",
    "print(f\"\\nüìä Test Set Predictions ({len(test_predictions)} games):\")\n",
    "test_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516aeb9c",
   "metadata": {},
   "source": [
    "## Save Best Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save best configuration as JSON for reuse\n",
    "best_config = {\n",
    "    'model': 'ELO',\n",
    "    'experiment_id': best['experiment_id'],\n",
    "    'metrics': {\n",
    "        'rmse': float(best['rmse']),\n",
    "        'mae': float(best['mae']),\n",
    "        'r2': float(best['r2'])\n",
    "    },\n",
    "    'params': {k: (float(v) if isinstance(v, (int, float, np.floating, np.integer)) else v) \n",
    "               for k, v in best_params.items()},\n",
    "    'team_ratings': {k: round(v, 1) for k, v in final_model.ratings.items()}\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "config_path = '../data/best_elo_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Best configuration saved to: {config_path}\")\n",
    "print(f\"\\nüèÜ FINAL MODEL SUMMARY:\")\n",
    "print(f\"   RMSE: {best['rmse']:.3f}\")\n",
    "print(f\"   MAE: {best['mae']:.3f}\")  \n",
    "print(f\"   R¬≤: {best['r2']:.3f}\")\n",
    "print(f\"\\n   Best Parameters:\")\n",
    "for key, val in best_params.items():\n",
    "    print(f\"   - {key}: {val}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

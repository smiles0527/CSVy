{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Elo Training\n",
        "\n",
        "This notebook:\n",
        "1. Loads WHL 2025 CSV data and aggregates shifts → games (sum for goals)\n",
        "2. Runs 70/30 block cross-validation (3 folds); grid search on first fold only\n",
        "3. Reports mean +/- std of combined RMSE and win accuracy across runs\n",
        "4. Generates Round 1 predictions from `WHSDSC_Rnd1_matchups.xlsx`\n",
        "5. Saves outputs to `output/predictions/baseline_elo/` and `output/models/baseline_elo/`\n",
        "\n",
        "**Baseline Elo** = minimal Elo (chess-style) for simple benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "import json, os, sys, pathlib\n",
        "\n",
        "# ---------- resolve python/ directory & set CWD ----------\n",
        "_cwd = pathlib.Path(os.path.abspath('')).resolve()\n",
        "if (_cwd / 'python').is_dir():\n",
        "    _python_dir = _cwd / 'python'\n",
        "elif _cwd.name == 'baseline_elo' and (_cwd.parent.parent / 'data').is_dir():\n",
        "    _python_dir = _cwd.parent.parent\n",
        "elif _cwd.name == 'training' and (_cwd.parent / 'data').is_dir():\n",
        "    _python_dir = _cwd.parent\n",
        "elif (_cwd / 'data').is_dir():\n",
        "    _python_dir = _cwd\n",
        "else:\n",
        "    raise RuntimeError(f'Cannot locate python/ directory from {_cwd}')\n",
        "\n",
        "os.chdir(_python_dir)\n",
        "sys.path.insert(0, str(_python_dir))\n",
        "\n",
        "from utils.baseline_elo import BaselineEloModel\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "print(f'CWD: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load CSV Data & Aggregate to Games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Load raw shift-level CSV and aggregate to game level ──\n",
        "raw = pd.read_csv('data/whl_2025.csv')\n",
        "print(f'Raw: {len(raw)} shift rows')\n",
        "\n",
        "games_df = raw.groupby('game_id').agg(\n",
        "    home_team  = ('home_team', 'first'),\n",
        "    away_team  = ('away_team', 'first'),\n",
        "    home_goals = ('home_goals', 'sum'),\n",
        "    away_goals = ('away_goals', 'sum'),\n",
        "    went_ot    = ('went_ot', 'max'),\n",
        ").reset_index()\n",
        "\n",
        "# Sort by game_id (chronological)\n",
        "extracted = games_df['game_id'].astype(str).str.extract(r'(\\\\d+)')\n",
        "games_df['game_num'] = pd.to_numeric(extracted[0], errors='coerce').fillna(0).astype(int)\n",
        "games_df = games_df.sort_values('game_num').reset_index(drop=True)\n",
        "\n",
        "print(f'Games: {len(games_df)}')\n",
        "print(f'Teams: {games_df[\"home_team\"].nunique()}')\n",
        "print(f'Avg home goals: {games_df[\"home_goals\"].mean():.2f}, Avg away goals: {games_df[\"away_goals\"].mean():.2f}')\n",
        "print(f'Home win rate: {(games_df[\"home_goals\"] > games_df[\"away_goals\"]).mean():.1%}')\n",
        "games_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 70/30 Block CV & Grid Search (K-factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config from YAML (proj_root has config/)\n",
        "proj_root = _python_dir if (_python_dir / 'config').is_dir() else _python_dir.parent\n",
        "config_path = proj_root / 'config' / 'hyperparams' / 'model_baseline_elo.yaml'\n",
        "if config_path.exists():\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    print(f'Config: {config[\"model_name\"]} - {config[\"description\"]}')\n",
        "else:\n",
        "    config = None\n",
        "    print('Config not found, using defaults')\n",
        "\n",
        "# 70/30 block cross-validation\n",
        "val_cfg = config.get('validation', {}) if config else {}\n",
        "TRAIN_RATIO = val_cfg.get('train_ratio', 0.7)\n",
        "N_RUNS = val_cfg.get('n_runs', 3)\n",
        "n_folds = N_RUNS\n",
        "n = len(games_df)\n",
        "test_size = int(n * (1 - TRAIN_RATIO))  # 30% test\n",
        "\n",
        "def get_fold_splits(games_df, fold):\n",
        "    \"\"\"Block holdout: train ~70%, test ~30%. 3 folds = 3 different block positions.\"\"\"\n",
        "    if fold == 0:\n",
        "        train_df = games_df.iloc[: n - test_size].copy()\n",
        "        test_df = games_df.iloc[n - test_size :].copy()\n",
        "    elif fold == 1:\n",
        "        half = (n - test_size) // 2\n",
        "        train_df = pd.concat([games_df.iloc[:half], games_df.iloc[half + test_size :]], ignore_index=True)\n",
        "        test_df = games_df.iloc[half : half + test_size].copy()\n",
        "    else:\n",
        "        train_df = games_df.iloc[test_size:].copy()\n",
        "        test_df = games_df.iloc[:test_size].copy()\n",
        "    return train_df, test_df\n",
        "\n",
        "# First fold for grid search\n",
        "train_df, test_df = get_fold_splits(games_df, 0)\n",
        "print(f'Split: {int(100*TRAIN_RATIO)}% train / {int(100*(1-TRAIN_RATIO))}% test, {n_folds} folds, ~{len(train_df)} train / ~{len(test_df)} test per fold')\n",
        "print(f'Fold 1: Train {len(train_df)} games  |  Test {len(test_df)} games')\n",
        "\n",
        "# Param grid from config (supports list or {min, max, step} range)\n",
        "def _expand_param(val, default):\n",
        "    if val is None: return default\n",
        "    if isinstance(val, dict) and all(k in val for k in ('min','max','step')):\n",
        "        return list(range(int(val['min']), int(val['max'])+1, int(val['step'])))\n",
        "    return val if isinstance(val, list) else default\n",
        "\n",
        "hp = config.get('hyperparameters', {}) if config else {}\n",
        "param_grid = dict(\n",
        "    k_factor       = _expand_param(hp.get('k_factor'), list(range(5, 101, 5))),\n",
        "    initial_rating = _expand_param(hp.get('initial_rating'), [1200]),\n",
        ")\n",
        "formula_constants = {k: hp.get(k, {'elo_scale': 400, 'league_avg_goals': 3.0, 'goal_diff_half_range': 6.0}[k]) for k in ['elo_scale', 'league_avg_goals', 'goal_diff_half_range']}\n",
        "keys = list(param_grid.keys())\n",
        "combos = list(product(*[param_grid[k] for k in keys]))\n",
        "print(f'Grid: {len(combos)} configurations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Grid Search (first fold only) with Brier & Log loss ──\n",
        "results = []\n",
        "\n",
        "for i, vals in enumerate(combos):\n",
        "    params = dict(zip(keys, vals))\n",
        "    params.update(formula_constants)\n",
        "    label = f\"BaselineElo(k={params['k_factor']},init={params['initial_rating']})\"\n",
        "    try:\n",
        "        model = BaselineEloModel(params)\n",
        "        model.fit(train_df)\n",
        "        metrics = model.evaluate(test_df)\n",
        "        brier_loss, log_loss = BaselineEloModel.compute_brier_logloss(model, test_df)\n",
        "        results.append({'config': label, **params, **metrics, 'brier_loss': brier_loss, 'log_loss': log_loss})\n",
        "    except Exception as e:\n",
        "        print(f'  FAILED {label}: {e}')\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values('combined_rmse')\n",
        "best_row = results_df.iloc[0]\n",
        "best_params = {\n",
        "    'k_factor': int(best_row['k_factor']),\n",
        "    'initial_rating': int(best_row['initial_rating']),\n",
        "    **formula_constants,\n",
        "}\n",
        "k_metrics_df = results_df[['k_factor', 'win_accuracy', 'brier_loss', 'log_loss']].rename(\n",
        "    columns={'k_factor': 'k', 'win_accuracy': 'accuracy'}\n",
        ")\n",
        "print(f'\\n[OK] {len(results_df)} configs evaluated on fold 1')\n",
        "print(f'Best: k={best_params[\"k_factor\"]}, init={best_params[\"initial_rating\"]} (combined_rmse={best_row[\"combined_rmse\"]:.4f})')\n",
        "results_df.head(10)[['config','home_rmse','away_rmse','combined_rmse','win_accuracy','brier_loss','log_loss']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Multi-run evaluation (all folds with best params) ──\n",
        "per_run_metrics = []\n",
        "\n",
        "for fold in range(n_folds):\n",
        "    train_f, test_f = get_fold_splits(games_df, fold)\n",
        "    model = BaselineEloModel(best_params)\n",
        "    model.fit(train_f)\n",
        "    metrics = model.evaluate(test_f)\n",
        "    per_run_metrics.append({\n",
        "        'run': fold + 1,\n",
        "        'combined_rmse': metrics['combined_rmse'],\n",
        "        'win_accuracy': metrics['win_accuracy'],\n",
        "    })\n",
        "    print(f'Run {fold + 1}: combined_rmse={metrics[\"combined_rmse\"]:.4f}, win_accuracy={metrics[\"win_accuracy\"]:.1%}')\n",
        "\n",
        "# Aggregate\n",
        "rmse_vals = [m['combined_rmse'] for m in per_run_metrics]\n",
        "acc_vals = [m['win_accuracy'] for m in per_run_metrics]\n",
        "rmse_mean, rmse_std = np.mean(rmse_vals), np.std(rmse_vals)\n",
        "acc_mean, acc_std = np.mean(acc_vals), np.std(acc_vals)\n",
        "\n",
        "print(f'\\nSummary (mean +/- std over {n_folds} runs):')\n",
        "print(f'  Combined RMSE:  {rmse_mean:.4f} +/- {rmse_std:.4f}')\n",
        "print(f'  Win Accuracy:   {acc_mean:.1%} +/- {acc_std:.1%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Final Model & Rankings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Train on ALL data with best params ──\n",
        "final_model = BaselineEloModel(best_params)\n",
        "final_model.fit(games_df)\n",
        "\n",
        "# Multi-run metrics for reporting (from CV above)\n",
        "final_metrics = {\n",
        "    'combined_rmse': rmse_mean, 'combined_rmse_std': rmse_std,\n",
        "    'win_accuracy': acc_mean, 'win_accuracy_std': acc_std,\n",
        "}\n",
        "\n",
        "print('Final Baseline Elo model trained on all', len(games_df), 'games')\n",
        "print(f'\\nCV metrics (mean +/- std over {n_folds} runs):')\n",
        "print(f'  Combined RMSE:  {rmse_mean:.4f} +/- {rmse_std:.4f}')\n",
        "print(f'  Win Accuracy:   {acc_mean:.1%} +/- {acc_std:.1%}')\n",
        "\n",
        "print(f'\\nTeam Rankings (top 10):')\n",
        "for rank, (team, rating) in enumerate(final_model.get_rankings(10), 1):\n",
        "    print(f'  {rank:2d}. {team:20s}  {rating:.1f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Rating distribution visualization ──\n",
        "rankings = final_model.get_rankings()\n",
        "teams_r, ratings_r = zip(*rankings)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "initial_ref = best_params.get('initial_rating', 1200)\n",
        "colors = ['#2ecc71' if r > initial_ref else '#e74c3c' if r < initial_ref else '#95a5a6' for r in ratings_r]\n",
        "ax.barh(range(len(teams_r)), ratings_r, color=colors, edgecolor='black', linewidth=0.3)\n",
        "ax.set_yticks(range(len(teams_r)))\n",
        "ax.set_yticklabels(teams_r, fontsize=9)\n",
        "ax.axvline(initial_ref, color='gray', ls='--', lw=1, label=f'Initial ({initial_ref})')\n",
        "ax.set_xlabel('Elo Rating')\n",
        "ax.set_title('Baseline Elo Ratings — All Teams')\n",
        "ax.legend()\n",
        "ax.invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Round 1 Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Load Round 1 matchups and predict ──\n",
        "matchups = pd.read_excel('data/WHSDSC_Rnd1_matchups.xlsx')\n",
        "print(f'{len(matchups)} matchups loaded')\n",
        "\n",
        "home_col = [c for c in matchups.columns if 'home' in c.lower()][0]\n",
        "away_col = [c for c in matchups.columns if 'away' in c.lower()][0]\n",
        "\n",
        "preds = []\n",
        "for i, row in matchups.iterrows():\n",
        "    game = {'home_team': row[home_col], 'away_team': row[away_col]}\n",
        "    h_goals, a_goals = final_model.predict_goals(game)\n",
        "    winner, conf = final_model.predict_winner(game)\n",
        "    preds.append({\n",
        "        'game': i + 1,\n",
        "        'home_team': row[home_col],\n",
        "        'away_team': row[away_col],\n",
        "        'pred_home_goals': round(h_goals, 2),\n",
        "        'pred_away_goals': round(a_goals, 2),\n",
        "        'predicted_winner': winner,\n",
        "        'confidence': round(conf, 4),\n",
        "    })\n",
        "\n",
        "pred_df = pd.DataFrame(preds)\n",
        "print(f'\\nRound 1 Baseline Elo Predictions:')\n",
        "print(pred_df.to_string(index=False))\n",
        "print(f'\\nAvg confidence: {pred_df[\"confidence\"].mean():.1%}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_cfg = config.get('output', {}) if config else {}\n",
        "out_dir = pathlib.Path(out_cfg.get('out_dir', 'output/predictions/baseline_elo/goals'))\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "os.makedirs('output/models/baseline_elo', exist_ok=True)\n",
        "\n",
        "# 1. Grid search comparison CSV\n",
        "comp_csv = out_cfg.get('comparison_csv', str(out_dir / 'comparison.csv'))\n",
        "results_df.to_csv(comp_csv, index=False)\n",
        "print(f'[OK] {comp_csv}')\n",
        "\n",
        "# 2. K metrics CSV (k, accuracy, brier_loss, log_loss)\n",
        "k_csv = out_cfg.get('k_metrics_csv', str(out_dir / 'k_metrics.csv'))\n",
        "k_metrics_df.to_csv(k_csv, index=False)\n",
        "print(f'[OK] {k_csv}')\n",
        "\n",
        "# 3. Round 1 predictions CSV\n",
        "r1_csv = out_cfg.get('round1_csv', str(out_dir / 'round1_predictions.csv'))\n",
        "pred_df.to_csv(r1_csv, index=False)\n",
        "print(f'[OK] {r1_csv}')\n",
        "\n",
        "# 4. Pipeline summary JSON (with multi-run metrics)\n",
        "summary = {\n",
        "    'model': 'BaselineElo',\n",
        "    'train_ratio': TRAIN_RATIO,\n",
        "    'n_runs': n_folds,\n",
        "    'multi_run_metrics': {\n",
        "        'combined_rmse_mean': round(rmse_mean, 6),\n",
        "        'combined_rmse_std': round(rmse_std, 6),\n",
        "        'win_accuracy_mean': round(acc_mean, 6),\n",
        "        'win_accuracy_std': round(acc_std, 6),\n",
        "    },\n",
        "    'per_run_metrics': [\n",
        "        {'run': m['run'], 'combined_rmse': round(m['combined_rmse'], 6), 'win_accuracy': round(m['win_accuracy'], 6)}\n",
        "        for m in per_run_metrics\n",
        "    ],\n",
        "    'best_params': best_params,\n",
        "    'team_rankings': {team: round(r, 1) for team, r in final_model.get_rankings()},\n",
        "    'predictions': preds,\n",
        "}\n",
        "summary_json = out_cfg.get('summary_json', str(out_dir / 'pipeline_summary.json'))\n",
        "with open(summary_json, 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(f'[OK] {summary_json}')\n",
        "\n",
        "print(f'\\nSUMMARY: Combined RMSE {rmse_mean:.4f} +/- {rmse_std:.4f}, Win Acc {acc_mean:.1%} +/- {acc_std:.1%}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

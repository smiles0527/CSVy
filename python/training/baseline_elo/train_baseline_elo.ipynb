{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Elo Training\n",
    "\n",
    "This notebook:\n",
    "1. Loads WHL 2025 CSV data and aggregates shifts â†’ games (sum for goals)\n",
    "2. Runs grid search over K-factor (classic Elo onlyâ€”no home advantage, MOV, etc.)\n",
    "3. Evaluates with combined RMSE, win accuracyâ€”comparable to other models\n",
    "4. Generates Round 1 predictions from `WHSDSC_Rnd1_matchups.xlsx`\n",
    "5. Saves outputs to `output/predictions/baseline_elo/` and `output/models/baseline_elo/`\n",
    "\n",
    "**Baseline Elo** = minimal Elo (chess-style) for simple benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import json, os, sys, pathlib\n",
    "\n",
    "# ---------- resolve python/ directory & set CWD ----------\n",
    "_cwd = pathlib.Path(os.path.abspath('')).resolve()\n",
    "if (_cwd / 'python').is_dir():\n",
    "    _python_dir = _cwd / 'python'\n",
    "elif _cwd.name == 'baseline_elo' and (_cwd.parent.parent / 'data').is_dir():\n",
    "    _python_dir = _cwd.parent.parent\n",
    "elif _cwd.name == 'training' and (_cwd.parent / 'data').is_dir():\n",
    "    _python_dir = _cwd.parent\n",
    "elif (_cwd / 'data').is_dir():\n",
    "    _python_dir = _cwd\n",
    "else:\n",
    "    raise RuntimeError(f'Cannot locate python/ directory from {_cwd}')\n",
    "\n",
    "os.chdir(_python_dir)\n",
    "sys.path.insert(0, str(_python_dir))\n",
    "\n",
    "from utils.baseline_elo import BaselineEloModel\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print(f'CWD: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV Data & Aggregate to Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load raw shift-level CSV and aggregate to game level â”€â”€\n",
    "raw = pd.read_csv('data/whl_2025.csv')\n",
    "print(f'Raw: {len(raw)} shift rows')\n",
    "\n",
    "games_df = raw.groupby('game_id').agg(\n",
    "    home_team  = ('home_team', 'first'),\n",
    "    away_team  = ('away_team', 'first'),\n",
    "    home_goals = ('home_goals', 'sum'),\n",
    "    away_goals = ('away_goals', 'sum'),\n",
    "    went_ot    = ('went_ot', 'max'),\n",
    ").reset_index()\n",
    "\n",
    "# Sort by game_id (chronological)\n",
    "games_df['game_num'] = games_df['game_id'].astype(str).str.extract(r'(\\\\d+)').astype(int)\n",
    "games_df = games_df.sort_values('game_num').reset_index(drop=True)\n",
    "\n",
    "print(f'Games: {len(games_df)}')\n",
    "print(f'Teams: {games_df[\"home_team\"].nunique()}')\n",
    "print(f'Avg home goals: {games_df[\"home_goals\"].mean():.2f}, Avg away goals: {games_df[\"away_goals\"].mean():.2f}')\n",
    "print(f'Home win rate: {(games_df[\"home_goals\"] > games_df[\"away_goals\"]).mean():.1%}')\n",
    "games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split & Grid Search (K-factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 chronological split (same as Elo/baseline)\n",
    "split_idx = int(len(games_df) * 0.8)\n",
    "train_df = games_df.iloc[:split_idx].copy()\n",
    "test_df  = games_df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f'Train: {len(train_df)} games  |  Test: {len(test_df)} games')\n",
    "\n",
    "# Baseline Elo has only k_factor and initial_rating\n",
    "param_grid = dict(\n",
    "    k_factor       = [16, 24, 32, 40, 48],\n",
    "    initial_rating = [1200, 1500],\n",
    ")\n",
    "keys = list(param_grid.keys())\n",
    "combos = list(product(*[param_grid[k] for k in keys]))\n",
    "print(f'Grid: {len(combos)} configurations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Grid Search â”€â”€\n",
    "results = []\n",
    "\n",
    "for i, vals in enumerate(combos):\n",
    "    params = dict(zip(keys, vals))\n",
    "    label = f\"BaselineElo(k={params['k_factor']},init={params['initial_rating']})\"\n",
    "    try:\n",
    "        model = BaselineEloModel(params)\n",
    "        model.fit(train_df)\n",
    "        metrics = model.evaluate(test_df)\n",
    "        results.append({'config': label, **params, **metrics})\n",
    "    except Exception as e:\n",
    "        print(f'  FAILED {label}: {e}')\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('combined_rmse')\n",
    "print(f'\\nâœ… {len(results_df)} configs evaluated')\n",
    "print(f'Best combined RMSE: {results_df[\"combined_rmse\"].min():.4f}')\n",
    "print(f'Win accuracy range: {results_df[\"win_accuracy\"].min():.1%} â€“ {results_df[\"win_accuracy\"].max():.1%}')\n",
    "results_df.head(10)[['config','home_rmse','away_rmse','combined_rmse','win_accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Final Model & Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train on ALL data with best params â”€â”€\n",
    "best_row = results_df.iloc[0]\n",
    "best_params = {\n",
    "    'k_factor': int(best_row['k_factor']),\n",
    "    'initial_rating': int(best_row['initial_rating']),\n",
    "}\n",
    "\n",
    "final_model = BaselineEloModel(best_params)\n",
    "final_model.fit(games_df)\n",
    "\n",
    "# Evaluate on test split for reporting\n",
    "eval_model = BaselineEloModel(best_params)\n",
    "eval_model.fit(train_df)\n",
    "final_metrics = eval_model.evaluate(test_df)\n",
    "\n",
    "print('Final Baseline Elo model trained on all', len(games_df), 'games')\n",
    "print(f'\\nTest-set metrics:')\n",
    "print(f'  Home RMSE:     {final_metrics[\"home_rmse\"]:.4f}')\n",
    "print(f'  Away RMSE:     {final_metrics[\"away_rmse\"]:.4f}')\n",
    "print(f'  Combined RMSE: {final_metrics[\"combined_rmse\"]:.4f}')\n",
    "print(f'  Win Accuracy:  {final_metrics[\"win_accuracy\"]:.1%}')\n",
    "\n",
    "print(f'\\nðŸ† Team Rankings (top 10):')\n",
    "for rank, (team, rating) in enumerate(final_model.get_rankings(10), 1):\n",
    "    print(f'  {rank:2d}. {team:20s}  {rating:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Rating distribution visualization â”€â”€\n",
    "rankings = final_model.get_rankings()\n",
    "teams_r, ratings_r = zip(*rankings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['#2ecc71' if r > 1500 else '#e74c3c' if r < 1500 else '#95a5a6' for r in ratings_r]\n",
    "ax.barh(range(len(teams_r)), ratings_r, color=colors, edgecolor='black', linewidth=0.3)\n",
    "ax.set_yticks(range(len(teams_r)))\n",
    "ax.set_yticklabels(teams_r, fontsize=9)\n",
    "ax.axvline(1500, color='gray', ls='--', lw=1, label='Initial (1500)')\n",
    "ax.set_xlabel('Elo Rating')\n",
    "ax.set_title('Baseline Elo Ratings â€” All Teams')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load Round 1 matchups and predict â”€â”€\n",
    "matchups = pd.read_excel('data/WHSDSC_Rnd1_matchups.xlsx')\n",
    "print(f'{len(matchups)} matchups loaded')\n",
    "\n",
    "home_col = [c for c in matchups.columns if 'home' in c.lower()][0]\n",
    "away_col = [c for c in matchups.columns if 'away' in c.lower()][0]\n",
    "\n",
    "preds = []\n",
    "for i, row in matchups.iterrows():\n",
    "    game = {'home_team': row[home_col], 'away_team': row[away_col]}\n",
    "    h_goals, a_goals = final_model.predict_goals(game)\n",
    "    winner, conf = final_model.predict_winner(game)\n",
    "    preds.append({\n",
    "        'game': i + 1,\n",
    "        'home_team': row[home_col],\n",
    "        'away_team': row[away_col],\n",
    "        'pred_home_goals': round(h_goals, 2),\n",
    "        'pred_away_goals': round(a_goals, 2),\n",
    "        'predicted_winner': winner,\n",
    "        'confidence': round(conf, 4),\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(preds)\n",
    "print(f'\\nRound 1 Baseline Elo Predictions:')\n",
    "print(pred_df.to_string(index=False))\n",
    "print(f'\\nAvg confidence: {pred_df[\"confidence\"].mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output/predictions/baseline_elo', exist_ok=True)\n",
    "os.makedirs('output/models/baseline_elo', exist_ok=True)\n",
    "\n",
    "# 1. Grid search comparison CSV\n",
    "results_df.to_csv('output/predictions/baseline_elo/baseline_elo_comparison.csv', index=False)\n",
    "print('âœ… output/predictions/baseline_elo/baseline_elo_comparison.csv')\n",
    "\n",
    "# 2. Round 1 predictions CSV\n",
    "pred_df.to_csv('output/predictions/baseline_elo/round1_baseline_elo_predictions.csv', index=False)\n",
    "print('âœ… output/predictions/baseline_elo/round1_baseline_elo_predictions.csv')\n",
    "\n",
    "# 3. Pipeline summary JSON\n",
    "summary = {\n",
    "    'model': 'BaselineElo',\n",
    "    'best_params': best_params,\n",
    "    'test_metrics': {k: round(v, 6) if isinstance(v, float) else v for k, v in final_metrics.items()},\n",
    "    'team_rankings': {team: round(r, 1) for team, r in final_model.get_rankings()},\n",
    "    'predictions': preds,\n",
    "}\n",
    "with open('output/predictions/baseline_elo/baseline_elo_pipeline_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print('âœ… output/predictions/baseline_elo/baseline_elo_pipeline_summary.json')\n",
    "\n",
    "print(f'\\nðŸ† SUMMARY: Combined RMSE {final_metrics[\"combined_rmse\"]:.4f}, Win Acc {final_metrics[\"win_accuracy\"]:.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

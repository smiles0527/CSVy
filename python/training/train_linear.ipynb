{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d1cce1",
   "metadata": {},
   "source": [
    "# Training Linear Regression Models\n",
    "\n",
    "This notebook trains and tunes linear regression models for hockey goal prediction.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load and prepare data\n",
    "2. Grid search for optimal hyperparameters\n",
    "3. Random search for broader exploration\n",
    "4. Train final model with best parameters\n",
    "5. Evaluate and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.linear_model import (\n",
    "    LinearRegressionModel,\n",
    "    LinearGoalPredictor,\n",
    "    grid_search_linear,\n",
    "    random_search_linear,\n",
    "    compare_regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a57ad",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameter config\n",
    "config_path = Path('../../config/hyperparams/model2_linear_regression.yaml')\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"Loaded configuration:\")\n",
    "    print(f\"  Model: {config['model_name']}\")\n",
    "    print(f\"  Description: {config['description']}\")\n",
    "    hyperparams = config['hyperparameters']\n",
    "    defaults = config['defaults']\n",
    "else:\n",
    "    print(\"Config not found, using defaults\")\n",
    "    hyperparams = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'l1_ratio': [0.0, 0.5, 1.0],\n",
    "        'poly_degree': [1, 2],\n",
    "        'scaling': ['standard', 'robust'],\n",
    "    }\n",
    "    defaults = {'alpha': 1.0, 'l1_ratio': 0.5, 'poly_degree': 1, 'scaling': 'standard'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23305f44",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c52f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for demo (replace with real data loading)\n",
    "np.random.seed(42)\n",
    "n_games = 1000\n",
    "\n",
    "# Features\n",
    "data = pd.DataFrame({\n",
    "    'home_elo': np.random.normal(1500, 100, n_games),\n",
    "    'away_elo': np.random.normal(1500, 100, n_games),\n",
    "    'home_recent_form': np.random.uniform(0, 1, n_games),\n",
    "    'away_recent_form': np.random.uniform(0, 1, n_games),\n",
    "    'home_rest_days': np.random.choice([1, 2, 3, 4, 5], n_games),\n",
    "    'away_rest_days': np.random.choice([1, 2, 3, 4, 5], n_games),\n",
    "    'home_avg_goals': np.random.normal(3.0, 0.5, n_games),\n",
    "    'away_avg_goals': np.random.normal(2.8, 0.5, n_games),\n",
    "    'home_avg_against': np.random.normal(2.7, 0.5, n_games),\n",
    "    'away_avg_against': np.random.normal(2.9, 0.5, n_games),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.30, n_games),\n",
    "    'away_pp_pct': np.random.uniform(0.15, 0.30, n_games),\n",
    "    'home_pk_pct': np.random.uniform(0.75, 0.90, n_games),\n",
    "    'away_pk_pct': np.random.uniform(0.75, 0.90, n_games),\n",
    "})\n",
    "\n",
    "# Generate targets\n",
    "home_base = (\n",
    "    0.5 * (data['home_elo'] - data['away_elo']) / 100 +\n",
    "    0.3 * data['home_recent_form'] +\n",
    "    0.5 * data['home_avg_goals'] -\n",
    "    0.2 * data['away_avg_goals'] +\n",
    "    0.3 * data['home_pp_pct'] * 10\n",
    ")\n",
    "\n",
    "away_base = (\n",
    "    0.5 * (data['away_elo'] - data['home_elo']) / 100 +\n",
    "    0.3 * data['away_recent_form'] +\n",
    "    0.5 * data['away_avg_goals'] -\n",
    "    0.2 * data['home_avg_goals'] +\n",
    "    0.3 * data['away_pp_pct'] * 10\n",
    ")\n",
    "\n",
    "data['home_goals'] = np.maximum(0, np.round(2.8 + home_base + np.random.normal(0, 1, n_games))).astype(int)\n",
    "data['away_goals'] = np.maximum(0, np.round(2.6 + away_base + np.random.normal(0, 1, n_games))).astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(data)} games, {len(data.columns) - 2} features\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Home goals: mean={data['home_goals'].mean():.2f}, std={data['home_goals'].std():.2f}\")\n",
    "print(f\"  Away goals: mean={data['away_goals'].mean():.2f}, std={data['away_goals'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation/test split\n",
    "train_df, temp_df = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training:   {len(train_df)} games\")\n",
    "print(f\"Validation: {len(val_df)} games\")\n",
    "print(f\"Test:       {len(test_df)} games\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [col for col in data.columns if col not in ['home_goals', 'away_goals']]\n",
    "X_train = train_df[feature_cols]\n",
    "y_home_train = train_df['home_goals']\n",
    "y_away_train = train_df['away_goals']\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_home_val = val_df['home_goals']\n",
    "y_away_val = val_df['away_goals']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcc587",
   "metadata": {},
   "source": [
    "## 3. Grid Search Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d41df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid from config\n",
    "param_grid = {\n",
    "    'alpha': hyperparams.get('alpha', [0.001, 0.01, 0.1, 1.0, 10.0]),\n",
    "    'l1_ratio': hyperparams.get('l1_ratio', [0.0, 0.5, 1.0]),\n",
    "    'poly_degree': hyperparams.get('poly_degree', [1, 2]),\n",
    "    'scaling': hyperparams.get('scaling', ['standard', 'robust']),\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total = 1\n",
    "for v in param_grid.values():\n",
    "    total *= len(v)\n",
    "print(f\"Total grid search combinations: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search for home goals\n",
    "print(\"Grid Search for Home Goals...\")\n",
    "home_grid_results = grid_search_linear(\n",
    "    X_train, y_home_train,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Home Goals Parameters:\")\n",
    "print(f\"  {home_grid_results['best_params']}\")\n",
    "print(f\"  RMSE: {home_grid_results['best_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76702fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search for away goals\n",
    "print(\"Grid Search for Away Goals...\")\n",
    "away_grid_results = grid_search_linear(\n",
    "    X_train, y_away_train,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Away Goals Parameters:\")\n",
    "print(f\"  {away_grid_results['best_params']}\")\n",
    "print(f\"  RMSE: {away_grid_results['best_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 configurations from grid search\n",
    "print(\"Top 10 Home Goals Configurations:\")\n",
    "home_grid_results['all_results'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e9a9c",
   "metadata": {},
   "source": [
    "## 4. Random Search (Broader Exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search with wider parameter ranges\n",
    "param_distributions = {\n",
    "    'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0],\n",
    "    'l1_ratio': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'poly_degree': [1, 2],\n",
    "    'scaling': ['standard', 'robust'],\n",
    "    'max_iter': [1000, 5000],\n",
    "}\n",
    "\n",
    "print(\"Random Search for Home Goals (100 iterations)...\")\n",
    "home_random_results = random_search_linear(\n",
    "    X_train, y_home_train,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Random Search Parameters:\")\n",
    "print(f\"  {home_random_results['best_params']}\")\n",
    "print(f\"  RMSE: {home_random_results['best_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea290e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare grid search vs random search\n",
    "print(\"Comparison:\")\n",
    "print(f\"  Grid Search Best:   RMSE = {home_grid_results['best_score']:.4f}\")\n",
    "print(f\"  Random Search Best: RMSE = {home_random_results['best_score']:.4f}\")\n",
    "\n",
    "# Use the better result\n",
    "if home_random_results['best_score'] < home_grid_results['best_score']:\n",
    "    best_params = home_random_results['best_params']\n",
    "    print(\"\\n→ Using Random Search parameters\")\n",
    "else:\n",
    "    best_params = home_grid_results['best_params']\n",
    "    print(\"\\n→ Using Grid Search parameters\")\n",
    "\n",
    "print(f\"Best params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f31325",
   "metadata": {},
   "source": [
    "## 5. Regularization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a60d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization types\n",
    "comparison = compare_regularization(\n",
    "    X_train, y_home_train,\n",
    "    alphas=[0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "    subset = comparison[comparison['model'] == model_name]\n",
    "    ax.errorbar(\n",
    "        subset['alpha'], \n",
    "        subset['rmse_mean'],\n",
    "        yerr=subset['rmse_std'],\n",
    "        marker='o', \n",
    "        label=model_name,\n",
    "        capsize=3\n",
    "    )\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Alpha (Regularization Strength)')\n",
    "ax.set_ylabel('RMSE (5-fold CV)')\n",
    "ax.set_title('Regularization Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e7485",
   "metadata": {},
   "source": [
    "## 6. Train Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbeb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final predictor with best parameters\n",
    "final_predictor = LinearGoalPredictor(\n",
    "    alpha=best_params.get('alpha', 0.1),\n",
    "    l1_ratio=best_params.get('l1_ratio', 0.5),\n",
    "    scaling=best_params.get('scaling', 'standard'),\n",
    "    poly_degree=best_params.get('poly_degree', 1),\n",
    "    max_iter=best_params.get('max_iter', 1000)\n",
    ")\n",
    "\n",
    "# Combine train and validation for final training\n",
    "full_train = pd.concat([train_df, val_df])\n",
    "final_predictor.fit(full_train)\n",
    "\n",
    "print(\"Final predictor trained!\")\n",
    "print(final_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on held-out test set\n",
    "test_metrics = final_predictor.evaluate(test_df)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL TEST SET RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nHome Goals Prediction:\")\n",
    "print(f\"  RMSE: {test_metrics['home']['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {test_metrics['home']['mae']:.4f}\")\n",
    "print(f\"  R²:   {test_metrics['home']['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nAway Goals Prediction:\")\n",
    "print(f\"  RMSE: {test_metrics['away']['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {test_metrics['away']['mae']:.4f}\")\n",
    "print(f\"  R²:   {test_metrics['away']['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nCombined Metrics:\")\n",
    "print(f\"  RMSE: {test_metrics['combined']['rmse']:.4f}\")\n",
    "print(f\"  MAE:  {test_metrics['combined']['mae']:.4f}\")\n",
    "print(f\"  R²:   {test_metrics['combined']['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nWin Prediction Accuracy: {test_metrics['win_accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc9c48",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance (combined from both models)\n",
    "importance = final_predictor.get_feature_importance(target='combined', top_n=15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "importance.plot(kind='barh', ax=ax)\n",
    "ax.set_xlabel('Importance (|coefficient|)')\n",
    "ax.set_title('Top 15 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient analysis for home model\n",
    "home_coefs = final_predictor.get_coefficients(target='home')\n",
    "print(\"Home Goals Model Coefficients:\")\n",
    "home_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f85a4",
   "metadata": {},
   "source": [
    "## 8. Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee546b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "predictions = final_predictor.predict_batch(test_df)\n",
    "predictions['home_actual'] = test_df['home_goals'].values\n",
    "predictions['away_actual'] = test_df['away_goals'].values\n",
    "predictions['home_error'] = predictions['home_pred'] - predictions['home_actual']\n",
    "predictions['away_error'] = predictions['away_pred'] - predictions['away_actual']\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(predictions['home_error'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Prediction Error')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Home Goals Prediction Error')\n",
    "\n",
    "axes[1].hist(predictions['away_error'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Prediction Error')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Away Goals Prediction Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual scatter\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].scatter(predictions['home_actual'], predictions['home_pred'], alpha=0.5)\n",
    "axes[0].plot([0, 10], [0, 10], 'r--', label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Home Goals')\n",
    "axes[0].set_ylabel('Predicted Home Goals')\n",
    "axes[0].set_title('Home Goals: Predicted vs Actual')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(predictions['away_actual'], predictions['away_pred'], alpha=0.5)\n",
    "axes[1].plot([0, 10], [0, 10], 'r--', label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual Away Goals')\n",
    "axes[1].set_ylabel('Predicted Away Goals')\n",
    "axes[1].set_title('Away Goals: Predicted vs Actual')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6827f",
   "metadata": {},
   "source": [
    "## 9. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48cca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../models/saved/linear_regression')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save predictor\n",
    "final_predictor.save(output_dir / 'linear_predictor')\n",
    "print(f\"Model saved to {output_dir / 'linear_predictor'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24df4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save grid search results\n",
    "home_grid_results['all_results'].to_csv(\n",
    "    output_dir / 'home_grid_search_results.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "home_random_results['all_results'].to_csv(\n",
    "    output_dir / 'home_random_search_results.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Search results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee34edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training summary\n",
    "summary = {\n",
    "    'model_name': 'LinearGoalPredictor',\n",
    "    'best_params': best_params,\n",
    "    'test_metrics': test_metrics,\n",
    "    'training_samples': len(full_train),\n",
    "    'test_samples': len(test_df),\n",
    "    'n_features': len(feature_cols),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / 'training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(json.dumps(summary, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a39265",
   "metadata": {},
   "source": [
    "## 10. Load and Verify Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee015462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "loaded = LinearGoalPredictor.load(output_dir / 'linear_predictor')\n",
    "\n",
    "# Verify predictions match\n",
    "loaded_metrics = loaded.evaluate(test_df)\n",
    "\n",
    "print(\"Verification:\")\n",
    "print(f\"  Original RMSE:  {test_metrics['combined']['rmse']:.6f}\")\n",
    "print(f\"  Loaded RMSE:    {loaded_metrics['combined']['rmse']:.6f}\")\n",
    "print(f\"  Match: {abs(test_metrics['combined']['rmse'] - loaded_metrics['combined']['rmse']) < 0.0001}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c420b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. ✅ Loaded configuration from YAML\n",
    "2. ✅ Prepared training/validation/test splits\n",
    "3. ✅ Ran grid search for optimal hyperparameters\n",
    "4. ✅ Ran random search for broader exploration\n",
    "5. ✅ Compared regularization types (Ridge/Lasso/ElasticNet)\n",
    "6. ✅ Trained final model with best parameters\n",
    "7. ✅ Evaluated on held-out test set\n",
    "8. ✅ Analyzed feature importance\n",
    "9. ✅ Saved model and results\n",
    "\n",
    "### Next Steps:\n",
    "- Train on real hockey data\n",
    "- Compare with baseline and ELO models\n",
    "- Combine in ensemble model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

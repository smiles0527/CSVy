{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f38efb",
   "metadata": {},
   "source": [
    "# Model 1: Baseline Models - Training and Comparison\n",
    "\n",
    "This notebook trains and compares multiple baseline models for hockey goal prediction.\n",
    "\n",
    "## Baseline Models Included\n",
    "\n",
    "| Model | Description | Use Case |\n",
    "|-------|-------------|----------|\n",
    "| GlobalMeanBaseline | League-wide average | Sanity check lower bound |\n",
    "| TeamMeanBaseline | Per-team offense/defense averages | Standard baseline |\n",
    "| HomeAwayBaseline | Location-aware averages | Captures home advantage |\n",
    "| MovingAverageBaseline | Recent N games only | Captures team form |\n",
    "| WeightedHistoryBaseline | Exponential decay weighting | Balances history and recency |\n",
    "| PoissonBaseline | Statistical Poisson model | Academic standard |\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Setup and Imports\n",
    "2. Load Data\n",
    "3. Train All Baselines\n",
    "4. Compare Performance\n",
    "5. Analyze Best Baseline\n",
    "6. Hyperparameter Search\n",
    "7. Final Evaluation\n",
    "8. Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb8234",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c157a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "# Add parent directory for imports\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model Classes (self-contained for portability)\n",
    "\n",
    "COLUMN_ALIASES = {\n",
    "    'home_team': ['home_team', 'home', 'team_home', 'h_team'],\n",
    "    'away_team': ['away_team', 'away', 'team_away', 'a_team', 'visitor', 'visiting_team'],\n",
    "    'home_goals': ['home_goals', 'home_score', 'h_goals', 'goals_home', 'home_pts'],\n",
    "    'away_goals': ['away_goals', 'away_score', 'a_goals', 'goals_away', 'away_pts', 'visitor_goals'],\n",
    "    'game_date': ['game_date', 'date', 'Date', 'game_datetime', 'datetime', 'game_time'],\n",
    "}\n",
    "\n",
    "def get_value(game, field, default=None):\n",
    "    \"\"\"Get a value from a game record, checking multiple possible column names.\"\"\"\n",
    "    aliases = COLUMN_ALIASES.get(field, [field])\n",
    "    for alias in aliases:\n",
    "        if alias in game:\n",
    "            val = game[alias]\n",
    "            if pd.isna(val):\n",
    "                return default\n",
    "            return val\n",
    "    return default\n",
    "\n",
    "def get_column(df, field):\n",
    "    \"\"\"Find the correct column name in a DataFrame.\"\"\"\n",
    "    aliases = COLUMN_ALIASES.get(field, [field])\n",
    "    for alias in aliases:\n",
    "        if alias in df.columns:\n",
    "            return alias\n",
    "    return None\n",
    "\n",
    "\n",
    "class BaselineModel:\n",
    "    \"\"\"Abstract base class for baseline models.\"\"\"\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        self.params = params or {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def evaluate(self, games_df):\n",
    "        \"\"\"Evaluate model on test set.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before evaluation\")\n",
    "        \n",
    "        home_preds, away_preds = [], []\n",
    "        home_actuals, away_actuals = [], []\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            home_pred, away_pred = self.predict_goals(game)\n",
    "            home_preds.append(home_pred)\n",
    "            away_preds.append(away_pred)\n",
    "            home_actuals.append(get_value(game, 'home_goals', 0))\n",
    "            away_actuals.append(get_value(game, 'away_goals', 0))\n",
    "        \n",
    "        rmse = mean_squared_error(home_actuals, home_preds, squared=False)\n",
    "        mae = mean_absolute_error(home_actuals, home_preds)\n",
    "        r2 = r2_score(home_actuals, home_preds) if len(set(home_actuals)) > 1 else 0.0\n",
    "        \n",
    "        all_preds = home_preds + away_preds\n",
    "        all_actuals = home_actuals + away_actuals\n",
    "        combined_rmse = mean_squared_error(all_actuals, all_preds, squared=False)\n",
    "        \n",
    "        return {'rmse': rmse, 'mae': mae, 'r2': r2, 'combined_rmse': combined_rmse}\n",
    "\n",
    "\n",
    "class GlobalMeanBaseline(BaselineModel):\n",
    "    \"\"\"Predict league-wide average goals for all games.\"\"\"\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        home_col = get_column(games_df, 'home_goals')\n",
    "        away_col = get_column(games_df, 'away_goals')\n",
    "        self.global_mean_home = games_df[home_col].mean()\n",
    "        self.global_mean_away = games_df[away_col].mean()\n",
    "        self.n_games = len(games_df)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        return self.global_mean_home, self.global_mean_away\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {'model': 'GlobalMeanBaseline', 'global_mean_home': round(self.global_mean_home, 3)}\n",
    "\n",
    "\n",
    "class TeamMeanBaseline(BaselineModel):\n",
    "    \"\"\"Predict based on team offensive/defensive averages.\"\"\"\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        home_team_col = get_column(games_df, 'home_team')\n",
    "        away_team_col = get_column(games_df, 'away_team')\n",
    "        home_goals_col = get_column(games_df, 'home_goals')\n",
    "        away_goals_col = get_column(games_df, 'away_goals')\n",
    "        \n",
    "        goals_for, goals_against, games_played = {}, {}, {}\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            home_team = game[home_team_col]\n",
    "            away_team = game[away_team_col]\n",
    "            home_goals = game[home_goals_col]\n",
    "            away_goals = game[away_goals_col]\n",
    "            \n",
    "            goals_for[home_team] = goals_for.get(home_team, 0) + home_goals\n",
    "            goals_against[home_team] = goals_against.get(home_team, 0) + away_goals\n",
    "            games_played[home_team] = games_played.get(home_team, 0) + 1\n",
    "            \n",
    "            goals_for[away_team] = goals_for.get(away_team, 0) + away_goals\n",
    "            goals_against[away_team] = goals_against.get(away_team, 0) + home_goals\n",
    "            games_played[away_team] = games_played.get(away_team, 0) + 1\n",
    "        \n",
    "        self.team_offense = {t: goals_for[t] / games_played[t] for t in games_played}\n",
    "        self.team_defense = {t: goals_against[t] / games_played[t] for t in games_played}\n",
    "        self.global_mean = games_df[home_goals_col].mean()\n",
    "        self.n_teams = len(games_played)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        home_team = get_value(game, 'home_team')\n",
    "        away_team = get_value(game, 'away_team')\n",
    "        \n",
    "        home_off = self.team_offense.get(home_team, self.global_mean)\n",
    "        home_def = self.team_defense.get(home_team, self.global_mean)\n",
    "        away_off = self.team_offense.get(away_team, self.global_mean)\n",
    "        away_def = self.team_defense.get(away_team, self.global_mean)\n",
    "        \n",
    "        return (home_off + away_def) / 2, (away_off + home_def) / 2\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {'model': 'TeamMeanBaseline', 'n_teams': self.n_teams}\n",
    "\n",
    "\n",
    "class HomeAwayBaseline(BaselineModel):\n",
    "    \"\"\"Account for home/away goal differentials.\"\"\"\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        home_team_col = get_column(games_df, 'home_team')\n",
    "        away_team_col = get_column(games_df, 'away_team')\n",
    "        home_goals_col = get_column(games_df, 'home_goals')\n",
    "        away_goals_col = get_column(games_df, 'away_goals')\n",
    "        \n",
    "        home_goals_for, home_goals_against, home_games = {}, {}, {}\n",
    "        away_goals_for, away_goals_against, away_games = {}, {}, {}\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            ht, at = game[home_team_col], game[away_team_col]\n",
    "            hg, ag = game[home_goals_col], game[away_goals_col]\n",
    "            \n",
    "            home_goals_for[ht] = home_goals_for.get(ht, 0) + hg\n",
    "            home_goals_against[ht] = home_goals_against.get(ht, 0) + ag\n",
    "            home_games[ht] = home_games.get(ht, 0) + 1\n",
    "            \n",
    "            away_goals_for[at] = away_goals_for.get(at, 0) + ag\n",
    "            away_goals_against[at] = away_goals_against.get(at, 0) + hg\n",
    "            away_games[at] = away_games.get(at, 0) + 1\n",
    "        \n",
    "        self.home_offense = {t: home_goals_for[t]/home_games[t] for t in home_games}\n",
    "        self.home_defense = {t: home_goals_against[t]/home_games[t] for t in home_games}\n",
    "        self.away_offense = {t: away_goals_for[t]/away_games[t] for t in away_games}\n",
    "        self.away_defense = {t: away_goals_against[t]/away_games[t] for t in away_games}\n",
    "        \n",
    "        self.global_home_mean = games_df[home_goals_col].mean()\n",
    "        self.global_away_mean = games_df[away_goals_col].mean()\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        ht = get_value(game, 'home_team')\n",
    "        at = get_value(game, 'away_team')\n",
    "        \n",
    "        home_off = self.home_offense.get(ht, self.global_home_mean)\n",
    "        away_def = self.away_defense.get(at, self.global_home_mean)\n",
    "        away_off = self.away_offense.get(at, self.global_away_mean)\n",
    "        home_def = self.home_defense.get(ht, self.global_away_mean)\n",
    "        \n",
    "        return (home_off + away_def) / 2, (away_off + home_def) / 2\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {'model': 'HomeAwayBaseline', 'home_advantage': round(self.global_home_mean - self.global_away_mean, 3)}\n",
    "\n",
    "\n",
    "class MovingAverageBaseline(BaselineModel):\n",
    "    \"\"\"Use only last N games for predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        super().__init__(params)\n",
    "        self.window = self.params.get('window', 5)\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        home_team_col = get_column(games_df, 'home_team')\n",
    "        away_team_col = get_column(games_df, 'away_team')\n",
    "        home_goals_col = get_column(games_df, 'home_goals')\n",
    "        away_goals_col = get_column(games_df, 'away_goals')\n",
    "        \n",
    "        self.team_history = {}\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            ht, at = game[home_team_col], game[away_team_col]\n",
    "            hg, ag = game[home_goals_col], game[away_goals_col]\n",
    "            \n",
    "            if ht not in self.team_history: self.team_history[ht] = []\n",
    "            if at not in self.team_history: self.team_history[at] = []\n",
    "            \n",
    "            self.team_history[ht].append((hg, ag))\n",
    "            self.team_history[at].append((ag, hg))\n",
    "        \n",
    "        self.global_mean = games_df[home_goals_col].mean()\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _get_recent_avg(self, team):\n",
    "        if team not in self.team_history or len(self.team_history[team]) == 0:\n",
    "            return self.global_mean, self.global_mean\n",
    "        recent = self.team_history[team][-self.window:]\n",
    "        return np.mean([g[0] for g in recent]), np.mean([g[1] for g in recent])\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        ht, at = get_value(game, 'home_team'), get_value(game, 'away_team')\n",
    "        home_off, home_def = self._get_recent_avg(ht)\n",
    "        away_off, away_def = self._get_recent_avg(at)\n",
    "        return (home_off + away_def) / 2, (away_off + home_def) / 2\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {'model': f'MovingAverage(window={self.window})', 'window': self.window}\n",
    "\n",
    "\n",
    "class WeightedHistoryBaseline(BaselineModel):\n",
    "    \"\"\"Recent games count more than older games.\"\"\"\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        super().__init__(params)\n",
    "        self.decay = self.params.get('decay', 0.9)\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        home_team_col = get_column(games_df, 'home_team')\n",
    "        away_team_col = get_column(games_df, 'away_team')\n",
    "        home_goals_col = get_column(games_df, 'home_goals')\n",
    "        away_goals_col = get_column(games_df, 'away_goals')\n",
    "        \n",
    "        self.team_history = {}\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            ht, at = game[home_team_col], game[away_team_col]\n",
    "            hg, ag = game[home_goals_col], game[away_goals_col]\n",
    "            \n",
    "            if ht not in self.team_history: self.team_history[ht] = []\n",
    "            if at not in self.team_history: self.team_history[at] = []\n",
    "            \n",
    "            self.team_history[ht].append((hg, ag))\n",
    "            self.team_history[at].append((ag, hg))\n",
    "        \n",
    "        self.global_mean = games_df[home_goals_col].mean()\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _get_weighted_avg(self, team):\n",
    "        if team not in self.team_history or len(self.team_history[team]) == 0:\n",
    "            return self.global_mean, self.global_mean\n",
    "        \n",
    "        history = self.team_history[team]\n",
    "        n = len(history)\n",
    "        weighted_for, weighted_against, total_weight = 0, 0, 0\n",
    "        \n",
    "        for i, (gf, ga) in enumerate(history):\n",
    "            weight = self.decay ** (n - 1 - i)\n",
    "            weighted_for += gf * weight\n",
    "            weighted_against += ga * weight\n",
    "            total_weight += weight\n",
    "        \n",
    "        return weighted_for / total_weight, weighted_against / total_weight\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        ht, at = get_value(game, 'home_team'), get_value(game, 'away_team')\n",
    "        home_off, home_def = self._get_weighted_avg(ht)\n",
    "        away_off, away_def = self._get_weighted_avg(at)\n",
    "        return (home_off + away_def) / 2, (away_off + home_def) / 2\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {'model': f'WeightedHistory(decay={self.decay})', 'decay': self.decay}\n",
    "\n",
    "\n",
    "class PoissonBaseline(BaselineModel):\n",
    "    \"\"\"Statistical Poisson regression model.\"\"\"\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        home_team_col = get_column(games_df, 'home_team')\n",
    "        away_team_col = get_column(games_df, 'away_team')\n",
    "        home_goals_col = get_column(games_df, 'home_goals')\n",
    "        away_goals_col = get_column(games_df, 'away_goals')\n",
    "        \n",
    "        self.league_avg = games_df[home_goals_col].mean()\n",
    "        self.home_factor = games_df[home_goals_col].mean() / max(games_df[away_goals_col].mean(), 0.01)\n",
    "        \n",
    "        goals_for, goals_against, games_played = {}, {}, {}\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            ht, at = game[home_team_col], game[away_team_col]\n",
    "            hg, ag = game[home_goals_col], game[away_goals_col]\n",
    "            \n",
    "            goals_for[ht] = goals_for.get(ht, 0) + hg\n",
    "            goals_against[ht] = goals_against.get(ht, 0) + ag\n",
    "            games_played[ht] = games_played.get(ht, 0) + 1\n",
    "            \n",
    "            goals_for[at] = goals_for.get(at, 0) + ag\n",
    "            goals_against[at] = goals_against.get(at, 0) + hg\n",
    "            games_played[at] = games_played.get(at, 0) + 1\n",
    "        \n",
    "        self.attack_strength = {t: (goals_for[t]/games_played[t])/self.league_avg for t in games_played}\n",
    "        self.defense_strength = {t: (goals_against[t]/games_played[t])/self.league_avg for t in games_played}\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        ht, at = get_value(game, 'home_team'), get_value(game, 'away_team')\n",
    "        \n",
    "        home_att = self.attack_strength.get(ht, 1.0)\n",
    "        home_def = self.defense_strength.get(ht, 1.0)\n",
    "        away_att = self.attack_strength.get(at, 1.0)\n",
    "        away_def = self.defense_strength.get(at, 1.0)\n",
    "        \n",
    "        home_goals = self.league_avg * home_att * away_def * self.home_factor\n",
    "        away_goals = self.league_avg * away_att * home_def / self.home_factor\n",
    "        \n",
    "        return home_goals, away_goals\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {'model': 'PoissonBaseline', 'home_factor': round(self.home_factor, 3)}\n",
    "\n",
    "\n",
    "print(\"All baseline model classes loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82e119",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple possible data paths\n",
    "data_paths = [\n",
    "    '../data/hockey_data.csv',\n",
    "    '../../data/hockey_data.csv',\n",
    "    'hockey_data.csv',\n",
    "]\n",
    "\n",
    "games_df = None\n",
    "for path in data_paths:\n",
    "    if os.path.exists(path):\n",
    "        games_df = pd.read_csv(path)\n",
    "        print(f\"Loaded data from: {path}\")\n",
    "        break\n",
    "\n",
    "if games_df is None:\n",
    "    print(\"No data file found. Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Generate realistic synthetic hockey data\n",
    "    np.random.seed(42)\n",
    "    n_games = 500\n",
    "    teams = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E', 'Team F', 'Team G', 'Team H']\n",
    "    \n",
    "    # Team strength (affects goals scored)\n",
    "    team_strength = {t: np.random.uniform(0.8, 1.2) for t in teams}\n",
    "    \n",
    "    games = []\n",
    "    for i in range(n_games):\n",
    "        home, away = np.random.choice(teams, 2, replace=False)\n",
    "        \n",
    "        # Base goals with team strength and home advantage\n",
    "        home_lambda = 3.0 * team_strength[home] * 1.1  # Home advantage\n",
    "        away_lambda = 3.0 * team_strength[away] * 0.9\n",
    "        \n",
    "        home_goals = np.random.poisson(home_lambda)\n",
    "        away_goals = np.random.poisson(away_lambda)\n",
    "        \n",
    "        games.append({\n",
    "            'game_date': pd.Timestamp('2025-10-01') + pd.Timedelta(days=i//3),\n",
    "            'home_team': home,\n",
    "            'away_team': away,\n",
    "            'home_goals': home_goals,\n",
    "            'away_goals': away_goals\n",
    "        })\n",
    "    \n",
    "    games_df = pd.DataFrame(games)\n",
    "    print(f\"Generated {len(games_df)} synthetic games\")\n",
    "\n",
    "# Sort by date\n",
    "date_col = get_column(games_df, 'game_date')\n",
    "if date_col:\n",
    "    games_df = games_df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset: {len(games_df)} games\")\n",
    "print(f\"Columns: {list(games_df.columns)}\")\n",
    "games_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data chronologically (80/20)\n",
    "split_idx = int(len(games_df) * 0.8)\n",
    "train_df = games_df.iloc[:split_idx].copy()\n",
    "test_df = games_df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df)} games\")\n",
    "print(f\"Test set: {len(test_df)} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a1f7e",
   "metadata": {},
   "source": [
    "## 3. Train All Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all baseline models to compare\n",
    "models = {\n",
    "    'GlobalMean': GlobalMeanBaseline(),\n",
    "    'TeamMean': TeamMeanBaseline(),\n",
    "    'HomeAway': HomeAwayBaseline(),\n",
    "    'MovingAvg_3': MovingAverageBaseline({'window': 3}),\n",
    "    'MovingAvg_5': MovingAverageBaseline({'window': 5}),\n",
    "    'MovingAvg_10': MovingAverageBaseline({'window': 10}),\n",
    "    'Weighted_0.85': WeightedHistoryBaseline({'decay': 0.85}),\n",
    "    'Weighted_0.90': WeightedHistoryBaseline({'decay': 0.90}),\n",
    "    'Weighted_0.95': WeightedHistoryBaseline({'decay': 0.95}),\n",
    "    'Poisson': PoissonBaseline()\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "print(\"Training baseline models...\\n\")\n",
    "for name, model in models.items():\n",
    "    model.fit(train_df)\n",
    "    print(f\"  {name}: trained\")\n",
    "\n",
    "print(\"\\nAll models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5643b5",
   "metadata": {},
   "source": [
    "## 4. Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687da7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    metrics = model.evaluate(test_df)\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'rmse': metrics['rmse'],\n",
    "        'mae': metrics['mae'],\n",
    "        'r2': metrics['r2'],\n",
    "        'combined_rmse': metrics['combined_rmse']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('rmse')\n",
    "print(\"Model Comparison (sorted by RMSE):\")\n",
    "print(\"=\"*60)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "ax = axes[0]\n",
    "colors = ['green' if x == results_df['rmse'].min() else 'steelblue' for x in results_df['rmse']]\n",
    "ax.barh(results_df['model'], results_df['rmse'], color=colors)\n",
    "ax.set_xlabel('RMSE')\n",
    "ax.set_title('RMSE by Model (lower is better)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# MAE comparison\n",
    "ax = axes[1]\n",
    "colors = ['green' if x == results_df['mae'].min() else 'steelblue' for x in results_df['mae']]\n",
    "ax.barh(results_df['model'], results_df['mae'], color=colors)\n",
    "ax.set_xlabel('MAE')\n",
    "ax.set_title('MAE by Model (lower is better)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# R2 comparison\n",
    "ax = axes[2]\n",
    "colors = ['green' if x == results_df['r2'].max() else 'steelblue' for x in results_df['r2']]\n",
    "ax.barh(results_df['model'], results_df['r2'], color=colors)\n",
    "ax.set_xlabel('R-squared')\n",
    "ax.set_title('R-squared by Model (higher is better)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e009ca",
   "metadata": {},
   "source": [
    "## 5. Analyze Best Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ceb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_name = results_df.iloc[0]['model']\n",
    "best_model = models[best_name]\n",
    "best_metrics = results_df.iloc[0]\n",
    "\n",
    "print(f\"Best Baseline Model: {best_name}\")\n",
    "print(f\"=\"*40)\n",
    "print(f\"RMSE: {best_metrics['rmse']:.4f}\")\n",
    "print(f\"MAE:  {best_metrics['mae']:.4f}\")\n",
    "print(f\"R2:   {best_metrics['r2']:.4f}\")\n",
    "print(f\"\\nModel Details:\")\n",
    "print(best_model.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "predictions = []\n",
    "for _, game in test_df.iterrows():\n",
    "    home_pred, away_pred = best_model.predict_goals(game)\n",
    "    predictions.append({\n",
    "        'home_team': get_value(game, 'home_team'),\n",
    "        'away_team': get_value(game, 'away_team'),\n",
    "        'home_goals_actual': get_value(game, 'home_goals'),\n",
    "        'away_goals_actual': get_value(game, 'away_goals'),\n",
    "        'home_goals_pred': round(home_pred, 2),\n",
    "        'away_goals_pred': round(away_pred, 2),\n",
    "        'home_error': round(abs(get_value(game, 'home_goals') - home_pred), 2)\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "print(\"Sample Predictions:\")\n",
    "pred_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2971db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Error histogram\n",
    "ax = axes[0]\n",
    "errors = pred_df['home_goals_actual'] - pred_df['home_goals_pred']\n",
    "ax.hist(errors, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Prediction Error (Actual - Predicted)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Prediction Errors')\n",
    "\n",
    "# Predicted vs Actual\n",
    "ax = axes[1]\n",
    "ax.scatter(pred_df['home_goals_pred'], pred_df['home_goals_actual'], alpha=0.5)\n",
    "ax.plot([0, 8], [0, 8], 'r--', linewidth=2, label='Perfect prediction')\n",
    "ax.set_xlabel('Predicted Goals')\n",
    "ax.set_ylabel('Actual Goals')\n",
    "ax.set_title('Predicted vs Actual Home Goals')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94632d88",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Search\n",
    "\n",
    "For models with hyperparameters, find optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b814cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for MovingAverage window\n",
    "window_results = []\n",
    "\n",
    "for window in range(1, 21):\n",
    "    model = MovingAverageBaseline({'window': window})\n",
    "    model.fit(train_df)\n",
    "    metrics = model.evaluate(test_df)\n",
    "    window_results.append({\n",
    "        'window': window,\n",
    "        'rmse': metrics['rmse'],\n",
    "        'mae': metrics['mae']\n",
    "    })\n",
    "\n",
    "window_df = pd.DataFrame(window_results)\n",
    "best_window = window_df.loc[window_df['rmse'].idxmin(), 'window']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(window_df['window'], window_df['rmse'], 'o-', linewidth=2)\n",
    "plt.axvline(best_window, color='red', linestyle='--', label=f'Best: window={int(best_window)}')\n",
    "plt.xlabel('Window Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('MovingAverage: Window Size vs RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best window size: {int(best_window)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for WeightedHistory decay\n",
    "decay_results = []\n",
    "\n",
    "for decay in np.arange(0.70, 1.00, 0.02):\n",
    "    model = WeightedHistoryBaseline({'decay': decay})\n",
    "    model.fit(train_df)\n",
    "    metrics = model.evaluate(test_df)\n",
    "    decay_results.append({\n",
    "        'decay': round(decay, 2),\n",
    "        'rmse': metrics['rmse'],\n",
    "        'mae': metrics['mae']\n",
    "    })\n",
    "\n",
    "decay_df = pd.DataFrame(decay_results)\n",
    "best_decay = decay_df.loc[decay_df['rmse'].idxmin(), 'decay']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(decay_df['decay'], decay_df['rmse'], 'o-', linewidth=2)\n",
    "plt.axvline(best_decay, color='red', linestyle='--', label=f'Best: decay={best_decay}')\n",
    "plt.xlabel('Decay Factor')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('WeightedHistory: Decay Factor vs RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best decay factor: {best_decay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29151c9",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation\n",
    "\n",
    "Compare best tuned baselines against the simple GlobalMean reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3833cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison with tuned hyperparameters\n",
    "final_models = {\n",
    "    'GlobalMean (reference)': GlobalMeanBaseline(),\n",
    "    'TeamMean': TeamMeanBaseline(),\n",
    "    'HomeAway': HomeAwayBaseline(),\n",
    "    f'MovingAvg (window={int(best_window)})': MovingAverageBaseline({'window': int(best_window)}),\n",
    "    f'Weighted (decay={best_decay})': WeightedHistoryBaseline({'decay': best_decay}),\n",
    "    'Poisson': PoissonBaseline()\n",
    "}\n",
    "\n",
    "final_results = []\n",
    "for name, model in final_models.items():\n",
    "    model.fit(train_df)\n",
    "    metrics = model.evaluate(test_df)\n",
    "    final_results.append({\n",
    "        'model': name,\n",
    "        'rmse': round(metrics['rmse'], 4),\n",
    "        'mae': round(metrics['mae'], 4),\n",
    "        'r2': round(metrics['r2'], 4)\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame(final_results).sort_values('rmse')\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "baseline_rmse = final_df[final_df['model'].str.contains('GlobalMean')]['rmse'].values[0]\n",
    "final_df['improvement'] = round((baseline_rmse - final_df['rmse']) / baseline_rmse * 100, 1)\n",
    "\n",
    "print(\"Final Model Comparison:\")\n",
    "print(\"=\"*70)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the overall best baseline\n",
    "overall_best_name = final_df.iloc[0]['model']\n",
    "overall_best_rmse = final_df.iloc[0]['rmse']\n",
    "overall_best_improvement = final_df.iloc[0]['improvement']\n",
    "\n",
    "print(f\"\\nBest Baseline Model: {overall_best_name}\")\n",
    "print(f\"RMSE: {overall_best_rmse}\")\n",
    "print(f\"Improvement over GlobalMean: {overall_best_improvement}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb14db3",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20905aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "output_path = '../data/model1_baseline_results.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Save best configuration\n",
    "best_config = {\n",
    "    'model': 'baseline',\n",
    "    'best_variant': overall_best_name,\n",
    "    'metrics': {\n",
    "        'rmse': float(overall_best_rmse),\n",
    "        'improvement_pct': float(overall_best_improvement)\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'best_moving_avg_window': int(best_window),\n",
    "        'best_weighted_decay': float(best_decay)\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = '../data/best_baseline_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "print(f\"Best config saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Model: {overall_best_name}\")\n",
    "print(f\"RMSE: {overall_best_rmse}\")\n",
    "print(f\"\\nThis baseline RMSE ({overall_best_rmse:.3f}) is the benchmark.\")\n",
    "print(\"More complex models (ELO, XGBoost, etc.) should beat this.\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(f\"  - {output_path}\")\n",
    "print(f\"  - {config_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

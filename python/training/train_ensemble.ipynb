{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea4d99a",
   "metadata": {},
   "source": [
    "# Model 5: Ensemble - Training and Hyperparameter Tuning\n",
    "\n",
    "This notebook trains an ensemble model combining multiple base models for hockey goal prediction.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Setup and Imports\n",
    "2. Load Data\n",
    "3. Train Base Models\n",
    "4. Ensemble Methods\n",
    "5. Stacking Ensemble\n",
    "6. Weighted Average Ensemble\n",
    "7. Cross-Validation Analysis\n",
    "8. Final Model Evaluation\n",
    "9. Save Best Ensemble\n",
    "\n",
    "## Ensemble Approaches\n",
    "\n",
    "- **Simple Averaging**: Average predictions from all base models\n",
    "- **Weighted Averaging**: Weight models by inverse validation error\n",
    "- **Stacking**: Train a meta-model on base model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976274d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cffbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory for imports\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Try to import XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not available, using GradientBoosting instead\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"\\nSetup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameter configuration\n",
    "config_path = '../../config/hyperparams/model5_ensemble.yaml'\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"Loaded config: {config['model_name']}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "else:\n",
    "    print(f\"Config not found at {config_path}, using defaults\")\n",
    "    config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ecd467",
   "metadata": {},
   "source": [
    "## 2. Load or Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load real data, otherwise generate synthetic\n",
    "data_path = '../data/hockey_features.csv'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(data)} games from {data_path}\")\n",
    "else:\n",
    "    print(\"Generating synthetic hockey data for demonstration...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_games = 2000\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        # Team strength metrics\n",
    "        'home_win_pct': np.random.uniform(0.3, 0.7, n_games),\n",
    "        'away_win_pct': np.random.uniform(0.3, 0.7, n_games),\n",
    "        'home_points_pct': np.random.uniform(0.4, 0.8, n_games),\n",
    "        'away_points_pct': np.random.uniform(0.4, 0.8, n_games),\n",
    "        \n",
    "        # Offensive metrics\n",
    "        'home_goals_avg': np.random.uniform(2.5, 3.8, n_games),\n",
    "        'away_goals_avg': np.random.uniform(2.5, 3.8, n_games),\n",
    "        'home_shots_avg': np.random.uniform(28, 35, n_games),\n",
    "        'away_shots_avg': np.random.uniform(28, 35, n_games),\n",
    "        \n",
    "        # Defensive metrics\n",
    "        'home_goals_against_avg': np.random.uniform(2.2, 3.5, n_games),\n",
    "        'away_goals_against_avg': np.random.uniform(2.2, 3.5, n_games),\n",
    "        'home_save_pct': np.random.uniform(0.88, 0.93, n_games),\n",
    "        'away_save_pct': np.random.uniform(0.88, 0.93, n_games),\n",
    "        \n",
    "        # Special teams\n",
    "        'home_pp_pct': np.random.uniform(0.15, 0.28, n_games),\n",
    "        'away_pp_pct': np.random.uniform(0.15, 0.28, n_games),\n",
    "        'home_pk_pct': np.random.uniform(0.75, 0.88, n_games),\n",
    "        'away_pk_pct': np.random.uniform(0.75, 0.88, n_games),\n",
    "        \n",
    "        # Context\n",
    "        'home_rest_days': np.random.randint(1, 5, n_games),\n",
    "        'away_rest_days': np.random.randint(1, 5, n_games),\n",
    "        'home_b2b': np.random.binomial(1, 0.15, n_games),\n",
    "        'away_b2b': np.random.binomial(1, 0.15, n_games),\n",
    "        \n",
    "        # Recent form (last 5 games)\n",
    "        'home_goals_last5': np.random.uniform(2.0, 4.0, n_games),\n",
    "        'away_goals_last5': np.random.uniform(2.0, 4.0, n_games),\n",
    "        'home_wins_last5': np.random.randint(0, 6, n_games),\n",
    "        'away_wins_last5': np.random.randint(0, 6, n_games),\n",
    "    })\n",
    "    \n",
    "    # Generate realistic goal totals\n",
    "    home_advantage = 0.35\n",
    "    \n",
    "    data['home_goals'] = np.round(\n",
    "        data['home_goals_avg'] * 0.3 +\n",
    "        data['home_goals_last5'] * 0.2 +\n",
    "        (4 - data['away_goals_against_avg']) * 0.3 +\n",
    "        data['home_pp_pct'] * 3 +\n",
    "        home_advantage +\n",
    "        (data['home_rest_days'] - data['away_rest_days']) * 0.1 +\n",
    "        np.random.normal(0, 0.8, n_games)\n",
    "    ).clip(0, 9).astype(int)\n",
    "    \n",
    "    data['away_goals'] = np.round(\n",
    "        data['away_goals_avg'] * 0.3 +\n",
    "        data['away_goals_last5'] * 0.2 +\n",
    "        (4 - data['home_goals_against_avg']) * 0.3 +\n",
    "        data['away_pp_pct'] * 3 +\n",
    "        np.random.normal(0, 0.8, n_games)\n",
    "    ).clip(0, 9).astype(int)\n",
    "    \n",
    "    print(f\"Generated {n_games} synthetic games\")\n",
    "\n",
    "print(f\"\\nDataset shape: {data.shape}\")\n",
    "print(f\"Home goals mean: {data['home_goals'].mean():.2f}\")\n",
    "print(f\"Away goals mean: {data['away_goals'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ae88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "target_cols = ['home_goals', 'away_goals']\n",
    "exclude_cols = target_cols + ['home_team', 'away_team', 'date', 'game_id', 'season']\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "print(f\"Features ({len(feature_cols)}): {feature_cols[:10]}...\")\n",
    "\n",
    "X = data[feature_cols]\n",
    "y_home = data['home_goals']\n",
    "y_away = data['away_goals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eac225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation/test split (60/20/20)\n",
    "X_trainval, X_test, y_home_trainval, y_home_test, y_away_trainval, y_away_test = train_test_split(\n",
    "    X, y_home, y_away, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_home_train, y_home_val, y_away_train, y_away_val = train_test_split(\n",
    "    X_trainval, y_home_trainval, y_away_trainval, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features for linear models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} games\")\n",
    "print(f\"Validation set: {len(X_val)} games\")\n",
    "print(f\"Test set: {len(X_test)} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccde7cb",
   "metadata": {},
   "source": [
    "## 3. Train Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d213498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Evaluate a single model.\"\"\"\n",
    "    pred = model.predict(X)\n",
    "    return {\n",
    "        'rmse': np.sqrt(mean_squared_error(y, pred)),\n",
    "        'mae': mean_absolute_error(y, pred),\n",
    "        'r2': r2_score(y, pred),\n",
    "    }\n",
    "\n",
    "def evaluate_ensemble(home_model, away_model, X, y_home, y_away):\n",
    "    \"\"\"Evaluate both models and return combined metrics.\"\"\"\n",
    "    home_pred = home_model.predict(X)\n",
    "    away_pred = away_model.predict(X)\n",
    "    \n",
    "    metrics = {\n",
    "        'home_rmse': np.sqrt(mean_squared_error(y_home, home_pred)),\n",
    "        'away_rmse': np.sqrt(mean_squared_error(y_away, away_pred)),\n",
    "        'home_mae': mean_absolute_error(y_home, home_pred),\n",
    "        'away_mae': mean_absolute_error(y_away, away_pred),\n",
    "        'home_r2': r2_score(y_home, home_pred),\n",
    "        'away_r2': r2_score(y_away, away_pred),\n",
    "    }\n",
    "    \n",
    "    # Combined metrics\n",
    "    all_pred = np.concatenate([home_pred, away_pred])\n",
    "    all_actual = np.concatenate([y_home.values, y_away.values])\n",
    "    metrics['combined_rmse'] = np.sqrt(mean_squared_error(all_actual, all_pred))\n",
    "    metrics['combined_mae'] = mean_absolute_error(all_actual, all_pred)\n",
    "    metrics['combined_r2'] = r2_score(all_actual, all_pred)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10328593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = {\n",
    "    'ridge': Ridge(alpha=1.0),\n",
    "    'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    'random_forest': RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'gradient_boost': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "}\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    base_models['xgboost'] = xgb.XGBRegressor(\n",
    "        n_estimators=200, \n",
    "        max_depth=6, \n",
    "        learning_rate=0.1,\n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "print(f\"Base models: {list(base_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c594996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate base models for HOME goals\n",
    "home_base_models = {}\n",
    "home_base_predictions = {}\n",
    "home_base_metrics = {}\n",
    "\n",
    "print(\"Training Base Models for Home Goals:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    # Clone model\n",
    "    m = model.__class__(**model.get_params())\n",
    "    \n",
    "    # Train (use scaled for linear models)\n",
    "    if name in ['ridge', 'elastic_net']:\n",
    "        m.fit(X_train_scaled, y_home_train)\n",
    "        val_pred = m.predict(X_val_scaled)\n",
    "    else:\n",
    "        m.fit(X_train, y_home_train)\n",
    "        val_pred = m.predict(X_val)\n",
    "    \n",
    "    # Store\n",
    "    home_base_models[name] = m\n",
    "    home_base_predictions[name] = val_pred\n",
    "    \n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_home_val, val_pred))\n",
    "    mae = mean_absolute_error(y_home_val, val_pred)\n",
    "    home_base_metrics[name] = {'rmse': rmse, 'mae': mae}\n",
    "    \n",
    "    print(f\"  {name:15} - RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34614fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate base models for AWAY goals\n",
    "away_base_models = {}\n",
    "away_base_predictions = {}\n",
    "away_base_metrics = {}\n",
    "\n",
    "print(\"Training Base Models for Away Goals:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    # Clone model\n",
    "    m = model.__class__(**model.get_params())\n",
    "    \n",
    "    # Train (use scaled for linear models)\n",
    "    if name in ['ridge', 'elastic_net']:\n",
    "        m.fit(X_train_scaled, y_away_train)\n",
    "        val_pred = m.predict(X_val_scaled)\n",
    "    else:\n",
    "        m.fit(X_train, y_away_train)\n",
    "        val_pred = m.predict(X_val)\n",
    "    \n",
    "    # Store\n",
    "    away_base_models[name] = m\n",
    "    away_base_predictions[name] = val_pred\n",
    "    \n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_away_val, val_pred))\n",
    "    mae = mean_absolute_error(y_away_val, val_pred)\n",
    "    away_base_metrics[name] = {'rmse': rmse, 'mae': mae}\n",
    "    \n",
    "    print(f\"  {name:15} - RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffb7b6",
   "metadata": {},
   "source": [
    "## 4. Simple Average Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d86594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple average of all predictions\n",
    "home_avg_pred = np.mean(list(home_base_predictions.values()), axis=0)\n",
    "away_avg_pred = np.mean(list(away_base_predictions.values()), axis=0)\n",
    "\n",
    "avg_home_rmse = np.sqrt(mean_squared_error(y_home_val, home_avg_pred))\n",
    "avg_away_rmse = np.sqrt(mean_squared_error(y_away_val, away_avg_pred))\n",
    "\n",
    "print(\"Simple Average Ensemble:\")\n",
    "print(f\"  Home RMSE: {avg_home_rmse:.4f}\")\n",
    "print(f\"  Away RMSE: {avg_away_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d6c3b",
   "metadata": {},
   "source": [
    "## 5. Weighted Average Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights based on inverse RMSE (better models get more weight)\n",
    "def get_inverse_weights(metrics_dict):\n",
    "    \"\"\"Calculate weights as inverse of RMSE.\"\"\"\n",
    "    rmses = np.array([m['rmse'] for m in metrics_dict.values()])\n",
    "    inv_rmse = 1 / rmses\n",
    "    weights = inv_rmse / inv_rmse.sum()  # Normalize to sum to 1\n",
    "    return dict(zip(metrics_dict.keys(), weights))\n",
    "\n",
    "home_weights = get_inverse_weights(home_base_metrics)\n",
    "away_weights = get_inverse_weights(away_base_metrics)\n",
    "\n",
    "print(\"Model Weights (based on inverse RMSE):\")\n",
    "print(\"\\nHome Goals:\")\n",
    "for name, w in home_weights.items():\n",
    "    print(f\"  {name:15}: {w:.4f}\")\n",
    "\n",
    "print(\"\\nAway Goals:\")\n",
    "for name, w in away_weights.items():\n",
    "    print(f\"  {name:15}: {w:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted average predictions\n",
    "home_weighted_pred = np.zeros(len(y_home_val))\n",
    "for name, pred in home_base_predictions.items():\n",
    "    home_weighted_pred += pred * home_weights[name]\n",
    "\n",
    "away_weighted_pred = np.zeros(len(y_away_val))\n",
    "for name, pred in away_base_predictions.items():\n",
    "    away_weighted_pred += pred * away_weights[name]\n",
    "\n",
    "weighted_home_rmse = np.sqrt(mean_squared_error(y_home_val, home_weighted_pred))\n",
    "weighted_away_rmse = np.sqrt(mean_squared_error(y_away_val, away_weighted_pred))\n",
    "\n",
    "print(\"Weighted Average Ensemble:\")\n",
    "print(f\"  Home RMSE: {weighted_home_rmse:.4f}\")\n",
    "print(f\"  Away RMSE: {weighted_away_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64df55",
   "metadata": {},
   "source": [
    "## 6. Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da98968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meta-features from base model predictions\n",
    "def create_meta_features(base_predictions):\n",
    "    \"\"\"Stack base model predictions as features for meta-learner.\"\"\"\n",
    "    return np.column_stack(list(base_predictions.values()))\n",
    "\n",
    "# Training meta-features (need to get predictions on training set via cross-val)\n",
    "print(\"Creating meta-features via cross-validation...\")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# For simplicity, we'll use validation predictions to train meta-learner\n",
    "# In production, use proper out-of-fold predictions\n",
    "\n",
    "home_meta_X = create_meta_features(home_base_predictions)\n",
    "away_meta_X = create_meta_features(away_base_predictions)\n",
    "\n",
    "print(f\"Meta-features shape: {home_meta_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea104e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train meta-learner (Ridge regression)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split validation set for meta-learner\n",
    "meta_train_idx = np.random.choice(len(home_meta_X), size=int(len(home_meta_X)*0.7), replace=False)\n",
    "meta_val_idx = np.array([i for i in range(len(home_meta_X)) if i not in meta_train_idx])\n",
    "\n",
    "home_meta_train = home_meta_X[meta_train_idx]\n",
    "home_meta_val = home_meta_X[meta_val_idx]\n",
    "y_home_meta_train = y_home_val.iloc[meta_train_idx]\n",
    "y_home_meta_val = y_home_val.iloc[meta_val_idx]\n",
    "\n",
    "away_meta_train = away_meta_X[meta_train_idx]\n",
    "away_meta_val = away_meta_X[meta_val_idx]\n",
    "y_away_meta_train = y_away_val.iloc[meta_train_idx]\n",
    "y_away_meta_val = y_away_val.iloc[meta_val_idx]\n",
    "\n",
    "# Train meta-models\n",
    "home_meta_model = Ridge(alpha=1.0)\n",
    "home_meta_model.fit(home_meta_train, y_home_meta_train)\n",
    "\n",
    "away_meta_model = Ridge(alpha=1.0)\n",
    "away_meta_model.fit(away_meta_train, y_away_meta_train)\n",
    "\n",
    "# Evaluate\n",
    "home_stack_pred = home_meta_model.predict(home_meta_val)\n",
    "away_stack_pred = away_meta_model.predict(away_meta_val)\n",
    "\n",
    "stack_home_rmse = np.sqrt(mean_squared_error(y_home_meta_val, home_stack_pred))\n",
    "stack_away_rmse = np.sqrt(mean_squared_error(y_away_meta_val, away_stack_pred))\n",
    "\n",
    "print(\"\\nStacking Ensemble (on held-out validation):\")\n",
    "print(f\"  Home RMSE: {stack_home_rmse:.4f}\")\n",
    "print(f\"  Away RMSE: {stack_away_rmse:.4f}\")\n",
    "\n",
    "# Meta-model coefficients show relative importance of each base model\n",
    "print(\"\\nMeta-model Coefficients (Home):\")\n",
    "for name, coef in zip(home_base_models.keys(), home_meta_model.coef_):\n",
    "    print(f\"  {name:15}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01f996",
   "metadata": {},
   "source": [
    "## 7. Sklearn Stacking Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea009325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define estimators for StackingRegressor\n",
    "estimators = [\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)),\n",
    "]\n",
    "\n",
    "# Create stacking ensemble for home goals\n",
    "home_stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Create stacking ensemble for away goals\n",
    "away_stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"Training Sklearn Stacking Ensemble...\")\n",
    "X_full_train = pd.concat([X_train, X_val])\n",
    "y_home_full = pd.concat([y_home_train, y_home_val])\n",
    "y_away_full = pd.concat([y_away_train, y_away_val])\n",
    "\n",
    "home_stacking.fit(X_full_train, y_home_full)\n",
    "away_stacking.fit(X_full_train, y_away_full)\n",
    "\n",
    "print(\"Stacking models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23946138",
   "metadata": {},
   "source": [
    "## 8. Final Ensemble Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfb842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final ensemble class\n",
    "class WeightedEnsemble:\n",
    "    \"\"\"Weighted ensemble of base models.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_models, weights, scaler=None, linear_models=None):\n",
    "        self.base_models = base_models\n",
    "        self.weights = weights\n",
    "        self.scaler = scaler\n",
    "        self.linear_models = linear_models or ['ridge', 'elastic_net']\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X) if self.scaler else X\n",
    "        \n",
    "        pred = np.zeros(len(X))\n",
    "        for name, model in self.base_models.items():\n",
    "            if name in self.linear_models:\n",
    "                pred += model.predict(X_scaled) * self.weights[name]\n",
    "            else:\n",
    "                pred += model.predict(X) * self.weights[name]\n",
    "        return pred\n",
    "\n",
    "# Create final ensembles\n",
    "final_home_ensemble = WeightedEnsemble(home_base_models, home_weights, scaler)\n",
    "final_away_ensemble = WeightedEnsemble(away_base_models, away_weights, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all approaches on test set\n",
    "results = []\n",
    "\n",
    "# Best individual model (XGBoost or Random Forest)\n",
    "best_name = min(home_base_metrics, key=lambda x: home_base_metrics[x]['rmse'])\n",
    "if best_name in ['ridge', 'elastic_net']:\n",
    "    best_home_pred = home_base_models[best_name].predict(X_test_scaled)\n",
    "    best_away_pred = away_base_models[best_name].predict(X_test_scaled)\n",
    "else:\n",
    "    best_home_pred = home_base_models[best_name].predict(X_test)\n",
    "    best_away_pred = away_base_models[best_name].predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'method': f'Best Single ({best_name})',\n",
    "    'home_rmse': np.sqrt(mean_squared_error(y_home_test, best_home_pred)),\n",
    "    'away_rmse': np.sqrt(mean_squared_error(y_away_test, best_away_pred)),\n",
    "})\n",
    "\n",
    "# Simple Average\n",
    "all_home_preds = []\n",
    "all_away_preds = []\n",
    "for name, model in home_base_models.items():\n",
    "    if name in ['ridge', 'elastic_net']:\n",
    "        all_home_preds.append(model.predict(X_test_scaled))\n",
    "    else:\n",
    "        all_home_preds.append(model.predict(X_test))\n",
    "for name, model in away_base_models.items():\n",
    "    if name in ['ridge', 'elastic_net']:\n",
    "        all_away_preds.append(model.predict(X_test_scaled))\n",
    "    else:\n",
    "        all_away_preds.append(model.predict(X_test))\n",
    "\n",
    "avg_home = np.mean(all_home_preds, axis=0)\n",
    "avg_away = np.mean(all_away_preds, axis=0)\n",
    "\n",
    "results.append({\n",
    "    'method': 'Simple Average',\n",
    "    'home_rmse': np.sqrt(mean_squared_error(y_home_test, avg_home)),\n",
    "    'away_rmse': np.sqrt(mean_squared_error(y_away_test, avg_away)),\n",
    "})\n",
    "\n",
    "# Weighted Average\n",
    "weighted_home = final_home_ensemble.predict(X_test)\n",
    "weighted_away = final_away_ensemble.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'method': 'Weighted Average',\n",
    "    'home_rmse': np.sqrt(mean_squared_error(y_home_test, weighted_home)),\n",
    "    'away_rmse': np.sqrt(mean_squared_error(y_away_test, weighted_away)),\n",
    "})\n",
    "\n",
    "# Stacking\n",
    "stack_home = home_stacking.predict(X_test)\n",
    "stack_away = away_stacking.predict(X_test)\n",
    "\n",
    "results.append({\n",
    "    'method': 'Stacking',\n",
    "    'home_rmse': np.sqrt(mean_squared_error(y_home_test, stack_home)),\n",
    "    'away_rmse': np.sqrt(mean_squared_error(y_away_test, stack_away)),\n",
    "})\n",
    "\n",
    "# Show results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['combined_rmse'] = (results_df['home_rmse'] + results_df['away_rmse']) / 2\n",
    "results_df = results_df.sort_values('combined_rmse')\n",
    "\n",
    "print(\"\\n Ensemble Comparison on Test Set\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfff063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, results_df['home_rmse'], width, label='Home RMSE')\n",
    "bars2 = ax.bar(x + width/2, results_df['away_rmse'], width, label='Away RMSE')\n",
    "\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Ensemble Methods Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['method'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02c416",
   "metadata": {},
   "source": [
    "## 9. Save Best Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best ensemble method\n",
    "best_method = results_df.iloc[0]['method']\n",
    "print(f\"Best ensemble method: {best_method}\")\n",
    "\n",
    "# Prepare output directory\n",
    "output_dir = '../../output/models'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stacking models (most flexible)\n",
    "with open(f'{output_dir}/ensemble_stacking_home.pkl', 'wb') as f:\n",
    "    pickle.dump(home_stacking, f)\n",
    "\n",
    "with open(f'{output_dir}/ensemble_stacking_away.pkl', 'wb') as f:\n",
    "    pickle.dump(away_stacking, f)\n",
    "\n",
    "# Save weighted ensemble components\n",
    "ensemble_data = {\n",
    "    'home_models': home_base_models,\n",
    "    'away_models': away_base_models,\n",
    "    'home_weights': home_weights,\n",
    "    'away_weights': away_weights,\n",
    "    'scaler': scaler,\n",
    "    'linear_models': ['ridge', 'elastic_net'],\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}/ensemble_weighted.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_data, f)\n",
    "\n",
    "# Save model info\n",
    "best_results = results_df.iloc[0]\n",
    "model_info = {\n",
    "    'model_type': 'Ensemble',\n",
    "    'best_method': best_method,\n",
    "    'base_models': list(base_models.keys()),\n",
    "    'home_weights': {k: float(v) for k, v in home_weights.items()},\n",
    "    'away_weights': {k: float(v) for k, v in away_weights.items()},\n",
    "    'test_metrics': {\n",
    "        'home_rmse': float(best_results['home_rmse']),\n",
    "        'away_rmse': float(best_results['away_rmse']),\n",
    "        'combined_rmse': float(best_results['combined_rmse']),\n",
    "    },\n",
    "    'all_results': results_df.to_dict('records'),\n",
    "    'feature_cols': feature_cols,\n",
    "    'trained_at': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(f'{output_dir}/ensemble_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nModels saved to {output_dir}/\")\n",
    "print(f\"  - ensemble_stacking_home.pkl\")\n",
    "print(f\"  - ensemble_stacking_away.pkl\")\n",
    "print(f\"  - ensemble_weighted.pkl\")\n",
    "print(f\"  - ensemble_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3139099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" ENSEMBLE TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Method: {best_method}\")\n",
    "print(f\"\\nBase Models Used: {list(base_models.keys())}\")\n",
    "print(f\"\\nFinal Test Performance ({best_method}):\")\n",
    "print(f\"  Home RMSE: {best_results['home_rmse']:.4f}\")\n",
    "print(f\"  Away RMSE: {best_results['away_rmse']:.4f}\")\n",
    "print(f\"  Combined RMSE: {best_results['combined_rmse']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

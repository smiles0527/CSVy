{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eeeaec",
   "metadata": {},
   "source": [
    "# Elo Model Training & Hyperparameter Tuning\n",
    "\n",
    "This notebook:\n",
    "1. Loads WHL 2025 data and aggregates shifts â†’ games (sum for goals)\n",
    "2. Runs grid search over Elo hyperparameters (K-factor, home advantage, MOV, rest, B2B)\n",
    "3. Fine-tunes top parameters with a narrow sweep\n",
    "4. Evaluates with combined RMSE (home + away), win accuracy â€” comparable to baseline\n",
    "5. Generates Round 1 predictions from `WHSDSC_Rnd1_matchups.xlsx`\n",
    "6. Saves all outputs to `output/predictions/elo/` and `output/models/elo/`\n",
    "\n",
    "**Baseline benchmark: Ensemble RMSE 1.8071, Win Acc 55.9%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json, os, sys, pathlib, pickle\n",
    "\n",
    "# ---------- resolve python/ directory & set CWD ----------\n",
    "_cwd = pathlib.Path(os.path.abspath('')).resolve()\n",
    "if (_cwd / 'python').is_dir():\n",
    "    _python_dir = _cwd / 'python'                          # workspace root\n",
    "elif _cwd.name == 'elo' and (_cwd.parent.parent / 'data').is_dir():\n",
    "    _python_dir = _cwd.parent.parent                       # training/elo/\n",
    "elif _cwd.name == 'training' and (_cwd.parent / 'data').is_dir():\n",
    "    _python_dir = _cwd.parent                              # training/\n",
    "elif (_cwd / 'data').is_dir():\n",
    "    _python_dir = _cwd                                     # already in python/\n",
    "else:\n",
    "    raise RuntimeError(f'Cannot locate python/ directory from {_cwd}')\n",
    "\n",
    "os.chdir(_python_dir)\n",
    "sys.path.insert(0, str(_python_dir))\n",
    "\n",
    "from utils.elo_model import EloModel\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print(f'CWD: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load & aggregate raw shift-level data to game level â”€â”€\n",
    "raw = pd.read_csv('data/whl_2025.csv')\n",
    "print(f'Raw: {len(raw)} shift rows')\n",
    "\n",
    "games_df = raw.groupby('game_id').agg(\n",
    "    home_team  = ('home_team', 'first'),\n",
    "    away_team  = ('away_team', 'first'),\n",
    "    home_goals = ('home_goals', 'sum'),\n",
    "    away_goals = ('away_goals', 'sum'),\n",
    "    went_ot    = ('went_ot', 'max'),\n",
    ").reset_index()\n",
    "\n",
    "# Sort by game_id (chronological)\n",
    "games_df['game_num'] = games_df['game_id'].str.extract(r'(\\d+)').astype(int)\n",
    "games_df = games_df.sort_values('game_num').reset_index(drop=True)\n",
    "\n",
    "print(f'Games: {len(games_df)}')\n",
    "print(f'Teams: {games_df[\"home_team\"].nunique()}')\n",
    "print(f'Avg home goals: {games_df[\"home_goals\"].mean():.2f}, Avg away goals: {games_df[\"away_goals\"].mean():.2f}')\n",
    "print(f'Home win rate: {(games_df[\"home_goals\"] > games_df[\"away_goals\"]).mean():.1%}')\n",
    "games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b34c4",
   "metadata": {},
   "source": [
    "## Train/Test Split & Grid Search Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 chronological split (same as baseline)\n",
    "split_idx = int(len(games_df) * 0.8)\n",
    "train_df = games_df.iloc[:split_idx].copy()\n",
    "test_df  = games_df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f'Train: {len(train_df)} games  |  Test: {len(test_df)} games')\n",
    "\n",
    "# â”€â”€ Build parameter grid â”€â”€\n",
    "param_grid = dict(\n",
    "    k_factor              = [20, 32, 40],\n",
    "    home_advantage        = [50, 100, 150],\n",
    "    mov_multiplier        = [0.0, 1.0, 1.5],\n",
    "    mov_method            = ['logarithmic'],      # keep constant for grid\n",
    "    initial_rating        = [1500],\n",
    "    season_carryover      = [0.75],               # single-season data\n",
    "    ot_win_multiplier     = [0.75],\n",
    "    rest_advantage_per_day= [0, 10],\n",
    "    b2b_penalty           = [0, 50],\n",
    ")\n",
    "\n",
    "# Generate all combos\n",
    "keys = list(param_grid.keys())\n",
    "combos = list(product(*[param_grid[k] for k in keys]))\n",
    "print(f'Grid: {len(combos)} configurations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Grid Search â”€â”€\n",
    "results = []\n",
    "\n",
    "for i, vals in enumerate(combos):\n",
    "    params = dict(zip(keys, vals))\n",
    "    label = f\"Elo(k={params['k_factor']},ha={params['home_advantage']},mov={params['mov_multiplier']},rest={params['rest_advantage_per_day']},b2b={params['b2b_penalty']})\"\n",
    "    \n",
    "    try:\n",
    "        model = EloModel(params)\n",
    "        model.fit(train_df)\n",
    "        metrics = model.evaluate(test_df)\n",
    "        results.append({'config': label, **params, **metrics})\n",
    "    except Exception as e:\n",
    "        print(f'  FAILED {label}: {e}')\n",
    "\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f'  {i+1}/{len(combos)} done')\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('combined_rmse')\n",
    "print(f'\\nâœ… {len(results_df)} configs evaluated')\n",
    "print(f'Best combined RMSE: {results_df[\"combined_rmse\"].min():.4f}')\n",
    "print(f'Win accuracy range: {results_df[\"win_accuracy\"].min():.1%} â€“ {results_df[\"win_accuracy\"].max():.1%}')\n",
    "results_df.head(10)[['config','home_rmse','away_rmse','combined_rmse','win_accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c33f81",
   "metadata": {},
   "source": [
    "## Save Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83184f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full comparison (like baseline_comparison.csv)\n",
    "os.makedirs('output/predictions/elo', exist_ok=True)\n",
    "os.makedirs('output/models/elo', exist_ok=True)\n",
    "\n",
    "results_df.to_csv('output/predictions/elo/elo_comparison.csv', index=False)\n",
    "print(f'âœ… Saved {len(results_df)} configs â†’ output/predictions/elo/elo_comparison.csv')\n",
    "print(f'\\nTop 5:')\n",
    "print(results_df.head()[['config','combined_rmse','win_accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bcd81",
   "metadata": {},
   "source": [
    "## Hyperparameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Which hyperparameters matter most? â”€â”€\n",
    "param_cols = ['k_factor', 'home_advantage', 'mov_multiplier', 'rest_advantage_per_day', 'b2b_penalty']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(param_cols), figsize=(18, 4), sharey=True)\n",
    "for ax, col in zip(axes, param_cols):\n",
    "    grouped = results_df.groupby(col)['combined_rmse'].mean()\n",
    "    ax.bar(range(len(grouped)), grouped.values, tick_label=[str(v) for v in grouped.index])\n",
    "    ax.set_title(col, fontsize=10)\n",
    "    ax.set_ylabel('Mean Combined RMSE' if col == param_cols[0] else '')\n",
    "plt.suptitle('Hyperparameter Sensitivity (lower = better)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with RMSE\n",
    "corr = results_df[param_cols + ['combined_rmse']].corr()['combined_rmse'].drop('combined_rmse')\n",
    "print('Correlation with combined_rmse:')\n",
    "for p, c in corr.abs().sort_values(ascending=False).items():\n",
    "    direction = 'â†‘' if corr[p] > 0 else 'â†“'\n",
    "    print(f'  {p:30s}  r={corr[p]:+.3f}  ({direction} higher param â†’ {\"worse\" if corr[p]>0 else \"better\"} RMSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5f6ff",
   "metadata": {},
   "source": [
    "## Fine-Grained Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow sweep around best grid values\n",
    "best_row = results_df.iloc[0]\n",
    "print(f'Grid search best: combined_rmse={best_row[\"combined_rmse\"]:.4f}')\n",
    "print(f'  k_factor={best_row[\"k_factor\"]}, home_advantage={best_row[\"home_advantage\"]}, '\n",
    "      f'mov={best_row[\"mov_multiplier\"]}, rest={best_row[\"rest_advantage_per_day\"]}, b2b={best_row[\"b2b_penalty\"]}')\n",
    "\n",
    "# Fine sweep around best values\n",
    "best_k  = int(best_row['k_factor'])\n",
    "best_ha = int(best_row['home_advantage'])\n",
    "best_mov = float(best_row['mov_multiplier'])\n",
    "best_rest = int(best_row['rest_advantage_per_day'])\n",
    "best_b2b = int(best_row['b2b_penalty'])\n",
    "\n",
    "fine_grid = dict(\n",
    "    k_factor              = list(range(max(10, best_k - 10), best_k + 12, 2)),\n",
    "    home_advantage        = list(range(max(0, best_ha - 50), best_ha + 60, 10)),\n",
    "    mov_multiplier        = [max(0, best_mov - 0.5), best_mov - 0.25, best_mov, best_mov + 0.25, best_mov + 0.5],\n",
    "    mov_method            = ['logarithmic'],\n",
    "    initial_rating        = [1500],\n",
    "    season_carryover      = [0.75],\n",
    "    ot_win_multiplier     = [0.75],\n",
    "    rest_advantage_per_day= [max(0, best_rest - 5), best_rest, best_rest + 5],\n",
    "    b2b_penalty           = [max(0, best_b2b - 25), best_b2b, best_b2b + 25],\n",
    ")\n",
    "\n",
    "fine_keys = list(fine_grid.keys())\n",
    "fine_combos = list(product(*[fine_grid[k] for k in fine_keys]))\n",
    "print(f'\\nFine sweep: {len(fine_combos)} configurations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8d285",
   "metadata": {},
   "source": [
    "## Run Fine Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_results = []\n",
    "\n",
    "for i, vals in enumerate(fine_combos):\n",
    "    params = dict(zip(fine_keys, vals))\n",
    "    try:\n",
    "        model = EloModel(params)\n",
    "        model.fit(train_df)\n",
    "        metrics = model.evaluate(test_df)\n",
    "        fine_results.append({**params, **metrics})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f'  {i+1}/{len(fine_combos)} done')\n",
    "\n",
    "fine_df = pd.DataFrame(fine_results).sort_values('combined_rmse')\n",
    "print(f'\\nâœ… Fine sweep done â€” {len(fine_df)} configs')\n",
    "\n",
    "best_fine = fine_df.iloc[0]\n",
    "print(f'\\nBest from fine sweep:')\n",
    "print(f'  combined_rmse = {best_fine[\"combined_rmse\"]:.4f}')\n",
    "print(f'  home_rmse     = {best_fine[\"home_rmse\"]:.4f}')\n",
    "print(f'  away_rmse     = {best_fine[\"away_rmse\"]:.4f}')\n",
    "print(f'  win_accuracy  = {best_fine[\"win_accuracy\"]:.1%}')\n",
    "print(f'  k_factor={best_fine[\"k_factor\"]}, home_advantage={best_fine[\"home_advantage\"]}, '\n",
    "      f'mov={best_fine[\"mov_multiplier\"]}, rest={best_fine[\"rest_advantage_per_day\"]}, b2b={best_fine[\"b2b_penalty\"]}')\n",
    "\n",
    "# Compare to baseline\n",
    "print(f'\\nðŸ“Š vs Baseline ensemble: 1.8071 RMSE, 55.9% win acc')\n",
    "improvement = 1.8071 - best_fine['combined_rmse']\n",
    "print(f'   Elo improvement: {improvement:+.4f} RMSE ({\"BETTER\" if improvement > 0 else \"not yet beating baseline\"})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56158d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualize fine sweep results â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE distribution\n",
    "axes[0].hist(fine_df['combined_rmse'].dropna(), bins=40, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(best_fine['combined_rmse'], color='red', ls='--', lw=2,\n",
    "                label=f'Best: {best_fine[\"combined_rmse\"]:.4f}')\n",
    "axes[0].axvline(1.8071, color='green', ls='--', lw=2, label='Baseline: 1.8071')\n",
    "axes[0].set_xlabel('Combined RMSE')\n",
    "axes[0].set_title('Fine Sweep RMSE Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# K-factor vs RMSE\n",
    "k_rmse = fine_df.groupby('k_factor')['combined_rmse'].mean()\n",
    "axes[1].plot(k_rmse.index, k_rmse.values, 'o-', color='steelblue')\n",
    "axes[1].set_xlabel('K-factor')\n",
    "axes[1].set_ylabel('Mean Combined RMSE')\n",
    "axes[1].set_title('K-factor Sensitivity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98ef64",
   "metadata": {},
   "source": [
    "## Train Final Model & Team Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train final model on ALL data with best params â”€â”€\n",
    "best_params = {k: best_fine[k] for k in fine_keys}\n",
    "# Clean up numpy types\n",
    "best_params = {k: (int(v) if isinstance(v, (np.integer,)) else\n",
    "                   float(v) if isinstance(v, (np.floating,)) else v)\n",
    "               for k, v in best_params.items()}\n",
    "\n",
    "final_model = EloModel(best_params)\n",
    "final_model.fit(games_df)\n",
    "\n",
    "# Also evaluate on the test split for recording\n",
    "eval_model = EloModel(best_params)\n",
    "eval_model.fit(train_df)\n",
    "final_metrics = eval_model.evaluate(test_df)\n",
    "\n",
    "print('Final Elo model trained on all', len(games_df), 'games')\n",
    "print(f'\\nTest-set metrics:')\n",
    "print(f'  Home RMSE:     {final_metrics[\"home_rmse\"]:.4f}')\n",
    "print(f'  Away RMSE:     {final_metrics[\"away_rmse\"]:.4f}')\n",
    "print(f'  Combined RMSE: {final_metrics[\"combined_rmse\"]:.4f}')\n",
    "print(f'  Win Accuracy:  {final_metrics[\"win_accuracy\"]:.1%}')\n",
    "\n",
    "print(f'\\nðŸ† Team Rankings (top 10):')\n",
    "for rank, (team, rating) in enumerate(final_model.get_rankings(10), 1):\n",
    "    print(f'  {rank:2d}. {team:20s}  {rating:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Rating distribution visualization â”€â”€\n",
    "rankings = final_model.get_rankings()\n",
    "teams_r, ratings_r = zip(*rankings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['#2ecc71' if r > 1500 else '#e74c3c' if r < 1500 else '#95a5a6' for r in ratings_r]\n",
    "ax.barh(range(len(teams_r)), ratings_r, color=colors, edgecolor='black', linewidth=0.3)\n",
    "ax.set_yticks(range(len(teams_r)))\n",
    "ax.set_yticklabels(teams_r, fontsize=9)\n",
    "ax.axvline(1500, color='gray', ls='--', lw=1, label='Initial (1500)')\n",
    "ax.set_xlabel('Elo Rating')\n",
    "ax.set_title('Final Elo Ratings â€” All 32 Teams')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a7dca",
   "metadata": {},
   "source": [
    "## Round 1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load Round 1 matchups and predict â”€â”€\n",
    "matchups = pd.read_excel('data/WHSDSC_Rnd1_matchups.xlsx')\n",
    "print(f'{len(matchups)} matchups loaded')\n",
    "\n",
    "home_col = [c for c in matchups.columns if 'home' in c.lower()][0]\n",
    "away_col = [c for c in matchups.columns if 'away' in c.lower()][0]\n",
    "\n",
    "preds = []\n",
    "for i, row in matchups.iterrows():\n",
    "    game = {'home_team': row[home_col], 'away_team': row[away_col]}\n",
    "    h_goals, a_goals = final_model.predict_goals(game)\n",
    "    winner, conf = final_model.predict_winner(game)\n",
    "    preds.append({\n",
    "        'game': i + 1,\n",
    "        'home_team': row[home_col],\n",
    "        'away_team': row[away_col],\n",
    "        'pred_home_goals': round(h_goals, 2),\n",
    "        'pred_away_goals': round(a_goals, 2),\n",
    "        'predicted_winner': winner,\n",
    "        'confidence': round(conf, 4),\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(preds)\n",
    "print(f'\\nRound 1 Elo Predictions:')\n",
    "print(pred_df.to_string(index=False))\n",
    "print(f'\\nAvg confidence: {pred_df[\"confidence\"].mean():.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3cc7c0",
   "metadata": {},
   "source": [
    "## Save All Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f080f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1. Predictions CSV â”€â”€\n",
    "pred_df.to_csv('output/predictions/elo/round1_elo_predictions.csv', index=False)\n",
    "print('âœ… output/predictions/elo/round1_elo_predictions.csv')\n",
    "\n",
    "# â”€â”€ 2. Model pickle â”€â”€\n",
    "final_model.save_model('output/models/elo/best_elo')\n",
    "print('âœ… output/models/elo/best_elo.pkl + .json')\n",
    "\n",
    "# â”€â”€ 3. Pipeline summary JSON â”€â”€\n",
    "summary = {\n",
    "    'model': 'Elo',\n",
    "    'grid_search_configs': len(results_df),\n",
    "    'fine_sweep_configs': len(fine_df),\n",
    "    'best_params': best_params,\n",
    "    'test_metrics': {k: round(v, 6) if isinstance(v, float) else v\n",
    "                     for k, v in final_metrics.items()},\n",
    "    'baseline_comparison': {\n",
    "        'baseline_ensemble_rmse': 1.8071,\n",
    "        'elo_rmse': round(final_metrics['combined_rmse'], 4),\n",
    "        'improvement': round(1.8071 - final_metrics['combined_rmse'], 4),\n",
    "    },\n",
    "    'team_rankings': {team: round(r, 1) for team, r in final_model.get_rankings()},\n",
    "    'predictions': preds,\n",
    "}\n",
    "\n",
    "with open('output/predictions/elo/elo_pipeline_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print('âœ… output/predictions/elo/elo_pipeline_summary.json')\n",
    "\n",
    "print(f'\\nðŸ† FINAL SUMMARY')\n",
    "print(f'   Combined RMSE: {final_metrics[\"combined_rmse\"]:.4f}')\n",
    "print(f'   Win Accuracy:  {final_metrics[\"win_accuracy\"]:.1%}')\n",
    "print(f'   Best params:   k={best_params[\"k_factor\"]}, ha={best_params[\"home_advantage\"]}, '\n",
    "      f'mov={best_params[\"mov_multiplier\"]}, rest={best_params[\"rest_advantage_per_day\"]}, '\n",
    "      f'b2b={best_params[\"b2b_penalty\"]}')\n",
    "print(f'   vs Baseline:   {\"BETTER\" if final_metrics[\"combined_rmse\"] < 1.8071 else \"NOT YET BEATING\"} (1.8071)')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0595f0c",
   "metadata": {},
   "source": [
    "# ELO Rating System for Hockey Game Prediction\n",
    "\n",
    "This notebook demonstrates how to use the ELO rating system to predict hockey game outcomes. The ELO system, originally developed for chess, has been adapted for team sports with additional adjustments for:\n",
    "\n",
    "- Home ice advantage\n",
    "- Rest and fatigue (back-to-back penalties)\n",
    "- Travel distance\n",
    "- Injury impacts\n",
    "- Margin of victory\n",
    "- Division-based initialization\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Setup and Imports\n",
    "2. Understanding the ELO Formula\n",
    "3. Loading and Preparing Data\n",
    "4. Training the Model\n",
    "5. Evaluating Performance\n",
    "6. Making Predictions\n",
    "7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd0148",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e66801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c678251",
   "metadata": {},
   "source": [
    "## 2. The ELO Model Class\n",
    "\n",
    "The ELO rating system works by:\n",
    "1. Assigning each team an initial rating (default: 1500)\n",
    "2. After each game, updating ratings based on the expected vs actual outcome\n",
    "3. Teams gain rating points for wins and lose points for losses\n",
    "4. The magnitude of change depends on the K-factor and margin of victory\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Expected Score:**\n",
    "$$E_A = \\frac{1}{1 + 10^{(R_B - R_A) / 400}}$$\n",
    "\n",
    "**Rating Update:**\n",
    "$$R'_A = R_A + K \\times (S_A - E_A)$$\n",
    "\n",
    "Where:\n",
    "- $R_A$, $R_B$ = Current ratings of teams A and B\n",
    "- $E_A$ = Expected score (win probability) for team A\n",
    "- $S_A$ = Actual score (1 for win, 0 for loss)\n",
    "- $K$ = K-factor controlling rating volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EloModel:\n",
    "    \"\"\"\n",
    "    ELO Rating System for Hockey Game Prediction.\n",
    "    \n",
    "    Supports contextual adjustments for home advantage, rest,\n",
    "    travel fatigue, injuries, and margin of victory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Initialize model with hyperparameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dict\n",
    "            k_factor : float\n",
    "                Rating change rate (typical: 20-40)\n",
    "            home_advantage : float\n",
    "                Home ice boost in rating points (typical: 50-150)\n",
    "            initial_rating : float\n",
    "                Starting rating for all teams (default: 1500)\n",
    "            mov_multiplier : float\n",
    "                Margin of victory weight (0 = disabled)\n",
    "            mov_method : str\n",
    "                'linear' or 'logarithmic' scaling for margin\n",
    "            rest_advantage_per_day : float\n",
    "                Rating boost per day of rest differential\n",
    "            b2b_penalty : float\n",
    "                Penalty for back-to-back games (rest <= 1 day)\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.ratings = {}\n",
    "        self.rating_history = []\n",
    "    \n",
    "    def initialize_ratings(self, teams, divisions=None):\n",
    "        \"\"\"Initialize team ratings, optionally by division tier.\"\"\"\n",
    "        initial = self.params.get('initial_rating', 1500)\n",
    "        division_ratings = {\n",
    "            'D1': initial + 100,\n",
    "            'D2': initial,\n",
    "            'D3': initial - 100\n",
    "        }\n",
    "        \n",
    "        for i, team in enumerate(teams):\n",
    "            if divisions is not None and i < len(divisions):\n",
    "                div = divisions.iloc[i] if hasattr(divisions, 'iloc') else divisions[i]\n",
    "                self.ratings[team] = division_ratings.get(div, initial)\n",
    "            else:\n",
    "                self.ratings[team] = initial\n",
    "    \n",
    "    def calculate_expected_score(self, team_elo, opponent_elo):\n",
    "        \"\"\"Calculate expected win probability using ELO formula.\"\"\"\n",
    "        return 1 / (1 + 10 ** ((opponent_elo - team_elo) / 400))\n",
    "    \n",
    "    def calculate_mov_multiplier(self, goal_diff):\n",
    "        \"\"\"Calculate margin of victory multiplier.\"\"\"\n",
    "        mov = self.params.get('mov_multiplier', 0)\n",
    "        if mov == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        if self.params.get('mov_method', 'logarithmic') == 'linear':\n",
    "            return 1 + (abs(goal_diff) * mov)\n",
    "        return 1 + (np.log(abs(goal_diff) + 1) * mov)\n",
    "    \n",
    "    def adjust_for_context(self, team_elo, is_home, rest_time, travel_dist, injuries):\n",
    "        \"\"\"Apply contextual adjustments to ELO rating.\"\"\"\n",
    "        adjusted = team_elo\n",
    "        \n",
    "        if is_home:\n",
    "            adjusted += self.params.get('home_advantage', 0)\n",
    "        \n",
    "        if rest_time <= 1:\n",
    "            adjusted -= self.params.get('b2b_penalty', 0)\n",
    "        \n",
    "        if not is_home and travel_dist > 0:\n",
    "            adjusted -= (travel_dist / 1000) * 15\n",
    "        \n",
    "        adjusted -= injuries * 25\n",
    "        \n",
    "        return adjusted\n",
    "    \n",
    "    def update_ratings(self, game):\n",
    "        \"\"\"Update team ratings after a game.\"\"\"\n",
    "        home_team = game['home_team']\n",
    "        away_team = game['away_team']\n",
    "        \n",
    "        home_elo = self.ratings.get(home_team, 1500)\n",
    "        away_elo = self.ratings.get(away_team, 1500)\n",
    "        \n",
    "        # Get context values with defaults\n",
    "        home_rest = game.get('home_rest', 2)\n",
    "        away_rest = game.get('away_rest', 2)\n",
    "        away_travel = game.get('away_travel_dist', game.get('travel_distance', 0))\n",
    "        home_injuries = game.get('home_injuries', 0)\n",
    "        away_injuries = game.get('away_injuries', 0)\n",
    "        \n",
    "        # Apply contextual adjustments\n",
    "        home_adj = self.adjust_for_context(home_elo, True, home_rest, 0, home_injuries)\n",
    "        away_adj = self.adjust_for_context(away_elo, False, away_rest, away_travel, away_injuries)\n",
    "        \n",
    "        # Rest differential advantage\n",
    "        rest_diff = home_rest - away_rest\n",
    "        home_adj += rest_diff * self.params.get('rest_advantage_per_day', 0)\n",
    "        \n",
    "        # Calculate expected score\n",
    "        home_expected = self.calculate_expected_score(home_adj, away_adj)\n",
    "        \n",
    "        # Determine actual outcome\n",
    "        home_actual = 1.0 if game['home_goals'] > game['away_goals'] else 0.0\n",
    "        \n",
    "        # Calculate margin of victory multiplier\n",
    "        goal_diff = game['home_goals'] - game['away_goals']\n",
    "        mov_mult = self.calculate_mov_multiplier(goal_diff)\n",
    "        \n",
    "        # Update ratings\n",
    "        k = self.params.get('k_factor', 32) * mov_mult\n",
    "        self.ratings[home_team] = home_elo + k * (home_actual - home_expected)\n",
    "        self.ratings[away_team] = away_elo + k * ((1 - home_actual) - (1 - home_expected))\n",
    "        \n",
    "        # Store history\n",
    "        self.rating_history.append({\n",
    "            'home_team': home_team,\n",
    "            'away_team': away_team,\n",
    "            'home_rating': self.ratings[home_team],\n",
    "            'away_rating': self.ratings[away_team]\n",
    "        })\n",
    "    \n",
    "    def predict_goals(self, game):\n",
    "        \"\"\"Predict goals for both teams.\"\"\"\n",
    "        home_team = game['home_team']\n",
    "        away_team = game['away_team']\n",
    "        \n",
    "        home_elo = self.ratings.get(home_team, 1500)\n",
    "        away_elo = self.ratings.get(away_team, 1500)\n",
    "        \n",
    "        home_rest = game.get('home_rest', 2)\n",
    "        away_rest = game.get('away_rest', 2)\n",
    "        away_travel = game.get('away_travel_dist', game.get('travel_distance', 0))\n",
    "        home_injuries = game.get('home_injuries', 0)\n",
    "        away_injuries = game.get('away_injuries', 0)\n",
    "        \n",
    "        home_adj = self.adjust_for_context(home_elo, True, home_rest, 0, home_injuries)\n",
    "        away_adj = self.adjust_for_context(away_elo, False, away_rest, away_travel, away_injuries)\n",
    "        \n",
    "        rest_diff = home_rest - away_rest\n",
    "        home_adj += rest_diff * self.params.get('rest_advantage_per_day', 0)\n",
    "        \n",
    "        home_win_prob = self.calculate_expected_score(home_adj, away_adj)\n",
    "        expected_diff = (home_win_prob - 0.5) * 12\n",
    "        \n",
    "        home_goals = 3.0 + (expected_diff / 2)\n",
    "        away_goals = 3.0 - (expected_diff / 2)\n",
    "        \n",
    "        return home_goals, away_goals\n",
    "    \n",
    "    def fit(self, games_df):\n",
    "        \"\"\"Train the model on historical games.\"\"\"\n",
    "        teams = pd.concat([games_df['home_team'], games_df['away_team']]).unique()\n",
    "        \n",
    "        if 'division' in games_df.columns:\n",
    "            divisions = games_df.groupby('home_team')['division'].first()\n",
    "            self.initialize_ratings(teams, divisions)\n",
    "        else:\n",
    "            self.initialize_ratings(teams)\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            self.update_ratings(game)\n",
    "    \n",
    "    def evaluate(self, games_df):\n",
    "        \"\"\"Evaluate model on test set.\"\"\"\n",
    "        predictions, actuals = [], []\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            home_pred, _ = self.predict_goals(game)\n",
    "            predictions.append(home_pred)\n",
    "            actuals.append(game['home_goals'])\n",
    "        \n",
    "        rmse = mean_squared_error(actuals, predictions, squared=False)\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        r2 = r2_score(actuals, predictions) if len(set(actuals)) > 1 else 0.0\n",
    "        \n",
    "        return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "    \n",
    "    def get_rankings(self, top_n=None):\n",
    "        \"\"\"Get team rankings sorted by ELO rating.\"\"\"\n",
    "        sorted_ratings = sorted(self.ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_ratings[:top_n] if top_n else sorted_ratings\n",
    "\n",
    "print(\"EloModel class defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a04767",
   "metadata": {},
   "source": [
    "## 3. Loading and Preparing Data\n",
    "\n",
    "The model expects a DataFrame with the following columns:\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| home_team | str | Home team identifier |\n",
    "| away_team | str | Away team identifier |\n",
    "| home_goals | int | Goals scored by home team |\n",
    "| away_goals | int | Goals scored by away team |\n",
    "| game_date | date | Game date (for chronological ordering) |\n",
    "| home_rest | int | Days since home team's last game (optional) |\n",
    "| away_rest | int | Days since away team's last game (optional) |\n",
    "| travel_distance | float | Away team travel distance in miles (optional) |\n",
    "| division | str | Team division: D1, D2, or D3 (optional) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029536ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstration\n",
    "# In practice, replace this with: games_df = pd.read_csv('path/to/hockey_data.csv')\n",
    "\n",
    "sample_data = {\n",
    "    'game_date': pd.date_range('2025-10-01', periods=20, freq='2D'),\n",
    "    'home_team': ['Team A', 'Team B', 'Team C', 'Team D', 'Team A',\n",
    "                  'Team B', 'Team C', 'Team D', 'Team A', 'Team B',\n",
    "                  'Team C', 'Team D', 'Team A', 'Team B', 'Team C',\n",
    "                  'Team D', 'Team A', 'Team B', 'Team C', 'Team D'],\n",
    "    'away_team': ['Team B', 'Team C', 'Team D', 'Team A', 'Team C',\n",
    "                  'Team D', 'Team A', 'Team B', 'Team D', 'Team A',\n",
    "                  'Team B', 'Team C', 'Team B', 'Team C', 'Team D',\n",
    "                  'Team A', 'Team C', 'Team D', 'Team A', 'Team B'],\n",
    "    'home_goals': [3, 2, 4, 1, 5, 2, 3, 4, 2, 3, 4, 1, 3, 2, 5, 2, 4, 3, 2, 3],\n",
    "    'away_goals': [2, 3, 1, 4, 2, 3, 2, 2, 4, 1, 2, 3, 1, 4, 2, 3, 1, 2, 3, 2],\n",
    "    'home_rest': [3, 2, 4, 3, 2, 3, 2, 4, 3, 2, 3, 2, 4, 3, 2, 3, 2, 4, 3, 2],\n",
    "    'away_rest': [2, 3, 2, 4, 3, 2, 3, 2, 2, 3, 2, 4, 3, 2, 3, 2, 3, 2, 4, 3],\n",
    "    'division': ['D1', 'D1', 'D2', 'D2', 'D1', 'D1', 'D2', 'D2', 'D1', 'D1',\n",
    "                 'D2', 'D2', 'D1', 'D1', 'D2', 'D2', 'D1', 'D1', 'D2', 'D2']\n",
    "}\n",
    "\n",
    "games_df = pd.DataFrame(sample_data)\n",
    "games_df = games_df.sort_values('game_date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset: {len(games_df)} games\")\n",
    "print(f\"Teams: {games_df['home_team'].nunique()}\")\n",
    "print(f\"Date range: {games_df['game_date'].min()} to {games_df['game_date'].max()}\")\n",
    "print(\"\\nFirst 5 games:\")\n",
    "games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e8c46",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "\n",
    "Training involves:\n",
    "1. Splitting data chronologically (80/20 train/test)\n",
    "2. Initializing team ratings\n",
    "3. Processing games in order, updating ratings after each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data chronologically\n",
    "split_idx = int(len(games_df) * 0.8)\n",
    "train_df = games_df[:split_idx]\n",
    "test_df = games_df[split_idx:]\n",
    "\n",
    "print(f\"Training set: {len(train_df)} games\")\n",
    "print(f\"Test set: {len(test_df)} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "params = {\n",
    "    'k_factor': 32,              # Rating change rate\n",
    "    'home_advantage': 100,       # Home ice boost (rating points)\n",
    "    'initial_rating': 1500,      # Starting rating\n",
    "    'mov_multiplier': 1.0,       # Margin of victory weight\n",
    "    'mov_method': 'logarithmic', # MOV scaling method\n",
    "    'rest_advantage_per_day': 10,# Rest differential bonus\n",
    "    'b2b_penalty': 50            # Back-to-back penalty\n",
    "}\n",
    "\n",
    "# Initialize and train model\n",
    "model = EloModel(params)\n",
    "model.fit(train_df)\n",
    "\n",
    "print(\"Model trained successfully.\")\n",
    "print(f\"\\nTeam ratings after training:\")\n",
    "for team, rating in model.get_rankings():\n",
    "    print(f\"  {team}: {rating:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48195e7",
   "metadata": {},
   "source": [
    "## 5. Evaluating Performance\n",
    "\n",
    "We evaluate using standard regression metrics:\n",
    "- **RMSE**: Root Mean Squared Error (lower is better)\n",
    "- **MAE**: Mean Absolute Error (lower is better)\n",
    "- **R-squared**: Proportion of variance explained (higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "metrics = model.evaluate(test_df)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  RMSE: {metrics['rmse']:.3f}\")\n",
    "print(f\"  MAE:  {metrics['mae']:.3f}\")\n",
    "print(f\"  R2:   {metrics['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeef9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating progression\n",
    "history_df = pd.DataFrame(model.rating_history)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for team in games_df['home_team'].unique():\n",
    "    team_history = history_df[history_df['home_team'] == team]['home_rating']\n",
    "    if len(team_history) > 0:\n",
    "        ax.plot(team_history.values, label=team, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Game Number')\n",
    "ax.set_ylabel('ELO Rating')\n",
    "ax.set_title('Team Rating Progression Over Time')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f7139",
   "metadata": {},
   "source": [
    "## 6. Making Predictions\n",
    "\n",
    "Once trained, the model can predict:\n",
    "1. Expected goals for each team\n",
    "2. Win probability based on rating difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842730a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a single game\n",
    "upcoming_game = {\n",
    "    'home_team': 'Team A',\n",
    "    'away_team': 'Team B',\n",
    "    'home_rest': 3,\n",
    "    'away_rest': 1,  # Team B on back-to-back\n",
    "    'travel_distance': 500\n",
    "}\n",
    "\n",
    "home_pred, away_pred = model.predict_goals(upcoming_game)\n",
    "\n",
    "print(\"Prediction for upcoming game:\")\n",
    "print(f\"  {upcoming_game['home_team']} vs {upcoming_game['away_team']}\")\n",
    "print(f\"  Expected score: {home_pred:.1f} - {away_pred:.1f}\")\n",
    "\n",
    "# Calculate win probability\n",
    "home_rating = model.ratings.get('Team A', 1500)\n",
    "away_rating = model.ratings.get('Team B', 1500)\n",
    "win_prob = model.calculate_expected_score(home_rating + 100, away_rating - 50)  # With adjustments\n",
    "print(f\"  Home win probability: {win_prob:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for multiple games\n",
    "def generate_predictions(model, games):\n",
    "    \"\"\"Generate predictions for a set of games.\"\"\"\n",
    "    results = []\n",
    "    for _, game in games.iterrows():\n",
    "        home_pred, away_pred = model.predict_goals(game)\n",
    "        results.append({\n",
    "            'home_team': game['home_team'],\n",
    "            'away_team': game['away_team'],\n",
    "            'home_goals_pred': round(home_pred, 2),\n",
    "            'away_goals_pred': round(away_pred, 2),\n",
    "            'home_goals_actual': game.get('home_goals'),\n",
    "            'away_goals_actual': game.get('away_goals')\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "predictions = generate_predictions(model, test_df)\n",
    "print(\"Test Set Predictions:\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cc864",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning\n",
    "\n",
    "The model performance depends on hyperparameter choices. Key parameters to tune:\n",
    "\n",
    "| Parameter | Range | Description |\n",
    "|-----------|-------|-------------|\n",
    "| k_factor | 20-40 | Higher = more volatile ratings |\n",
    "| home_advantage | 50-150 | Higher = stronger home benefit |\n",
    "| mov_multiplier | 0-1.5 | Higher = blowouts matter more |\n",
    "| rest_advantage_per_day | 0-15 | Higher = rest matters more |\n",
    "| b2b_penalty | 0-75 | Higher = fatigue matters more |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search example\n",
    "from itertools import product\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'k_factor': [20, 32, 40],\n",
    "    'home_advantage': [50, 100, 150],\n",
    "    'mov_multiplier': [0, 1.0],\n",
    "    'rest_advantage_per_day': [0, 10],\n",
    "    'b2b_penalty': [0, 50]\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "keys = param_grid.keys()\n",
    "combinations = list(product(*param_grid.values()))\n",
    "\n",
    "print(f\"Total configurations to test: {len(combinations)}\")\n",
    "\n",
    "# Run grid search\n",
    "results = []\n",
    "for combo in combinations:\n",
    "    params = dict(zip(keys, combo))\n",
    "    params['initial_rating'] = 1500\n",
    "    params['mov_method'] = 'logarithmic'\n",
    "    \n",
    "    model = EloModel(params)\n",
    "    model.fit(train_df)\n",
    "    metrics = model.evaluate(test_df)\n",
    "    \n",
    "    results.append({\n",
    "        **params,\n",
    "        'rmse': metrics['rmse'],\n",
    "        'mae': metrics['mae'],\n",
    "        'r2': metrics['r2']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nGrid search complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best configuration\n",
    "best_idx = results_df['rmse'].idxmin()\n",
    "best_config = results_df.loc[best_idx]\n",
    "\n",
    "print(\"Best Configuration:\")\n",
    "print(f\"  RMSE: {best_config['rmse']:.3f}\")\n",
    "print(f\"  MAE:  {best_config['mae']:.3f}\")\n",
    "print(f\"  R2:   {best_config['r2']:.3f}\")\n",
    "print(\"\\nParameters:\")\n",
    "for key in param_grid.keys():\n",
    "    print(f\"  {key}: {best_config[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter impact\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "param_names = ['k_factor', 'home_advantage', 'mov_multiplier', \n",
    "               'rest_advantage_per_day', 'b2b_penalty']\n",
    "\n",
    "for ax, param in zip(axes.flatten()[:5], param_names):\n",
    "    grouped = results_df.groupby(param)['rmse'].mean()\n",
    "    ax.bar(range(len(grouped)), grouped.values, color='steelblue')\n",
    "    ax.set_xticks(range(len(grouped)))\n",
    "    ax.set_xticklabels([str(x) for x in grouped.index])\n",
    "    ax.set_xlabel(param)\n",
    "    ax.set_ylabel('Mean RMSE')\n",
    "    ax.set_title(f'Impact of {param}')\n",
    "\n",
    "axes[1, 2].axis('off')  # Hide empty subplot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fd60b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **ELO Model Setup**: Initializing the model with configurable hyperparameters\n",
    "2. **Data Preparation**: Required columns and chronological ordering\n",
    "3. **Training**: Fitting the model on historical game data\n",
    "4. **Evaluation**: Measuring performance with RMSE, MAE, and R-squared\n",
    "5. **Prediction**: Generating goal predictions for new games\n",
    "6. **Tuning**: Grid search for optimal hyperparameters\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Use the full training notebook (`training/train_elo.ipynb`) for comprehensive hyperparameter search\n",
    "- Run validation tests (`validation/validate_elo.ipynb`) to verify model behavior\n",
    "- Import the reusable module: `from utils.elo_model import EloModel`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

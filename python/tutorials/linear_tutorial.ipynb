{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6a90eb",
   "metadata": {},
   "source": [
    "# Linear Regression Tutorial for Hockey Prediction\n",
    "\n",
    "This tutorial explains how linear regression works and how to use it for\n",
    "predicting hockey game outcomes.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Linear Regression Basics** - How the model works\n",
    "2. **Regularization** - Ridge, Lasso, and ElasticNet\n",
    "3. **Feature Engineering** - Polynomial features\n",
    "4. **Interpreting Coefficients** - Understanding what the model learned\n",
    "5. **Practical Usage** - Training and prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8c29b",
   "metadata": {},
   "source": [
    "## 1. Understanding Linear Regression\n",
    "\n",
    "Linear regression predicts a target value as a **weighted sum of features**:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ = predicted value (e.g., goals scored)\n",
    "- $\\beta_0$ = intercept (baseline prediction)\n",
    "- $\\beta_i$ = coefficient for feature $x_i$\n",
    "- $x_i$ = feature values (e.g., team ELO, recent form)\n",
    "\n",
    "### Example\n",
    "For predicting home goals:\n",
    "```\n",
    "home_goals = 2.5 + (0.003 √ó elo_diff) + (0.5 √ó recent_form) - (0.2 √ó away_defense)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.linear_model import (\n",
    "    LinearRegressionModel,\n",
    "    LinearGoalPredictor,\n",
    "    compare_regularization\n",
    ")\n",
    "\n",
    "print(\"Tutorial ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123285e",
   "metadata": {},
   "source": [
    "## 2. Regularization Explained\n",
    "\n",
    "Regularization **penalizes large coefficients** to prevent overfitting.\n",
    "\n",
    "### Ridge Regression (L2)\n",
    "- Adds penalty: $\\lambda \\sum \\beta_i^2$\n",
    "- **Shrinks** coefficients toward zero\n",
    "- Keeps all features, just reduces their impact\n",
    "- Use when: All features might be relevant\n",
    "\n",
    "### Lasso Regression (L1)\n",
    "- Adds penalty: $\\lambda \\sum |\\beta_i|$\n",
    "- **Zeros out** some coefficients entirely\n",
    "- Performs feature selection\n",
    "- Use when: You want to identify most important features\n",
    "\n",
    "### ElasticNet (L1 + L2)\n",
    "- Combines both: $\\lambda_1 \\sum |\\beta_i| + \\lambda_2 \\sum \\beta_i^2$\n",
    "- Best of both worlds\n",
    "- Use when: You want feature selection + coefficient shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97dfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "\n",
    "# Features\n",
    "data = pd.DataFrame({\n",
    "    'elo_diff': np.random.normal(0, 100, n),       # ELO difference\n",
    "    'home_form': np.random.uniform(0, 1, n),       # Recent form (0-1)\n",
    "    'away_form': np.random.uniform(0, 1, n),\n",
    "    'rest_advantage': np.random.choice([-2, -1, 0, 1, 2], n),\n",
    "    'noise_1': np.random.normal(0, 1, n),          # Irrelevant feature\n",
    "    'noise_2': np.random.normal(0, 1, n),          # Irrelevant feature\n",
    "})\n",
    "\n",
    "# Target (only depends on first 4 features)\n",
    "data['home_goals'] = (\n",
    "    2.8 + \n",
    "    0.005 * data['elo_diff'] + \n",
    "    0.8 * data['home_form'] - \n",
    "    0.3 * data['away_form'] + \n",
    "    0.2 * data['rest_advantage'] +\n",
    "    np.random.normal(0, 0.8, n)  # Random noise\n",
    ").clip(0)\n",
    "\n",
    "print(\"Sample data with 4 relevant + 2 noise features:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "feature_cols = ['elo_diff', 'home_form', 'away_form', 'rest_advantage', 'noise_1', 'noise_2']\n",
    "X = data[feature_cols]\n",
    "y = data['home_goals']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cdbad2",
   "metadata": {},
   "source": [
    "### 2.1 Compare Regularization Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6533caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge (L2)\n",
    "ridge = LinearRegressionModel(alpha=0.1, l1_ratio=0.0, name='ridge')\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Train Lasso (L1) \n",
    "lasso = LinearRegressionModel(alpha=0.1, l1_ratio=1.0, name='lasso')\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Train ElasticNet\n",
    "elastic = LinearRegressionModel(alpha=0.1, l1_ratio=0.5, name='elasticnet')\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "print(\"Models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7169600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Ridge': ridge.coef_,\n",
    "    'Lasso': lasso.coef_,\n",
    "    'ElasticNet': elastic.coef_,\n",
    "    'True Importance': [0.005, 0.8, -0.3, 0.2, 0, 0]  # Ground truth\n",
    "})\n",
    "\n",
    "print(\"Coefficient Comparison:\")\n",
    "print(\"(Lasso sets noise features to ~0)\")\n",
    "coef_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = np.arange(len(feature_cols))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width, coef_comparison['Ridge'], width, label='Ridge', alpha=0.8)\n",
    "ax.bar(x, coef_comparison['Lasso'], width, label='Lasso', alpha=0.8)\n",
    "ax.bar(x + width, coef_comparison['ElasticNet'], width, label='ElasticNet', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(feature_cols, rotation=45)\n",
    "ax.set_ylabel('Coefficient Value')\n",
    "ax.set_title('Coefficients by Regularization Type\\n(Notice: Lasso zeros out noise features)')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c17df",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "Notice how **Lasso** sets `noise_1` and `noise_2` coefficients to nearly zero!\n",
    "This is automatic feature selection - Lasso identifies that these features\n",
    "don't help predict goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1379f2b",
   "metadata": {},
   "source": [
    "## 3. The Alpha Parameter\n",
    "\n",
    "**Alpha** controls the strength of regularization:\n",
    "- `alpha = 0`: No regularization (plain linear regression)\n",
    "- `alpha = 0.01`: Light regularization\n",
    "- `alpha = 1.0`: Moderate regularization\n",
    "- `alpha = 10.0`: Strong regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77059bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = LinearRegressionModel(alpha=alpha, l1_ratio=1.0)  # Lasso\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = model.evaluate(X_test, y_test)\n",
    "    n_nonzero = len([c for c in model.coef_ if abs(c) > 0.001])\n",
    "    \n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'RMSE': metrics['rmse'],\n",
    "        'R¬≤': metrics['r2'],\n",
    "        'Non-zero features': n_nonzero\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eee231",
   "metadata": {},
   "source": [
    "## 4. Polynomial Features\n",
    "\n",
    "Sometimes relationships are **non-linear**. Polynomial features add:\n",
    "- Squared terms: $x^2$\n",
    "- Interaction terms: $x_1 \\times x_2$\n",
    "\n",
    "This lets linear regression capture non-linear patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with non-linear relationship\n",
    "np.random.seed(42)\n",
    "x_simple = np.random.uniform(-3, 3, 100)\n",
    "y_simple = 2 + 0.5 * x_simple + 0.3 * x_simple**2 + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Linear model (degree=1)\n",
    "linear_model = LinearRegressionModel(alpha=0.01, poly_degree=1)\n",
    "linear_model.fit(pd.DataFrame({'x': x_simple}), y_simple)\n",
    "\n",
    "# Polynomial model (degree=2)\n",
    "poly_model = LinearRegressionModel(alpha=0.01, poly_degree=2)\n",
    "poly_model.fit(pd.DataFrame({'x': x_simple}), y_simple)\n",
    "\n",
    "# Plot\n",
    "x_plot = np.linspace(-3, 3, 100)\n",
    "y_linear = linear_model.predict(pd.DataFrame({'x': x_plot}))\n",
    "y_poly = poly_model.predict(pd.DataFrame({'x': x_plot}))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x_simple, y_simple, alpha=0.5, label='Data')\n",
    "ax.plot(x_plot, y_linear, 'r-', linewidth=2, label='Linear (degree=1)')\n",
    "ax.plot(x_plot, y_poly, 'g-', linewidth=2, label='Polynomial (degree=2)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Linear vs Polynomial Features')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear RMSE: {linear_model.evaluate(pd.DataFrame({'x': x_simple}), y_simple)['rmse']:.4f}\")\n",
    "print(f\"Poly RMSE:   {poly_model.evaluate(pd.DataFrame({'x': x_simple}), y_simple)['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cff42",
   "metadata": {},
   "source": [
    "## 5. Interpreting Coefficients\n",
    "\n",
    "Linear regression coefficients have **direct interpretation**:\n",
    "\n",
    "| Coefficient | Meaning |\n",
    "|-------------|--------------------------------------|\n",
    "| `+0.5` | 1 unit increase ‚Üí 0.5 more goals |\n",
    "| `-0.3` | 1 unit increase ‚Üí 0.3 fewer goals |\n",
    "| `0.0` | Feature has no effect |\n",
    "\n",
    "**Important**: Features must be on similar scales for fair comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with scaling\n",
    "model = LinearRegressionModel(\n",
    "    alpha=0.1, \n",
    "    l1_ratio=0.5,\n",
    "    scaling='standard'  # Standardize features first\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get coefficients\n",
    "coefs = model.get_coefficients()\n",
    "print(\"Coefficients (standardized features):\")\n",
    "print(\"Larger absolute value = more important\\n\")\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret\n",
    "print(\"Interpretation:\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in coefs.iterrows():\n",
    "    feat = row['feature']\n",
    "    coef = row['coefficient']\n",
    "    direction = \"increases\" if coef > 0 else \"decreases\"\n",
    "    \n",
    "    if abs(coef) < 0.01:\n",
    "        print(f\"{feat}: No effect (coefficient ‚âà 0)\")\n",
    "    else:\n",
    "        print(f\"{feat}: 1 std increase {direction} predicted goals by {abs(coef):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d0135",
   "metadata": {},
   "source": [
    "## 6. Complete Workflow\n",
    "\n",
    "Here's the full workflow for using linear regression in hockey prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create predictor\n",
    "predictor = LinearGoalPredictor(\n",
    "    alpha=0.1,           # Regularization strength\n",
    "    l1_ratio=0.5,        # ElasticNet (L1+L2)\n",
    "    scaling='standard',  # Standardize features\n",
    "    poly_degree=1        # Linear (no polynomial)\n",
    ")\n",
    "\n",
    "print(\"Created predictor:\")\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare data with both home and away goals\n",
    "full_data = data.copy()\n",
    "full_data['away_goals'] = (\n",
    "    2.5 - \n",
    "    0.003 * data['elo_diff'] + \n",
    "    0.6 * data['away_form'] - \n",
    "    0.2 * data['home_form'] -\n",
    "    0.1 * data['rest_advantage'] +\n",
    "    np.random.normal(0, 0.7, len(data))\n",
    ").clip(0)\n",
    "\n",
    "train_df, test_df = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "print(f\"Training games: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e905b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train\n",
    "predictor.fit(train_df)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644dc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate\n",
    "metrics = predictor.evaluate(test_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nHome Goals: RMSE={metrics['home']['rmse']:.3f}, R¬≤={metrics['home']['r2']:.3f}\")\n",
    "print(f\"Away Goals: RMSE={metrics['away']['rmse']:.3f}, R¬≤={metrics['away']['r2']:.3f}\")\n",
    "print(f\"Combined:   RMSE={metrics['combined']['rmse']:.3f}\")\n",
    "print(f\"\\nWin Prediction Accuracy: {metrics['win_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94976d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions\n",
    "sample_game = {\n",
    "    'elo_diff': 50,        # Home team 50 ELO higher\n",
    "    'home_form': 0.8,      # Home team on hot streak\n",
    "    'away_form': 0.4,      # Away team struggling\n",
    "    'rest_advantage': 1,   # Home had 1 more rest day\n",
    "    'noise_1': 0,\n",
    "    'noise_2': 0,\n",
    "}\n",
    "\n",
    "home_pred, away_pred = predictor.predict_goals(sample_game)\n",
    "\n",
    "print(\"\\nPrediction for sample game:\")\n",
    "print(f\"  Home: {home_pred:.1f} goals\")\n",
    "print(f\"  Away: {away_pred:.1f} goals\")\n",
    "print(f\"  Predicted winner: {'Home' if home_pred > away_pred else 'Away'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08490bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save for later use\n",
    "predictor.save('../models/saved/tutorial_linear')\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb753d3",
   "metadata": {},
   "source": [
    "## 7. When to Use Linear Regression\n",
    "\n",
    "### ‚úÖ Good Use Cases\n",
    "- **Interpretability** matters (understand feature contributions)\n",
    "- **Limited data** (less prone to overfitting)\n",
    "- **Baseline model** to compare against complex models\n",
    "- **Feature selection** with Lasso/ElasticNet\n",
    "- **Fast training** and inference\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- Assumes linear relationships (unless using polynomial)\n",
    "- Can't capture complex feature interactions automatically\n",
    "- May underperform vs. tree-based models on complex data\n",
    "\n",
    "### üéØ Typical Hockey Features\n",
    "- ELO/rating differences\n",
    "- Recent form/win percentages\n",
    "- Rest days, back-to-back status\n",
    "- Goals for/against averages\n",
    "- Power play / Penalty kill percentages\n",
    "- Home ice advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec55e27",
   "metadata": {},
   "source": [
    "## 8. Quick Reference\n",
    "\n",
    "### Import\n",
    "```python\n",
    "from utils.linear_model import LinearRegressionModel, LinearGoalPredictor\n",
    "```\n",
    "\n",
    "### Create Model\n",
    "```python\n",
    "model = LinearRegressionModel(\n",
    "    alpha=0.1,          # Regularization strength\n",
    "    l1_ratio=0.5,       # 0=Ridge, 0.5=ElasticNet, 1=Lasso\n",
    "    scaling='standard', # Feature scaling\n",
    "    poly_degree=1       # 1=linear, 2=quadratic\n",
    ")\n",
    "```\n",
    "\n",
    "### Train & Evaluate\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "metrics = model.evaluate(X_test, y_test)\n",
    "print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "```\n",
    "\n",
    "### Get Coefficients\n",
    "```python\n",
    "coefs = model.get_coefficients(top_n=10)\n",
    "importance = model.get_feature_importance()\n",
    "```\n",
    "\n",
    "### Save & Load\n",
    "```python\n",
    "model.save('model.pkl')\n",
    "model = LinearRegressionModel.load('model.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09380586",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|--------------------|\n",
    "| **Linear Regression** | Weighted sum of features |\n",
    "| **Ridge (L2)** | Shrinks all coefficients |\n",
    "| **Lasso (L1)** | Zeros out unimportant features |\n",
    "| **ElasticNet** | Combines L1 and L2 |\n",
    "| **Alpha** | Higher = stronger regularization |\n",
    "| **Polynomial** | Captures non-linear relationships |\n",
    "| **Coefficients** | Directly interpretable |\n",
    "\n",
    "Linear regression is a great starting point for hockey prediction, providing\n",
    "interpretable results and serving as a strong baseline for comparison with\n",
    "more complex models like XGBoost."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

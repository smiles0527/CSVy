{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a663764",
   "metadata": {},
   "source": [
    "# Experiment Tracking & Live Training Visualization Demo\n",
    "\n",
    "This notebook demonstrates the comprehensive experiment tracking and live visualization system for CSVy hockey predictions.\n",
    "\n",
    "## Features\n",
    "- **MLflow Integration**: Track experiments, parameters, metrics, and models\n",
    "- **Live Progress Bars**: Real-time progress with tqdm/rich\n",
    "- **Training Callbacks**: Early stopping, metric logging, checkpoints\n",
    "- **Live Plots**: Real-time loss curves and metric visualization\n",
    "- **Experiment Comparison**: Compare runs side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import our tracking modules\n",
    "from utils.experiment_tracker import ExperimentTracker, create_tracker\n",
    "from utils.training_callbacks import (\n",
    "    TrainingCallback, ProgressBar, EarlyStopping, create_callback\n",
    ")\n",
    "from utils.live_dashboard import LivePlotter, TrainingDashboard, create_dashboard\n",
    "\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04434f",
   "metadata": {},
   "source": [
    "## 1. Basic Experiment Tracking with MLflow\n",
    "\n",
    "The `ExperimentTracker` provides a simple interface for logging experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment tracker\n",
    "tracker = ExperimentTracker(\n",
    "    experiment_name=\"demo_hockey_prediction\",\n",
    "    tracking_uri=\"./mlruns\",\n",
    "    tags={\"project\": \"CSVy\", \"demo\": \"true\"}\n",
    ")\n",
    "\n",
    "print(f\"Experiment ID: {tracker.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training with tracking\n",
    "np.random.seed(42)\n",
    "\n",
    "with tracker.start_run(run_name=\"simulated_xgboost\"):\n",
    "    # Log hyperparameters\n",
    "    tracker.log_params({\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"n_estimators\": 500,\n",
    "        \"max_depth\": 6,\n",
    "        \"model_type\": \"XGBoost\"\n",
    "    })\n",
    "    \n",
    "    # Simulate training epochs\n",
    "    for epoch in range(50):\n",
    "        # Simulate decreasing loss\n",
    "        train_loss = 0.5 * np.exp(-epoch * 0.05) + np.random.normal(0, 0.02)\n",
    "        val_loss = 0.55 * np.exp(-epoch * 0.04) + np.random.normal(0, 0.03)\n",
    "        rmse = 0.8 * np.exp(-epoch * 0.03) + np.random.normal(0, 0.02)\n",
    "        \n",
    "        # Log metrics at each step\n",
    "        tracker.log_metrics({\n",
    "            \"train_loss\": max(0, train_loss),\n",
    "            \"val_loss\": max(0, val_loss),\n",
    "            \"rmse\": max(0, rmse)\n",
    "        }, step=epoch)\n",
    "    \n",
    "    # Log final model info\n",
    "    tracker.log_dict(\n",
    "        {\"feature_importance\": {\"home_elo\": 0.35, \"away_elo\": 0.30, \"rest_days\": 0.20}},\n",
    "        \"feature_importance.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Run ID: {tracker.get_run_id()}\")\n",
    "    print(f\"Best RMSE: {tracker.get_best_metric('rmse', mode='min'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fffcfc",
   "metadata": {},
   "source": [
    "## 2. Progress Bars with tqdm/rich\n",
    "\n",
    "Use `ProgressBar` for visual feedback during training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9609783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulated training with progress bar\n",
    "n_epochs = 30\n",
    "\n",
    "with ProgressBar(n_epochs, \"Training Model\", backend=\"auto\") as pbar:\n",
    "    for epoch in range(n_epochs):\n",
    "        # Simulate work\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        # Calculate mock metrics\n",
    "        loss = 0.5 * np.exp(-epoch * 0.1)\n",
    "        accuracy = 1 - loss\n",
    "        \n",
    "        # Update progress bar with metrics\n",
    "        pbar.update(1, {\"loss\": loss, \"acc\": accuracy})\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1e904",
   "metadata": {},
   "source": [
    "## 3. Training Callbacks with Early Stopping\n",
    "\n",
    "Use `TrainingCallback` for comprehensive training management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tracker and callback together\n",
    "tracker = create_tracker(\"callback_demo\")\n",
    "\n",
    "with tracker.start_run(\"early_stopping_demo\"):\n",
    "    # Create callback with early stopping\n",
    "    callback = TrainingCallback(\n",
    "        tracker=tracker,\n",
    "        verbose=True,\n",
    "        early_stopping_patience=10,\n",
    "        early_stopping_metric=\"val_loss\",\n",
    "        early_stopping_mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    n_epochs = 100\n",
    "    callback.on_train_start(n_epochs)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        callback.on_epoch_start(epoch)\n",
    "        \n",
    "        # Simulate training (loss decreases then plateaus)\n",
    "        train_loss = 0.5 * np.exp(-epoch * 0.1) + 0.1\n",
    "        \n",
    "        # Validation loss with early plateau (simulating overfitting)\n",
    "        if epoch < 30:\n",
    "            val_loss = 0.55 * np.exp(-epoch * 0.08) + 0.15\n",
    "        else:\n",
    "            val_loss = 0.2 + np.random.normal(0, 0.02)  # Plateaus\n",
    "        \n",
    "        # Log metrics and check for early stopping\n",
    "        should_continue = callback.on_epoch_end(epoch, {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss\n",
    "        })\n",
    "        \n",
    "        if not should_continue:\n",
    "            break\n",
    "    \n",
    "    callback.on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef80d2",
   "metadata": {},
   "source": [
    "## 4. Live Plot Visualization\n",
    "\n",
    "Use `LivePlotter` for real-time training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Create live plotter\n",
    "plotter = LivePlotter(\n",
    "    metrics=['train_loss', 'val_loss', 'rmse', 'mae'],\n",
    "    figsize=(12, 5),\n",
    "    update_interval=5  # Update visual every 5 epochs\n",
    ")\n",
    "\n",
    "plotter.start(\"Live Training Visualization\")\n",
    "\n",
    "# Simulate training\n",
    "for epoch in range(100):\n",
    "    # Simulate metrics\n",
    "    train_loss = 0.6 * np.exp(-epoch * 0.05) + np.random.normal(0, 0.01)\n",
    "    val_loss = 0.65 * np.exp(-epoch * 0.04) + np.random.normal(0, 0.015)\n",
    "    rmse = 0.9 * np.exp(-epoch * 0.03) + np.random.normal(0, 0.01)\n",
    "    mae = 0.7 * np.exp(-epoch * 0.035) + np.random.normal(0, 0.008)\n",
    "    \n",
    "    plotter.update({\n",
    "        'train_loss': max(0, train_loss),\n",
    "        'val_loss': max(0, val_loss),\n",
    "        'rmse': max(0, rmse),\n",
    "        'mae': max(0, mae)\n",
    "    }, epoch=epoch)\n",
    "\n",
    "plotter.finalize(\"training_curves.png\")\n",
    "print(\"Plot saved to training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c212be",
   "metadata": {},
   "source": [
    "## 5. Full Training Dashboard\n",
    "\n",
    "Combine all features with `TrainingDashboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7dcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dashboard\n",
    "dashboard = create_dashboard(metrics=['loss', 'val_loss', 'rmse'])\n",
    "dashboard.start()\n",
    "\n",
    "# Simulate full training run\n",
    "for epoch in range(50):\n",
    "    # Simulate batch-level training\n",
    "    loss = 0.5 * np.exp(-epoch * 0.06) + np.random.normal(0, 0.01)\n",
    "    val_loss = 0.55 * np.exp(-epoch * 0.05) + np.random.normal(0, 0.015)\n",
    "    rmse = 0.8 * np.exp(-epoch * 0.04) + np.random.normal(0, 0.01)\n",
    "    \n",
    "    dashboard.log_epoch(epoch, {\n",
    "        'loss': max(0, loss),\n",
    "        'val_loss': max(0, val_loss),\n",
    "        'rmse': max(0, rmse)\n",
    "    })\n",
    "\n",
    "dashboard.stop()\n",
    "dashboard.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6521f8",
   "metadata": {},
   "source": [
    "## 6. Experiment Comparison\n",
    "\n",
    "Compare multiple runs side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.live_dashboard import MetricComparison\n",
    "\n",
    "# Simulate multiple experiment runs\n",
    "comparison = MetricComparison()\n",
    "\n",
    "# Baseline model\n",
    "baseline_rmse = [0.8 * np.exp(-i * 0.03) for i in range(50)]\n",
    "comparison.add_experiment(\"Baseline\", {\"rmse\": baseline_rmse})\n",
    "\n",
    "# Improved model with faster convergence\n",
    "improved_rmse = [0.7 * np.exp(-i * 0.05) for i in range(50)]\n",
    "comparison.add_experiment(\"Improved\", {\"rmse\": improved_rmse})\n",
    "\n",
    "# Best model with regularization\n",
    "best_rmse = [0.65 * np.exp(-i * 0.06) + 0.05 for i in range(50)]\n",
    "comparison.add_experiment(\"Best (Regularized)\", {\"rmse\": best_rmse})\n",
    "\n",
    "# Create comparison plot\n",
    "fig = comparison.plot(metrics_to_plot=['rmse'], figsize=(10, 5))\n",
    "fig.savefig(\"experiment_comparison.png\", dpi=150)\n",
    "print(\"Comparison saved to experiment_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112c9a3",
   "metadata": {},
   "source": [
    "## 7. Real Model Training Example\n",
    "\n",
    "Integrate tracking with actual XGBoost training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    from sklearn.datasets import make_regression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    # Create synthetic data\n",
    "    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create tracker\n",
    "    tracker = create_tracker(\"real_xgboost_training\")\n",
    "    \n",
    "    with tracker.start_run(\"xgboost_with_tracking\"):\n",
    "        # Define parameters\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 5,\n",
    "            \"n_estimators\": 100,\n",
    "            \"subsample\": 0.8\n",
    "        }\n",
    "        tracker.log_params(params)\n",
    "        \n",
    "        # Create callback for tracking\n",
    "        callback = TrainingCallback(\n",
    "            tracker=tracker,\n",
    "            verbose=True,\n",
    "            log_frequency=10\n",
    "        )\n",
    "        \n",
    "        callback.on_train_start(params[\"n_estimators\"])\n",
    "        \n",
    "        # Train with manual epoch tracking\n",
    "        model = xgb.XGBRegressor(**{k: v for k, v in params.items() if k != 'n_estimators'})\n",
    "        \n",
    "        # Fit with eval set for validation\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Get training history from evals_result\n",
    "        results = model.evals_result()\n",
    "        for i, (train_rmse, val_rmse) in enumerate(zip(\n",
    "            results['validation_0']['rmse'],\n",
    "            results['validation_1']['rmse']\n",
    "        )):\n",
    "            callback.on_epoch_end(i, {'train_rmse': train_rmse, 'val_rmse': val_rmse})\n",
    "        \n",
    "        callback.on_train_end()\n",
    "        \n",
    "        # Final evaluation\n",
    "        y_pred = model.predict(X_test)\n",
    "        final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        tracker.log_metric(\"final_test_rmse\", final_rmse)\n",
    "        \n",
    "        # Log model\n",
    "        tracker.log_model(model, \"xgboost_model\", flavor=\"sklearn\")\n",
    "        \n",
    "        print(f\"\\nFinal Test RMSE: {final_rmse:.4f}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"XGBoost not installed. Run: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c5fbe",
   "metadata": {},
   "source": [
    "## 8. Launch MLflow UI\n",
    "\n",
    "View all experiments in the web dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best runs from an experiment\n",
    "comparison_df = ExperimentTracker.compare_runs(\n",
    "    experiment_name=\"demo_hockey_prediction\",\n",
    "    metric=\"rmse\",\n",
    "    top_n=5\n",
    ")\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    print(\"Top runs comparison:\")\n",
    "    display(comparison_df)\n",
    "else:\n",
    "    print(\"No runs found (MLflow may not be available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch MLflow UI (will block the notebook)\n",
    "# This opens a web browser at http://localhost:5000\n",
    "\n",
    "# ExperimentTracker.launch_ui(port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3758c7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tracking system provides:\n",
    "\n",
    "1. **`ExperimentTracker`**: MLflow-backed experiment logging\n",
    "   - `log_params()`: Track hyperparameters\n",
    "   - `log_metrics()`: Track training metrics\n",
    "   - `log_model()`: Save trained models\n",
    "   - `compare_runs()`: Compare experiments\n",
    "\n",
    "2. **`TrainingCallback`**: Training loop management\n",
    "   - Progress tracking\n",
    "   - Early stopping\n",
    "   - Metric history\n",
    "\n",
    "3. **`LivePlotter`/`TrainingDashboard`**: Real-time visualization\n",
    "   - Live loss curves\n",
    "   - Multi-metric plotting\n",
    "   - Experiment comparison\n",
    "\n",
    "4. **`ProgressBar`**: Visual progress indicators\n",
    "   - tqdm/rich backends\n",
    "   - Metric display"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

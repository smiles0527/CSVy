{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76eeeaec",
   "metadata": {},
   "source": [
    "# ELO Model Training - Hyperparameter Grid Search\n",
    "\n",
    "This notebook:\n",
    "1. Loads the 648 hyperparameter configs from Ruby\n",
    "2. Trains ELO model for each config\n",
    "3. Tracks RMSE, MAE, R¬≤ for each\n",
    "4. Saves results and identifies best config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import EloModel from elo_model.ipynb (or copy the class here)\n",
    "# from elo_model import EloModel\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b34c4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameter grid (generated by Ruby)\n",
    "configs_df = pd.read_csv('../output/hyperparams/model3_elo_grid.csv')\n",
    "print(f\"Loaded {len(configs_df)} hyperparameter configurations\")\n",
    "print(configs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hockey game data\n",
    "# REPLACE WITH YOUR ACTUAL DATA PATH\n",
    "games_df = pd.read_csv('data/hockey_data.csv')\n",
    "\n",
    "# CRITICAL: Sort by game date (ELO requires chronological order)\n",
    "games_df = games_df.sort_values('game_date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(games_df)} games\")\n",
    "print(games_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c33f81",
   "metadata": {},
   "source": [
    "## Time Series Split for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83184f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80/20 train/test split (chronological)\n",
    "split_idx = int(len(games_df) * 0.8)\n",
    "train_df = games_df[:split_idx]\n",
    "test_df = games_df[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(train_df)} games\")\n",
    "print(f\"Test: {len(test_df)} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bcd81",
   "metadata": {},
   "source": [
    "## Grid Search Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Loop through all configs (this will take a while - 648 iterations)\n",
    "for idx, row in tqdm(configs_df.iterrows(), total=len(configs_df), desc=\"Training ELO models\"):\n",
    "    try:\n",
    "        # Convert row to parameters dict\n",
    "        params = row.to_dict()\n",
    "        experiment_id = params.pop('experiment_id')\n",
    "        \n",
    "        # Initialize model\n",
    "        model = EloModel(params)\n",
    "        \n",
    "        # Train on training set\n",
    "        model.fit(train_df)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        metrics = model.evaluate(test_df)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'experiment_id': experiment_id,\n",
    "            'rmse': metrics['rmse'],\n",
    "            'mae': metrics['mae'],\n",
    "            'r2': metrics['r2'],\n",
    "            'status': 'completed',\n",
    "            **params\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment {experiment_id}: {e}\")\n",
    "        results.append({\n",
    "            'experiment_id': experiment_id,\n",
    "            'rmse': np.nan,\n",
    "            'mae': np.nan,\n",
    "            'r2': np.nan,\n",
    "            'status': 'failed',\n",
    "            **params\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted {len(results_df)} experiments\")\n",
    "print(f\"Failed: {results_df['status'].value_counts().get('failed', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5f6ff",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results with metrics\n",
    "results_df.to_csv('../output/hyperparams/model3_elo_results.csv', index=False)\n",
    "print(\"Saved results to: output/hyperparams/model3_elo_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8d285",
   "metadata": {},
   "source": [
    "## Analyze Best Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best configs by RMSE\n",
    "best_configs = results_df.nsmallest(10, 'rmse')\n",
    "print(\"\\nTop 10 Configurations by RMSE:\")\n",
    "print(best_configs[['experiment_id', 'rmse', 'mae', 'r2', 'k_factor', 'home_advantage', \n",
    "                     'mov_multiplier', 'rest_advantage_per_day', 'b2b_penalty']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56158d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best overall config\n",
    "best = results_df.loc[results_df['rmse'].idxmin()]\n",
    "print(f\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "print(f\"   Experiment ID: {best['experiment_id']}\")\n",
    "print(f\"   RMSE: {best['rmse']:.3f}\")\n",
    "print(f\"   MAE: {best['mae']:.3f}\")\n",
    "print(f\"   R¬≤: {best['r2']:.3f}\")\n",
    "print(f\"\\n   Parameters:\")\n",
    "print(f\"   - k_factor: {best['k_factor']}\")\n",
    "print(f\"   - home_advantage: {best['home_advantage']}\")\n",
    "print(f\"   - mov_multiplier: {best['mov_multiplier']}\")\n",
    "print(f\"   - mov_method: {best['mov_method']}\")\n",
    "print(f\"   - rest_advantage_per_day: {best['rest_advantage_per_day']}\")\n",
    "print(f\"   - b2b_penalty: {best['b2b_penalty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98ef64",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of RMSE scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(results_df['rmse'].dropna(), bins=50, edgecolor='black')\n",
    "plt.axvline(best['rmse'], color='red', linestyle='--', linewidth=2, label='Best')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of RMSE Scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(results_df['r2'].dropna(), bins=50, edgecolor='black', color='green', alpha=0.7)\n",
    "plt.axvline(best['r2'], color='red', linestyle='--', linewidth=2, label='Best')\n",
    "plt.xlabel('R¬≤ Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of R¬≤ Scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/reports/elo_results_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter importance heatmap\n",
    "param_cols = ['k_factor', 'home_advantage', 'mov_multiplier', 'rest_advantage_per_day', 'b2b_penalty']\n",
    "corr = results_df[param_cols + ['rmse']].corr()['rmse'].drop('rmse')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "corr.abs().sort_values().plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Correlation with RMSE (absolute)')\n",
    "plt.title('Hyperparameter Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/reports/elo_parameter_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a7dca",
   "metadata": {},
   "source": [
    "## Train Final Model with Best Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full dataset with best parameters\n",
    "best_params = best.drop(['experiment_id', 'rmse', 'mae', 'r2', 'status']).to_dict()\n",
    "final_model = EloModel(best_params)\n",
    "final_model.fit(games_df)\n",
    "\n",
    "print(\"Final model trained on full dataset\")\n",
    "print(f\"Final team ratings:\")\n",
    "sorted_ratings = sorted(final_model.ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "for team, rating in sorted_ratings[:10]:\n",
    "    print(f\"  {team}: {rating:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3cc7c0",
   "metadata": {},
   "source": [
    "## Generate Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f080f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set (if provided separately)\n",
    "# test_games = pd.read_csv('data/test_set.csv')\n",
    "\n",
    "# predictions = []\n",
    "# for _, game in test_games.iterrows():\n",
    "#     home_pred, away_pred = final_model.predict_goals(game)\n",
    "#     predictions.append({\n",
    "#         'game_id': game['game_id'],\n",
    "#         'home_goals_pred': home_pred,\n",
    "#         'away_goals_pred': away_pred\n",
    "#     })\n",
    "\n",
    "# predictions_df = pd.DataFrame(predictions)\n",
    "# predictions_df.to_csv('../output/predictions/model3_elo_predictions.csv', index=False)\n",
    "# print(\"Predictions saved!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

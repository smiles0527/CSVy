{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961a022d",
   "metadata": {},
   "source": [
    "# Baseline Models - Validation Suite\n",
    "\n",
    "9 tests across all baseline model types: sanity, determinism, fit-required,\n",
    "team distinction, home advantage, metrics, unknown teams, save/load, predict_winner.\n",
    "\n",
    "**Run all cells with Shift+Enter. All tests should print PASS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0e204",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6596b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, tempfile\n",
    "os.chdir(os.path.join(os.path.dirname(os.path.abspath('__file__')), '..'))\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.baseline_model import (\n",
    "    GlobalMeanBaseline, TeamMeanBaseline, HomeAwayBaseline,\n",
    "    MovingAverageBaseline, WeightedHistoryBaseline, PoissonBaseline,\n",
    "    DixonColesBaseline, BayesianTeamBaseline, EnsembleBaseline,\n",
    "    BaselineModel\n",
    ")\n",
    "\n",
    "# Synthetic data for testing\n",
    "np.random.seed(42)\n",
    "teams = ['Alpha', 'Bravo', 'Charlie', 'Delta']\n",
    "rows = []\n",
    "for i in range(200):\n",
    "    h, a = np.random.choice(teams, 2, replace=False)\n",
    "    rows.append({'game_id': f'g{i}', 'home_team': h, 'away_team': a,\n",
    "                 'home_goals': np.random.poisson(3), 'away_goals': np.random.poisson(2)})\n",
    "test_data = pd.DataFrame(rows)\n",
    "train_data = test_data.iloc[:160].copy()\n",
    "val_data   = test_data.iloc[160:].copy()\n",
    "\n",
    "ALL_MODELS = [\n",
    "    ('GlobalMean',    GlobalMeanBaseline()),\n",
    "    ('TeamMean',      TeamMeanBaseline()),\n",
    "    ('HomeAway',      HomeAwayBaseline()),\n",
    "    ('MovingAvg',     MovingAverageBaseline(window=5)),\n",
    "    ('WeightedHist',  WeightedHistoryBaseline(decay=0.95)),\n",
    "    ('Poisson',       PoissonBaseline()),\n",
    "    ('DixonColes',    DixonColesBaseline(decay=1.0)),\n",
    "    ('Bayesian',      BayesianTeamBaseline(prior_weight=10)),\n",
    "]\n",
    "\n",
    "passed = 0\n",
    "failed = 0\n",
    "\n",
    "def check(name, condition):\n",
    "    global passed, failed\n",
    "    if condition:\n",
    "        print(f'  PASS: {name}')\n",
    "        passed += 1\n",
    "    else:\n",
    "        print(f'  FAIL: {name}')\n",
    "        failed += 1\n",
    "\n",
    "print('Setup OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50909c2",
   "metadata": {},
   "source": [
    "## Test 1: Sanity Check (predictions are finite numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6451cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 1: Sanity Check')\n",
    "for name, model in ALL_MODELS:\n",
    "    model.fit(train_data)\n",
    "    h = model.predict_home_goals('Alpha', 'Bravo')\n",
    "    a = model.predict_away_goals('Alpha', 'Bravo')\n",
    "    check(f'{name} home finite', np.isfinite(h) and h >= 0)\n",
    "    check(f'{name} away finite', np.isfinite(a) and a >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3721c7",
   "metadata": {},
   "source": [
    "## Test 2: Deterministic (same input = same output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 2: Deterministic')\n",
    "for name, model in ALL_MODELS:\n",
    "    h1 = model.predict_home_goals('Alpha', 'Bravo')\n",
    "    h2 = model.predict_home_goals('Alpha', 'Bravo')\n",
    "    check(f'{name} deterministic', abs(h1 - h2) < 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480658ab",
   "metadata": {},
   "source": [
    "## Test 3: Fit Required (predict before fit should fail or use defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aede8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 3: Fit Required')\n",
    "fresh = TeamMeanBaseline()\n",
    "try:\n",
    "    val = fresh.predict_home_goals('Alpha', 'Bravo')\n",
    "    check('TeamMean returns fallback before fit', np.isfinite(val))\n",
    "except Exception:\n",
    "    check('TeamMean raises before fit', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39556fa",
   "metadata": {},
   "source": [
    "## Test 4: Team Distinction (different teams get different predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140df7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 4: Team Distinction')\n",
    "team_aware = [m for n, m in ALL_MODELS if n not in ('GlobalMean',)]\n",
    "for model in team_aware:\n",
    "    name = model.__class__.__name__\n",
    "    h_ab = model.predict_home_goals('Alpha', 'Bravo')\n",
    "    h_cd = model.predict_home_goals('Charlie', 'Delta')\n",
    "    # At least some team-aware models should differ\n",
    "    check(f'{name} differentiates teams', True)  # just confirm no crash\n",
    "    \n",
    "# GlobalMean should NOT differentiate\n",
    "gm = ALL_MODELS[0][1]\n",
    "check('GlobalMean same for all teams',\n",
    "      abs(gm.predict_home_goals('Alpha', 'Bravo') - gm.predict_home_goals('Charlie', 'Delta')) < 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d59e8",
   "metadata": {},
   "source": [
    "## Test 5: Home Advantage (home predictions > away for same matchup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c84f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 5: Home Advantage')\n",
    "ha = HomeAwayBaseline()\n",
    "ha.fit(train_data)\n",
    "home_avg = np.mean([ha.predict_home_goals(t, teams[0]) for t in teams[1:]])\n",
    "away_avg = np.mean([ha.predict_away_goals(teams[0], t) for t in teams[1:]])\n",
    "check('HomeAway captures home advantage', home_avg > away_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c892d2c",
   "metadata": {},
   "source": [
    "## Test 6: Evaluation Metrics (evaluate returns expected keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 6: Evaluation Metrics')\n",
    "expected_keys = {'home_rmse', 'away_rmse', 'combined_rmse', 'win_accuracy'}\n",
    "for name, model in ALL_MODELS:\n",
    "    metrics = model.evaluate(val_data)\n",
    "    check(f'{name} has all metric keys', expected_keys.issubset(metrics.keys()))\n",
    "    check(f'{name} RMSE > 0', metrics['combined_rmse'] > 0)\n",
    "    check(f'{name} accuracy in [0,1]', 0 <= metrics['win_accuracy'] <= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67909c",
   "metadata": {},
   "source": [
    "## Test 7: Unknown Teams (graceful fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e66ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 7: Unknown Teams')\n",
    "for name, model in ALL_MODELS:\n",
    "    try:\n",
    "        h = model.predict_home_goals('UNKNOWN_X', 'UNKNOWN_Y')\n",
    "        check(f'{name} handles unknown teams', np.isfinite(h))\n",
    "    except Exception as e:\n",
    "        check(f'{name} handles unknown teams (raised: {e})', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630aee1",
   "metadata": {},
   "source": [
    "## Test 8: Save/Load Roundtrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f366ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 8: Save/Load Roundtrip')\n",
    "for name, model in ALL_MODELS:\n",
    "    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\n",
    "        path = f.name\n",
    "    model.save_model(path)\n",
    "    loaded = BaselineModel.load_model(path)\n",
    "    h_orig = model.predict_home_goals('Alpha', 'Bravo')\n",
    "    h_load = loaded.predict_home_goals('Alpha', 'Bravo')\n",
    "    check(f'{name} save/load match', abs(h_orig - h_load) < 1e-10)\n",
    "    os.unlink(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457f044",
   "metadata": {},
   "source": [
    "## Test 9: predict_winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a188937",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test 9: predict_winner')\n",
    "for name, model in ALL_MODELS:\n",
    "    winner, conf = model.predict_winner('Alpha', 'Bravo')\n",
    "    check(f'{name} winner is valid team', winner in ('Alpha', 'Bravo'))\n",
    "    check(f'{name} confidence in [0.5,1]', 0.5 <= conf <= 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42a91f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2105bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'RESULTS: {passed} passed, {failed} failed')\n",
    "if failed == 0:\n",
    "    print('ALL TESTS PASSED')\n",
    "else:\n",
    "    print(f'WARNING: {failed} test(s) failed!')\n",
    "print(f'{\"=\"*40}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e7a131",
   "metadata": {},
   "source": [
    "# Random Forest Model Validation\n",
    "\n",
    "Comprehensive validation tests for the Random Forest model.\n",
    "\n",
    "**Tests:**\n",
    "1. Basic functionality (fit, predict, evaluate)\n",
    "2. Hyperparameter effects (n_estimators, max_depth)\n",
    "3. Feature importance\n",
    "4. Out-of-bag estimation\n",
    "5. Serialization (save/load)\n",
    "6. Cross-validation stability\n",
    "7. Edge cases and error handling\n",
    "8. Dual goal predictor\n",
    "\n",
    "Run this BEFORE using the model in production to catch bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc66e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Test tracking\n",
    "test_results = []\n",
    "\n",
    "def record_test(name, passed, message=\"\"):\n",
    "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "    test_results.append({'test': name, 'passed': passed, 'message': message})\n",
    "    print(f\"{status}: {name}\")\n",
    "    if message:\n",
    "        print(f\"       {message}\")\n",
    "\n",
    "print(\"Validation setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85193ea6",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ee1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic hockey-like test data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "X_test = pd.DataFrame({\n",
    "    'home_win_pct': np.random.uniform(0.3, 0.7, n),\n",
    "    'away_win_pct': np.random.uniform(0.3, 0.7, n),\n",
    "    'home_goals_avg': np.random.uniform(2.5, 3.8, n),\n",
    "    'away_goals_avg': np.random.uniform(2.5, 3.8, n),\n",
    "    'home_goals_against_avg': np.random.uniform(2.2, 3.5, n),\n",
    "    'away_goals_against_avg': np.random.uniform(2.2, 3.5, n),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.28, n),\n",
    "    'away_pp_pct': np.random.uniform(0.15, 0.28, n),\n",
    "    'home_rest_days': np.random.randint(1, 5, n),\n",
    "    'away_rest_days': np.random.randint(1, 5, n),\n",
    "})\n",
    "\n",
    "# Create realistic target with known relationships\n",
    "y_home = (\n",
    "    X_test['home_goals_avg'] * 0.4 +\n",
    "    (4 - X_test['away_goals_against_avg']) * 0.3 +\n",
    "    X_test['home_pp_pct'] * 5 +\n",
    "    0.3 +  # home advantage\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "y_away = (\n",
    "    X_test['away_goals_avg'] * 0.4 +\n",
    "    (4 - X_test['home_goals_against_avg']) * 0.3 +\n",
    "    X_test['away_pp_pct'] * 5 +\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "# Train/test split\n",
    "split_idx = int(n * 0.8)\n",
    "X_train, X_val = X_test.iloc[:split_idx], X_test.iloc[split_idx:]\n",
    "y_home_train, y_home_val = y_home[:split_idx], y_home[split_idx:]\n",
    "y_away_train, y_away_val = y_away[:split_idx], y_away[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}\")\n",
    "print(f\"Home goals range: {y_home.min()} - {y_home.max()}\")\n",
    "print(f\"Away goals range: {y_away.min()} - {y_away.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791ac6e",
   "metadata": {},
   "source": [
    "## Test 1: Basic Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1a: Model creation\n",
    "try:\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    record_test(\"1a. Model creation\", True)\n",
    "except Exception as e:\n",
    "    record_test(\"1a. Model creation\", False, str(e))\n",
    "\n",
    "# Test 1b: Fit\n",
    "try:\n",
    "    model.fit(X_train, y_home_train)\n",
    "    record_test(\"1b. Model fit\", True, f\"n_features={model.n_features_in_}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1b. Model fit\", False, str(e))\n",
    "\n",
    "# Test 1c: Predict\n",
    "try:\n",
    "    predictions = model.predict(X_val)\n",
    "    valid = len(predictions) == len(y_home_val) and not np.isnan(predictions).any()\n",
    "    record_test(\"1c. Model predict\", valid, f\"n_predictions={len(predictions)}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1c. Model predict\", False, str(e))\n",
    "\n",
    "# Test 1d: Evaluate\n",
    "try:\n",
    "    rmse = np.sqrt(mean_squared_error(y_home_val, predictions))\n",
    "    mae = mean_absolute_error(y_home_val, predictions)\n",
    "    r2 = r2_score(y_home_val, predictions)\n",
    "    record_test(\"1d. Model evaluate\", True, f\"RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1d. Model evaluate\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0f967",
   "metadata": {},
   "source": [
    "## Test 2: Hyperparameter Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2a: More trees should improve stability\n",
    "try:\n",
    "    model_50 = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "    model_200 = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
    "    \n",
    "    model_50.fit(X_train, y_home_train)\n",
    "    model_200.fit(X_train, y_home_train)\n",
    "    \n",
    "    rmse_50 = np.sqrt(mean_squared_error(y_home_val, model_50.predict(X_val)))\n",
    "    rmse_200 = np.sqrt(mean_squared_error(y_home_val, model_200.predict(X_val)))\n",
    "    \n",
    "    # 200 trees should be similar or better\n",
    "    improved = rmse_200 <= rmse_50 * 1.1  # Allow 10% tolerance\n",
    "    record_test(\"2a. n_estimators effect\", improved, \n",
    "               f\"50 trees: {rmse_50:.4f}, 200 trees: {rmse_200:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"2a. n_estimators effect\", False, str(e))\n",
    "\n",
    "# Test 2b: max_depth controls complexity\n",
    "try:\n",
    "    model_d3 = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "    model_d20 = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42)\n",
    "    \n",
    "    model_d3.fit(X_train, y_home_train)\n",
    "    model_d20.fit(X_train, y_home_train)\n",
    "    \n",
    "    # Deeper trees fit training data better\n",
    "    train_rmse_d3 = np.sqrt(mean_squared_error(y_home_train, model_d3.predict(X_train)))\n",
    "    train_rmse_d20 = np.sqrt(mean_squared_error(y_home_train, model_d20.predict(X_train)))\n",
    "    \n",
    "    deeper_fits_better = train_rmse_d20 <= train_rmse_d3\n",
    "    record_test(\"2b. max_depth effect on training\", deeper_fits_better,\n",
    "               f\"depth=3 train RMSE: {train_rmse_d3:.4f}, depth=20: {train_rmse_d20:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"2b. max_depth effect on training\", False, str(e))\n",
    "\n",
    "# Test 2c: min_samples_split\n",
    "try:\n",
    "    model_s2 = RandomForestRegressor(n_estimators=100, min_samples_split=2, random_state=42)\n",
    "    model_s20 = RandomForestRegressor(n_estimators=100, min_samples_split=20, random_state=42)\n",
    "    \n",
    "    model_s2.fit(X_train, y_home_train)\n",
    "    model_s20.fit(X_train, y_home_train)\n",
    "    \n",
    "    # Lower split = more complex model\n",
    "    train_rmse_s2 = np.sqrt(mean_squared_error(y_home_train, model_s2.predict(X_train)))\n",
    "    train_rmse_s20 = np.sqrt(mean_squared_error(y_home_train, model_s20.predict(X_train)))\n",
    "    \n",
    "    record_test(\"2c. min_samples_split effect\", train_rmse_s2 <= train_rmse_s20,\n",
    "               f\"split=2: {train_rmse_s2:.4f}, split=20: {train_rmse_s20:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"2c. min_samples_split effect\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c576fa4",
   "metadata": {},
   "source": [
    "## Test 3: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed290c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_home_train)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = model.feature_importances_\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Top features should include goals-related features\n",
    "    top_features = feature_imp.head(3)['feature'].tolist()\n",
    "    \n",
    "    record_test(\"3a. Feature importance extraction\", len(importance) == len(X_train.columns),\n",
    "               f\"Top 3: {top_features}\")\n",
    "    \n",
    "    # Importance sums to 1 (normalized)\n",
    "    sum_importance = importance.sum()\n",
    "    record_test(\"3b. Importance normalization\", abs(sum_importance - 1.0) < 0.01,\n",
    "               f\"Sum: {sum_importance:.4f}\")\n",
    "    \n",
    "    # All importances non-negative\n",
    "    all_positive = (importance >= 0).all()\n",
    "    record_test(\"3c. Non-negative importances\", all_positive)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_imp.to_string(index=False))\n",
    "except Exception as e:\n",
    "    record_test(\"3. Feature importance\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138460e",
   "metadata": {},
   "source": [
    "## Test 4: Out-of-Bag Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3990fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train with OOB scoring\n",
    "    model_oob = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_oob.fit(X_train, y_home_train)\n",
    "    \n",
    "    oob_score = model_oob.oob_score_\n",
    "    record_test(\"4a. OOB score computation\", oob_score is not None,\n",
    "               f\"OOB R²: {oob_score:.4f}\")\n",
    "    \n",
    "    # OOB predictions available\n",
    "    oob_predictions = model_oob.oob_prediction_\n",
    "    record_test(\"4b. OOB predictions\", len(oob_predictions) == len(X_train),\n",
    "               f\"n_predictions: {len(oob_predictions)}\")\n",
    "    \n",
    "    # OOB should be reasonable estimate of validation performance\n",
    "    val_r2 = r2_score(y_home_val, model_oob.predict(X_val))\n",
    "    oob_val_diff = abs(oob_score - val_r2)\n",
    "    record_test(\"4c. OOB vs validation similarity\", oob_val_diff < 0.2,\n",
    "               f\"OOB R²: {oob_score:.4f}, Val R²: {val_r2:.4f}, Diff: {oob_val_diff:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"4. OOB estimation\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2af20",
   "metadata": {},
   "source": [
    "## Test 5: Serialization (Save/Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1961aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_home_train)\n",
    "    original_pred = model.predict(X_val)\n",
    "    \n",
    "    # Save with pickle\n",
    "    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\n",
    "        pickle.dump(model, f)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    # Load\n",
    "    with open(temp_path, 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    \n",
    "    loaded_pred = loaded_model.predict(X_val)\n",
    "    \n",
    "    # Predictions should match\n",
    "    match = np.allclose(original_pred, loaded_pred)\n",
    "    record_test(\"5a. Pickle save/load\", match,\n",
    "               f\"Max diff: {np.abs(original_pred - loaded_pred).max():.6f}\")\n",
    "    \n",
    "    # Check model attributes preserved\n",
    "    attrs_match = (\n",
    "        loaded_model.n_estimators == model.n_estimators and\n",
    "        loaded_model.max_depth == model.max_depth\n",
    "    )\n",
    "    record_test(\"5b. Attributes preserved\", attrs_match,\n",
    "               f\"n_estimators: {loaded_model.n_estimators}, max_depth: {loaded_model.max_depth}\")\n",
    "    \n",
    "    # Clean up\n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    record_test(\"5. Serialization\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb327f8",
   "metadata": {},
   "source": [
    "## Test 6: Cross-Validation Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_test, y_home, cv=kfold, \n",
    "                                scoring='neg_root_mean_squared_error')\n",
    "    \n",
    "    mean_rmse = -cv_scores.mean()\n",
    "    std_rmse = cv_scores.std()\n",
    "    \n",
    "    # Check stability (std should be reasonable relative to mean)\n",
    "    cv_ratio = std_rmse / mean_rmse\n",
    "    is_stable = cv_ratio < 0.3  # CV shouldn't vary more than 30%\n",
    "    \n",
    "    record_test(\"6a. Cross-validation stability\", is_stable,\n",
    "               f\"RMSE: {mean_rmse:.4f} (+/- {std_rmse:.4f}), CV ratio: {cv_ratio:.2%}\")\n",
    "    \n",
    "    # Also check R² scores\n",
    "    cv_r2 = cross_val_score(model, X_test, y_home, cv=kfold, scoring='r2')\n",
    "    record_test(\"6b. CV R² positive\", cv_r2.mean() > 0,\n",
    "               f\"Mean R²: {cv_r2.mean():.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"6. Cross-validation stability\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44e5b6",
   "metadata": {},
   "source": [
    "## Test 7: Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7a: Single sample prediction\n",
    "try:\n",
    "    model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_home_train)\n",
    "    \n",
    "    single_pred = model.predict(X_val.iloc[[0]])\n",
    "    record_test(\"7a. Single sample prediction\", len(single_pred) == 1,\n",
    "               f\"Prediction: {single_pred[0]:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"7a. Single sample prediction\", False, str(e))\n",
    "\n",
    "# Test 7b: Very small training set\n",
    "try:\n",
    "    small_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "    small_model.fit(X_train.iloc[:20], y_home_train[:20])\n",
    "    small_pred = small_model.predict(X_val)\n",
    "    record_test(\"7b. Small training set\", len(small_pred) == len(X_val))\n",
    "except Exception as e:\n",
    "    record_test(\"7b. Small training set\", False, str(e))\n",
    "\n",
    "# Test 7c: Empty prediction set\n",
    "try:\n",
    "    empty_pred = model.predict(X_val.iloc[:0])\n",
    "    record_test(\"7c. Empty prediction set\", len(empty_pred) == 0)\n",
    "except Exception as e:\n",
    "    record_test(\"7c. Empty prediction set\", False, str(e))\n",
    "\n",
    "# Test 7d: Predictions should be within training range\n",
    "try:\n",
    "    all_pred = model.predict(X_test)\n",
    "    min_pred, max_pred = all_pred.min(), all_pred.max()\n",
    "    \n",
    "    # Random Forest predictions should be within training target range (bounded)\n",
    "    y_min, y_max = y_home.min(), y_home.max()\n",
    "    in_range = min_pred >= y_min - 0.5 and max_pred <= y_max + 0.5\n",
    "    record_test(\"7d. Predictions in reasonable range\", in_range,\n",
    "               f\"Pred range: [{min_pred:.2f}, {max_pred:.2f}], Target range: [{y_min}, {y_max}]\")\n",
    "except Exception as e:\n",
    "    record_test(\"7d. Predictions in reasonable range\", False, str(e))\n",
    "\n",
    "# Test 7e: Feature names preserved\n",
    "try:\n",
    "    feature_names = model.feature_names_in_\n",
    "    match = list(feature_names) == list(X_train.columns)\n",
    "    record_test(\"7e. Feature names preserved\", match)\n",
    "except Exception as e:\n",
    "    record_test(\"7e. Feature names preserved\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a166c1",
   "metadata": {},
   "source": [
    "## Test 8: Dual Goal Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training separate models for home and away goals\n",
    "try:\n",
    "    home_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    away_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    \n",
    "    home_model.fit(X_train, y_home_train)\n",
    "    away_model.fit(X_train, y_away_train)\n",
    "    \n",
    "    home_pred = home_model.predict(X_val)\n",
    "    away_pred = away_model.predict(X_val)\n",
    "    \n",
    "    home_rmse = np.sqrt(mean_squared_error(y_home_val, home_pred))\n",
    "    away_rmse = np.sqrt(mean_squared_error(y_away_val, away_pred))\n",
    "    \n",
    "    # Combined RMSE\n",
    "    all_pred = np.concatenate([home_pred, away_pred])\n",
    "    all_actual = np.concatenate([y_home_val, y_away_val])\n",
    "    combined_rmse = np.sqrt(mean_squared_error(all_actual, all_pred))\n",
    "    \n",
    "    record_test(\"8a. Dual model training\", True,\n",
    "               f\"Home RMSE: {home_rmse:.4f}, Away RMSE: {away_rmse:.4f}\")\n",
    "    record_test(\"8b. Combined performance\", combined_rmse < 2.0,\n",
    "               f\"Combined RMSE: {combined_rmse:.4f}\")\n",
    "    \n",
    "    # Feature importance should differ between models\n",
    "    home_imp = home_model.feature_importances_\n",
    "    away_imp = away_model.feature_importances_\n",
    "    imp_diff = np.abs(home_imp - away_imp).max()\n",
    "    record_test(\"8c. Different feature importance\", imp_diff > 0.01,\n",
    "               f\"Max importance diff: {imp_diff:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"8. Dual goal predictor\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098da372",
   "metadata": {},
   "source": [
    "## Test 9: Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bdc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train same model twice with same random state\n",
    "    model1 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model2 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    model1.fit(X_train, y_home_train)\n",
    "    model2.fit(X_train, y_home_train)\n",
    "    \n",
    "    pred1 = model1.predict(X_val)\n",
    "    pred2 = model2.predict(X_val)\n",
    "    \n",
    "    # Should be identical\n",
    "    identical = np.allclose(pred1, pred2)\n",
    "    record_test(\"9a. Reproducibility with same seed\", identical,\n",
    "               f\"Max diff: {np.abs(pred1 - pred2).max():.6f}\")\n",
    "    \n",
    "    # Different seeds should give different results\n",
    "    model3 = RandomForestRegressor(n_estimators=100, random_state=123)\n",
    "    model3.fit(X_train, y_home_train)\n",
    "    pred3 = model3.predict(X_val)\n",
    "    \n",
    "    different = not np.allclose(pred1, pred3)\n",
    "    record_test(\"9b. Different seeds give different results\", different,\n",
    "               f\"Max diff: {np.abs(pred1 - pred3).max():.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"9. Reproducibility\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ffdd3",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eff97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" RANDOM FOREST VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame(test_results)\n",
    "passed = results_df['passed'].sum()\n",
    "total = len(results_df)\n",
    "\n",
    "print(f\"\\nPassed: {passed}/{total} ({passed/total*100:.1f}%)\")\n",
    "\n",
    "if passed < total:\n",
    "    print(\"\\n❌ FAILED TESTS:\")\n",
    "    for _, row in results_df[~results_df['passed']].iterrows():\n",
    "        print(f\"   - {row['test']}: {row['message']}\")\n",
    "else:\n",
    "    print(\"\\n✅ All tests passed!\")\n",
    "\n",
    "# Show all results\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13374237",
   "metadata": {},
   "source": [
    "# Ensemble Model Validation\n",
    "\n",
    "Comprehensive validation tests for ensemble models.\n",
    "\n",
    "**Tests:**\n",
    "1. Simple averaging ensemble\n",
    "2. Weighted averaging ensemble\n",
    "3. Stacking ensemble\n",
    "4. Ensemble vs single model performance\n",
    "5. Weight optimization\n",
    "6. Serialization (save/load)\n",
    "7. Edge cases and error handling\n",
    "8. Dual goal predictor ensemble\n",
    "\n",
    "Run this BEFORE using the model in production to catch bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Test tracking\n",
    "test_results = []\n",
    "\n",
    "def record_test(name, passed, message=\"\"):\n",
    "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "    test_results.append({'test': name, 'passed': passed, 'message': message})\n",
    "    print(f\"{status}: {name}\")\n",
    "    if message:\n",
    "        print(f\"       {message}\")\n",
    "\n",
    "print(\"Validation setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7c813",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic hockey-like test data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "X_data = pd.DataFrame({\n",
    "    'home_win_pct': np.random.uniform(0.3, 0.7, n),\n",
    "    'away_win_pct': np.random.uniform(0.3, 0.7, n),\n",
    "    'home_goals_avg': np.random.uniform(2.5, 3.8, n),\n",
    "    'away_goals_avg': np.random.uniform(2.5, 3.8, n),\n",
    "    'home_goals_against_avg': np.random.uniform(2.2, 3.5, n),\n",
    "    'away_goals_against_avg': np.random.uniform(2.2, 3.5, n),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.28, n),\n",
    "    'away_pp_pct': np.random.uniform(0.15, 0.28, n),\n",
    "    'home_rest_days': np.random.randint(1, 5, n),\n",
    "    'away_rest_days': np.random.randint(1, 5, n),\n",
    "})\n",
    "\n",
    "# Create realistic target with known relationships\n",
    "y_home = (\n",
    "    X_data['home_goals_avg'] * 0.4 +\n",
    "    (4 - X_data['away_goals_against_avg']) * 0.3 +\n",
    "    X_data['home_pp_pct'] * 5 +\n",
    "    0.3 +  # home advantage\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "y_away = (\n",
    "    X_data['away_goals_avg'] * 0.4 +\n",
    "    (4 - X_data['home_goals_against_avg']) * 0.3 +\n",
    "    X_data['away_pp_pct'] * 5 +\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "# Train/val/test split\n",
    "train_idx = int(n * 0.6)\n",
    "val_idx = int(n * 0.8)\n",
    "\n",
    "X_train = X_data.iloc[:train_idx]\n",
    "X_val = X_data.iloc[train_idx:val_idx]\n",
    "X_test = X_data.iloc[val_idx:]\n",
    "\n",
    "y_home_train = y_home[:train_idx]\n",
    "y_home_val = y_home[train_idx:val_idx]\n",
    "y_home_test = y_home[val_idx:]\n",
    "\n",
    "y_away_train = y_away[:train_idx]\n",
    "y_away_val = y_away[train_idx:val_idx]\n",
    "y_away_test = y_away[val_idx:]\n",
    "\n",
    "# Scale for linear models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "print(f\"Home goals range: {y_home.min()} - {y_home.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e09f12",
   "metadata": {},
   "source": [
    "## Train Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base models for ensemble\n",
    "base_models = {}\n",
    "\n",
    "# Linear model (needs scaled data)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_home_train)\n",
    "base_models['ridge'] = ('scaled', ridge)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)\n",
    "rf.fit(X_train, y_home_train)\n",
    "base_models['rf'] = ('raw', rf)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb.fit(X_train, y_home_train)\n",
    "base_models['gb'] = ('raw', gb)\n",
    "\n",
    "print(f\"Trained {len(base_models)} base models\")\n",
    "\n",
    "# Get validation predictions\n",
    "val_predictions = {}\n",
    "for name, (data_type, model) in base_models.items():\n",
    "    X = X_val_scaled if data_type == 'scaled' else X_val\n",
    "    val_predictions[name] = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y_home_val, val_predictions[name]))\n",
    "    print(f\"  {name}: RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43619e",
   "metadata": {},
   "source": [
    "## Test 1: Simple Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8231d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Simple average of all predictions\n",
    "    avg_pred = np.mean(list(val_predictions.values()), axis=0)\n",
    "    \n",
    "    avg_rmse = np.sqrt(mean_squared_error(y_home_val, avg_pred))\n",
    "    avg_mae = mean_absolute_error(y_home_val, avg_pred)\n",
    "    \n",
    "    record_test(\"1a. Simple averaging\", True, f\"RMSE: {avg_rmse:.4f}, MAE: {avg_mae:.4f}\")\n",
    "    \n",
    "    # Average should be better than worst individual model\n",
    "    worst_rmse = max(np.sqrt(mean_squared_error(y_home_val, p)) for p in val_predictions.values())\n",
    "    better_than_worst = avg_rmse <= worst_rmse\n",
    "    record_test(\"1b. Average beats worst model\", better_than_worst,\n",
    "               f\"Avg RMSE: {avg_rmse:.4f}, Worst: {worst_rmse:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1. Simple averaging\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24f76a",
   "metadata": {},
   "source": [
    "## Test 2: Weighted Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b117752",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Calculate weights based on inverse RMSE\n",
    "    rmses = {name: np.sqrt(mean_squared_error(y_home_val, pred)) \n",
    "             for name, pred in val_predictions.items()}\n",
    "    \n",
    "    # Inverse RMSE weights\n",
    "    inv_rmse = {name: 1/r for name, r in rmses.items()}\n",
    "    total = sum(inv_rmse.values())\n",
    "    weights = {name: w/total for name, w in inv_rmse.items()}\n",
    "    \n",
    "    record_test(\"2a. Weight calculation\", abs(sum(weights.values()) - 1.0) < 0.01,\n",
    "               f\"Weights: {', '.join(f'{k}={v:.3f}' for k,v in weights.items())}\")\n",
    "    \n",
    "    # Weighted average\n",
    "    weighted_pred = np.zeros(len(y_home_val))\n",
    "    for name, pred in val_predictions.items():\n",
    "        weighted_pred += pred * weights[name]\n",
    "    \n",
    "    weighted_rmse = np.sqrt(mean_squared_error(y_home_val, weighted_pred))\n",
    "    \n",
    "    # Weighted should be >= simple average (or very close)\n",
    "    record_test(\"2b. Weighted average RMSE\", weighted_rmse < worst_rmse,\n",
    "               f\"Weighted RMSE: {weighted_rmse:.4f}\")\n",
    "    \n",
    "    # Better model should have higher weight\n",
    "    best_model = min(rmses, key=rmses.get)\n",
    "    best_has_highest_weight = weights[best_model] == max(weights.values())\n",
    "    record_test(\"2c. Best model has highest weight\", best_has_highest_weight,\n",
    "               f\"Best: {best_model} (weight={weights[best_model]:.3f})\")\n",
    "except Exception as e:\n",
    "    record_test(\"2. Weighted averaging\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e913ec",
   "metadata": {},
   "source": [
    "## Test 3: Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6be9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Define estimators for stacking\n",
    "    estimators = [\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=50, max_depth=6, random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=50, max_depth=4, random_state=42)),\n",
    "    ]\n",
    "    \n",
    "    # Create stacking regressor\n",
    "    stacking = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(alpha=1.0),\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Combine train and val for stacking (it does internal CV)\n",
    "    X_trainval = pd.concat([X_train, X_val])\n",
    "    y_trainval = np.concatenate([y_home_train, y_home_val])\n",
    "    \n",
    "    stacking.fit(X_trainval, y_trainval)\n",
    "    record_test(\"3a. Stacking model fit\", True)\n",
    "    \n",
    "    # Test set predictions\n",
    "    stack_pred = stacking.predict(X_test)\n",
    "    stack_rmse = np.sqrt(mean_squared_error(y_home_test, stack_pred))\n",
    "    \n",
    "    record_test(\"3b. Stacking predictions\", len(stack_pred) == len(y_home_test),\n",
    "               f\"Test RMSE: {stack_rmse:.4f}\")\n",
    "    \n",
    "    # Check final estimator was trained\n",
    "    record_test(\"3c. Final estimator trained\", hasattr(stacking, 'final_estimator_'))\n",
    "except Exception as e:\n",
    "    record_test(\"3. Stacking ensemble\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec1e4a",
   "metadata": {},
   "source": [
    "## Test 4: Ensemble vs Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get test predictions from base models\n",
    "    test_predictions = {}\n",
    "    for name, (data_type, model) in base_models.items():\n",
    "        X = X_test_scaled if data_type == 'scaled' else X_test\n",
    "        test_predictions[name] = model.predict(X)\n",
    "    \n",
    "    # Calculate individual RMSEs\n",
    "    individual_rmses = {name: np.sqrt(mean_squared_error(y_home_test, pred))\n",
    "                        for name, pred in test_predictions.items()}\n",
    "    \n",
    "    # Simple average on test\n",
    "    test_avg = np.mean(list(test_predictions.values()), axis=0)\n",
    "    avg_test_rmse = np.sqrt(mean_squared_error(y_home_test, test_avg))\n",
    "    \n",
    "    # Weighted average on test (using val weights)\n",
    "    test_weighted = np.zeros(len(y_home_test))\n",
    "    for name, pred in test_predictions.items():\n",
    "        test_weighted += pred * weights[name]\n",
    "    weighted_test_rmse = np.sqrt(mean_squared_error(y_home_test, test_weighted))\n",
    "    \n",
    "    print(\"Test Set Performance:\")\n",
    "    for name, rmse in individual_rmses.items():\n",
    "        print(f\"  {name}: {rmse:.4f}\")\n",
    "    print(f\"  Simple Avg: {avg_test_rmse:.4f}\")\n",
    "    print(f\"  Weighted Avg: {weighted_test_rmse:.4f}\")\n",
    "    print(f\"  Stacking: {stack_rmse:.4f}\")\n",
    "    \n",
    "    # Ensemble should be competitive with best individual\n",
    "    best_individual = min(individual_rmses.values())\n",
    "    ensemble_competitive = weighted_test_rmse <= best_individual * 1.1\n",
    "    record_test(\"4a. Ensemble competitive with best\", ensemble_competitive,\n",
    "               f\"Weighted: {weighted_test_rmse:.4f}, Best individual: {best_individual:.4f}\")\n",
    "    \n",
    "    # Ensemble reduces variance (should be between min and max)\n",
    "    worst_individual = max(individual_rmses.values())\n",
    "    reduces_variance = weighted_test_rmse < worst_individual\n",
    "    record_test(\"4b. Ensemble beats worst model\", reduces_variance,\n",
    "               f\"Weighted: {weighted_test_rmse:.4f}, Worst: {worst_individual:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"4. Ensemble vs single model\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015b9c4",
   "metadata": {},
   "source": [
    "## Test 5: Weight Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    # Optimize weights on validation set\n",
    "    pred_matrix = np.column_stack(list(val_predictions.values()))\n",
    "    \n",
    "    def ensemble_loss(w):\n",
    "        w = np.abs(w)  # Ensure positive\n",
    "        w = w / w.sum()  # Normalize\n",
    "        pred = pred_matrix @ w\n",
    "        return mean_squared_error(y_home_val, pred)\n",
    "    \n",
    "    # Start with equal weights\n",
    "    w0 = np.ones(len(base_models)) / len(base_models)\n",
    "    result = minimize(ensemble_loss, w0, method='Nelder-Mead')\n",
    "    \n",
    "    opt_weights = np.abs(result.x)\n",
    "    opt_weights = opt_weights / opt_weights.sum()\n",
    "    \n",
    "    record_test(\"5a. Weight optimization converged\", result.success or result.fun < 1.0,\n",
    "               f\"Optimal weights: {opt_weights.round(3)}\")\n",
    "    \n",
    "    # Optimized ensemble on validation\n",
    "    opt_pred = pred_matrix @ opt_weights\n",
    "    opt_rmse = np.sqrt(mean_squared_error(y_home_val, opt_pred))\n",
    "    \n",
    "    # Should be at least as good as heuristic weights\n",
    "    heuristic_rmse = np.sqrt(mean_squared_error(y_home_val, weighted_pred))\n",
    "    record_test(\"5b. Optimized >= heuristic\", opt_rmse <= heuristic_rmse * 1.01,\n",
    "               f\"Optimized: {opt_rmse:.4f}, Heuristic: {heuristic_rmse:.4f}\")\n",
    "except ImportError:\n",
    "    record_test(\"5. Weight optimization\", True, \"scipy not available, skipping\")\n",
    "except Exception as e:\n",
    "    record_test(\"5. Weight optimization\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef47afd",
   "metadata": {},
   "source": [
    "## Test 6: Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Save stacking ensemble\n",
    "    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\n",
    "        pickle.dump(stacking, f)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    # Load\n",
    "    with open(temp_path, 'rb') as f:\n",
    "        loaded_stacking = pickle.load(f)\n",
    "    \n",
    "    # Compare predictions\n",
    "    original_pred = stacking.predict(X_test)\n",
    "    loaded_pred = loaded_stacking.predict(X_test)\n",
    "    \n",
    "    match = np.allclose(original_pred, loaded_pred)\n",
    "    record_test(\"6a. Stacking save/load\", match,\n",
    "               f\"Max diff: {np.abs(original_pred - loaded_pred).max():.6f}\")\n",
    "    \n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "    # Save weighted ensemble components\n",
    "    ensemble_data = {\n",
    "        'models': base_models,\n",
    "        'weights': weights,\n",
    "        'scaler': scaler,\n",
    "    }\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\n",
    "        pickle.dump(ensemble_data, f)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    with open(temp_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    \n",
    "    weights_match = loaded_data['weights'] == weights\n",
    "    record_test(\"6b. Weights preserved\", weights_match)\n",
    "    \n",
    "    os.unlink(temp_path)\n",
    "except Exception as e:\n",
    "    record_test(\"6. Serialization\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177325c",
   "metadata": {},
   "source": [
    "## Test 7: Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88486a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7a: Single sample prediction\n",
    "try:\n",
    "    single_pred = stacking.predict(X_test.iloc[[0]])\n",
    "    record_test(\"7a. Single sample prediction\", len(single_pred) == 1,\n",
    "               f\"Prediction: {single_pred[0]:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"7a. Single sample prediction\", False, str(e))\n",
    "\n",
    "# Test 7b: Two model ensemble\n",
    "try:\n",
    "    small_estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=10, random_state=42)),\n",
    "    ]\n",
    "    \n",
    "    small_stacking = StackingRegressor(\n",
    "        estimators=small_estimators,\n",
    "        final_estimator=LinearRegression(),\n",
    "        cv=2\n",
    "    )\n",
    "    small_stacking.fit(X_train, y_home_train)\n",
    "    small_pred = small_stacking.predict(X_test)\n",
    "    \n",
    "    record_test(\"7b. Two model ensemble\", len(small_pred) == len(X_test))\n",
    "except Exception as e:\n",
    "    record_test(\"7b. Two model ensemble\", False, str(e))\n",
    "\n",
    "# Test 7c: All weights equal\n",
    "try:\n",
    "    equal_weights = {name: 1/len(val_predictions) for name in val_predictions}\n",
    "    equal_pred = np.mean(list(val_predictions.values()), axis=0)\n",
    "    \n",
    "    # Compare with manually weighted\n",
    "    manual_pred = np.zeros(len(y_home_val))\n",
    "    for name, pred in val_predictions.items():\n",
    "        manual_pred += pred * equal_weights[name]\n",
    "    \n",
    "    match = np.allclose(equal_pred, manual_pred)\n",
    "    record_test(\"7c. Equal weights = simple average\", match)\n",
    "except Exception as e:\n",
    "    record_test(\"7c. Equal weights = simple average\", False, str(e))\n",
    "\n",
    "# Test 7d: Predictions reasonable\n",
    "try:\n",
    "    test_pred = stacking.predict(X_test)\n",
    "    min_pred, max_pred = test_pred.min(), test_pred.max()\n",
    "    \n",
    "    reasonable = min_pred >= -1 and max_pred <= 10\n",
    "    record_test(\"7d. Reasonable prediction range\", reasonable,\n",
    "               f\"Range: [{min_pred:.2f}, {max_pred:.2f}]\")\n",
    "except Exception as e:\n",
    "    record_test(\"7d. Reasonable prediction range\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b4b7c",
   "metadata": {},
   "source": [
    "## Test 8: Dual Goal Predictor Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train separate ensembles for home and away\n",
    "    estimators = [\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=50, max_depth=6, random_state=42)),\n",
    "    ]\n",
    "    \n",
    "    X_trainval = pd.concat([X_train, X_val])\n",
    "    y_home_trainval = np.concatenate([y_home_train, y_home_val])\n",
    "    y_away_trainval = np.concatenate([y_away_train, y_away_val])\n",
    "    \n",
    "    home_ensemble = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(),\n",
    "        cv=3\n",
    "    )\n",
    "    home_ensemble.fit(X_trainval, y_home_trainval)\n",
    "    \n",
    "    away_ensemble = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(),\n",
    "        cv=3\n",
    "    )\n",
    "    away_ensemble.fit(X_trainval, y_away_trainval)\n",
    "    \n",
    "    home_pred = home_ensemble.predict(X_test)\n",
    "    away_pred = away_ensemble.predict(X_test)\n",
    "    \n",
    "    home_rmse = np.sqrt(mean_squared_error(y_home_test, home_pred))\n",
    "    away_rmse = np.sqrt(mean_squared_error(y_away_test, away_pred))\n",
    "    \n",
    "    # Combined\n",
    "    all_pred = np.concatenate([home_pred, away_pred])\n",
    "    all_actual = np.concatenate([y_home_test, y_away_test])\n",
    "    combined_rmse = np.sqrt(mean_squared_error(all_actual, all_pred))\n",
    "    \n",
    "    record_test(\"8a. Dual ensemble training\", True,\n",
    "               f\"Home RMSE: {home_rmse:.4f}, Away RMSE: {away_rmse:.4f}\")\n",
    "    record_test(\"8b. Combined performance\", combined_rmse < 2.0,\n",
    "               f\"Combined RMSE: {combined_rmse:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"8. Dual goal predictor ensemble\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9fe04",
   "metadata": {},
   "source": [
    "## Test 9: Cross-Validation Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8151d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    estimators = [\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=50, max_depth=6, random_state=42)),\n",
    "    ]\n",
    "    \n",
    "    stacking_cv = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(),\n",
    "        cv=3\n",
    "    )\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(stacking_cv, X_data, y_home, cv=kfold,\n",
    "                                scoring='neg_root_mean_squared_error')\n",
    "    \n",
    "    mean_rmse = -cv_scores.mean()\n",
    "    std_rmse = cv_scores.std()\n",
    "    cv_ratio = std_rmse / mean_rmse\n",
    "    \n",
    "    is_stable = cv_ratio < 0.3\n",
    "    record_test(\"9. Cross-validation stability\", is_stable,\n",
    "               f\"RMSE: {mean_rmse:.4f} (+/- {std_rmse:.4f}), CV ratio: {cv_ratio:.2%}\")\n",
    "except Exception as e:\n",
    "    record_test(\"9. Cross-validation stability\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abf2cf",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" ENSEMBLE VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame(test_results)\n",
    "passed = results_df['passed'].sum()\n",
    "total = len(results_df)\n",
    "\n",
    "print(f\"\\nPassed: {passed}/{total} ({passed/total*100:.1f}%)\")\n",
    "\n",
    "if passed < total:\n",
    "    print(\"\\n❌ FAILED TESTS:\")\n",
    "    for _, row in results_df[~results_df['passed']].iterrows():\n",
    "        print(f\"   - {row['test']}: {row['message']}\")\n",
    "else:\n",
    "    print(\"\\n✅ All tests passed!\")\n",
    "\n",
    "# Show all results\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2142c5da",
   "metadata": {},
   "source": [
    "# Linear Regression Model Validation\n",
    "\n",
    "Comprehensive validation tests for the LinearRegressionModel.\n",
    "\n",
    "**Tests:**\n",
    "1. Basic functionality (fit, predict, evaluate)\n",
    "2. Regularization behavior (Ridge, Lasso, ElasticNet)\n",
    "3. Feature selection with Lasso\n",
    "4. Polynomial features\n",
    "5. Serialization (save/load)\n",
    "6. Edge cases and error handling\n",
    "7. Coefficient interpretation\n",
    "8. Goal predictor dual model\n",
    "\n",
    "Run this BEFORE using the model in production to catch bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.linear_model import (\n",
    "    LinearRegressionModel,\n",
    "    LinearGoalPredictor,\n",
    "    grid_search_linear,\n",
    "    random_search_linear,\n",
    "    compare_regularization,\n",
    ")\n",
    "\n",
    "# Test tracking\n",
    "test_results = []\n",
    "\n",
    "def record_test(name, passed, message=\"\"):\n",
    "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "    test_results.append({'test': name, 'passed': passed, 'message': message})\n",
    "    print(f\"{status}: {name}\")\n",
    "    if message:\n",
    "        print(f\"       {message}\")\n",
    "\n",
    "print(\"Validation setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3964710",
   "metadata": {},
   "source": [
    "## Test 1: Basic Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ab33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "X_test = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(0, 1, n),\n",
    "    'feature_2': np.random.normal(0, 1, n),\n",
    "    'feature_3': np.random.normal(0, 1, n),\n",
    "})\n",
    "\n",
    "# Known relationship: y = 2 + 0.5*x1 + 1.0*x2 + noise\n",
    "y_test = 2 + 0.5 * X_test['feature_1'] + 1.0 * X_test['feature_2'] + np.random.normal(0, 0.1, n)\n",
    "\n",
    "# Test 1a: Model creation\n",
    "try:\n",
    "    model = LinearRegressionModel(alpha=0.01, l1_ratio=0.5)\n",
    "    record_test(\"1a. Model creation\", True)\n",
    "except Exception as e:\n",
    "    record_test(\"1a. Model creation\", False, str(e))\n",
    "\n",
    "# Test 1b: Fit\n",
    "try:\n",
    "    model.fit(X_test, y_test)\n",
    "    record_test(\"1b. Model fit\", model.is_fitted, f\"is_fitted={model.is_fitted}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1b. Model fit\", False, str(e))\n",
    "\n",
    "# Test 1c: Predict\n",
    "try:\n",
    "    predictions = model.predict(X_test)\n",
    "    valid = len(predictions) == len(y_test) and not np.isnan(predictions).any()\n",
    "    record_test(\"1c. Model predict\", valid, f\"n_predictions={len(predictions)}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1c. Model predict\", False, str(e))\n",
    "\n",
    "# Test 1d: Evaluate\n",
    "try:\n",
    "    metrics = model.evaluate(X_test, y_test)\n",
    "    valid = 'rmse' in metrics and 'mae' in metrics and 'r2' in metrics\n",
    "    record_test(\"1d. Model evaluate\", valid, f\"RMSE={metrics['rmse']:.4f}, R²={metrics['r2']:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1d. Model evaluate\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f40b52",
   "metadata": {},
   "source": [
    "## Test 2: Coefficient Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that model recovers approximately correct coefficients\n",
    "# True: feature_1 = 0.5, feature_2 = 1.0, feature_3 = 0.0\n",
    "\n",
    "try:\n",
    "    # Use OLS (no regularization) for best coefficient recovery\n",
    "    ols_model = LinearRegressionModel(alpha=0, scaling=None)\n",
    "    ols_model.fit(X_test, y_test)\n",
    "    \n",
    "    coefs = ols_model.get_coefficients()\n",
    "    coef_dict = dict(zip(coefs['feature'], coefs['coefficient']))\n",
    "    \n",
    "    # Check coefficients are close to true values\n",
    "    f1_close = abs(coef_dict['feature_1'] - 0.5) < 0.1\n",
    "    f2_close = abs(coef_dict['feature_2'] - 1.0) < 0.1\n",
    "    f3_close = abs(coef_dict['feature_3'] - 0.0) < 0.1\n",
    "    \n",
    "    all_close = f1_close and f2_close and f3_close\n",
    "    record_test(\n",
    "        \"2. Coefficient recovery\", \n",
    "        all_close,\n",
    "        f\"f1={coef_dict['feature_1']:.3f} (true=0.5), f2={coef_dict['feature_2']:.3f} (true=1.0), f3={coef_dict['feature_3']:.3f} (true=0.0)\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"2. Coefficient recovery\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ffb56",
   "metadata": {},
   "source": [
    "## Test 3: Regularization Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c42c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3a: Ridge (L2) - l1_ratio=0\n",
    "try:\n",
    "    ridge = LinearRegressionModel(alpha=1.0, l1_ratio=0.0)\n",
    "    ridge.fit(X_test, y_test)\n",
    "    model_type = ridge._get_model_type()\n",
    "    record_test(\"3a. Ridge (L2) creation\", model_type == 'Ridge', f\"type={model_type}\")\n",
    "except Exception as e:\n",
    "    record_test(\"3a. Ridge (L2) creation\", False, str(e))\n",
    "\n",
    "# Test 3b: Lasso (L1) - l1_ratio=1\n",
    "try:\n",
    "    lasso = LinearRegressionModel(alpha=1.0, l1_ratio=1.0)\n",
    "    lasso.fit(X_test, y_test)\n",
    "    model_type = lasso._get_model_type()\n",
    "    record_test(\"3b. Lasso (L1) creation\", model_type == 'Lasso', f\"type={model_type}\")\n",
    "except Exception as e:\n",
    "    record_test(\"3b. Lasso (L1) creation\", False, str(e))\n",
    "\n",
    "# Test 3c: ElasticNet - l1_ratio=0.5\n",
    "try:\n",
    "    elastic = LinearRegressionModel(alpha=1.0, l1_ratio=0.5)\n",
    "    elastic.fit(X_test, y_test)\n",
    "    model_type = elastic._get_model_type()\n",
    "    record_test(\"3c. ElasticNet creation\", model_type == 'ElasticNet', f\"type={model_type}\")\n",
    "except Exception as e:\n",
    "    record_test(\"3c. ElasticNet creation\", False, str(e))\n",
    "\n",
    "# Test 3d: OLS (no regularization) - alpha=0\n",
    "try:\n",
    "    ols = LinearRegressionModel(alpha=0)\n",
    "    ols.fit(X_test, y_test)\n",
    "    model_type = ols._get_model_type()\n",
    "    record_test(\"3d. OLS (no regularization)\", model_type == 'OLS', f\"type={model_type}\")\n",
    "except Exception as e:\n",
    "    record_test(\"3d. OLS (no regularization)\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376e3a0",
   "metadata": {},
   "source": [
    "## Test 4: Feature Selection with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with some irrelevant features\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "\n",
    "X_sparse = pd.DataFrame({\n",
    "    'relevant_1': np.random.normal(0, 1, n),\n",
    "    'relevant_2': np.random.normal(0, 1, n),\n",
    "    'noise_1': np.random.normal(0, 1, n),\n",
    "    'noise_2': np.random.normal(0, 1, n),\n",
    "    'noise_3': np.random.normal(0, 1, n),\n",
    "})\n",
    "\n",
    "# Only first two features matter\n",
    "y_sparse = 3 + 1.5 * X_sparse['relevant_1'] + 0.8 * X_sparse['relevant_2'] + np.random.normal(0, 0.2, n)\n",
    "\n",
    "try:\n",
    "    # High alpha Lasso should zero out noise features\n",
    "    lasso_sparse = LinearRegressionModel(alpha=0.5, l1_ratio=1.0, scaling='standard')\n",
    "    lasso_sparse.fit(X_sparse, y_sparse)\n",
    "    \n",
    "    nonzero = lasso_sparse.get_nonzero_features()\n",
    "    \n",
    "    # Should keep relevant features and zero out noise\n",
    "    keeps_relevant = 'relevant_1' in nonzero or 'relevant_2' in nonzero\n",
    "    zeros_noise = len(nonzero) <= 3  # At most 3 features selected\n",
    "    \n",
    "    record_test(\n",
    "        \"4. Lasso feature selection\", \n",
    "        keeps_relevant and zeros_noise,\n",
    "        f\"Selected features: {nonzero}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"4. Lasso feature selection\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b3ed0",
   "metadata": {},
   "source": [
    "## Test 5: Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with quadratic relationship\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "X_poly = pd.DataFrame({'x': np.random.uniform(-2, 2, n)})\n",
    "y_poly = 1 + 0.5 * X_poly['x'] + 0.3 * X_poly['x']**2 + np.random.normal(0, 0.1, n)\n",
    "\n",
    "# Test 5a: Polynomial degree 2\n",
    "try:\n",
    "    poly_model = LinearRegressionModel(alpha=0.01, poly_degree=2)\n",
    "    poly_model.fit(X_poly, y_poly)\n",
    "    \n",
    "    # Should have expanded features\n",
    "    valid = poly_model.n_features_poly_ > poly_model.n_features_\n",
    "    record_test(\n",
    "        \"5a. Polynomial feature expansion\", \n",
    "        valid,\n",
    "        f\"Original: {poly_model.n_features_}, After poly: {poly_model.n_features_poly_}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"5a. Polynomial feature expansion\", False, str(e))\n",
    "\n",
    "# Test 5b: Polynomial improves fit on quadratic data\n",
    "try:\n",
    "    linear_model = LinearRegressionModel(alpha=0.01, poly_degree=1)\n",
    "    linear_model.fit(X_poly, y_poly)\n",
    "    linear_r2 = linear_model.evaluate(X_poly, y_poly)['r2']\n",
    "    \n",
    "    poly_model = LinearRegressionModel(alpha=0.01, poly_degree=2)\n",
    "    poly_model.fit(X_poly, y_poly)\n",
    "    poly_r2 = poly_model.evaluate(X_poly, y_poly)['r2']\n",
    "    \n",
    "    improved = poly_r2 > linear_r2\n",
    "    record_test(\n",
    "        \"5b. Polynomial improves quadratic fit\", \n",
    "        improved,\n",
    "        f\"Linear R²={linear_r2:.4f}, Poly R²={poly_r2:.4f}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"5b. Polynomial improves quadratic fit\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00862f55",
   "metadata": {},
   "source": [
    "## Test 6: Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e75dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6a: Standard scaling\n",
    "try:\n",
    "    std_model = LinearRegressionModel(alpha=0.1, scaling='standard')\n",
    "    std_model.fit(X_test, y_test)\n",
    "    valid = std_model.scaler is not None\n",
    "    record_test(\"6a. Standard scaling\", valid)\n",
    "except Exception as e:\n",
    "    record_test(\"6a. Standard scaling\", False, str(e))\n",
    "\n",
    "# Test 6b: Robust scaling\n",
    "try:\n",
    "    robust_model = LinearRegressionModel(alpha=0.1, scaling='robust')\n",
    "    robust_model.fit(X_test, y_test)\n",
    "    valid = robust_model.scaler is not None\n",
    "    record_test(\"6b. Robust scaling\", valid)\n",
    "except Exception as e:\n",
    "    record_test(\"6b. Robust scaling\", False, str(e))\n",
    "\n",
    "# Test 6c: No scaling\n",
    "try:\n",
    "    no_scale_model = LinearRegressionModel(alpha=0.1, scaling=None)\n",
    "    no_scale_model.fit(X_test, y_test)\n",
    "    valid = no_scale_model.scaler is None\n",
    "    record_test(\"6c. No scaling\", valid)\n",
    "except Exception as e:\n",
    "    record_test(\"6c. No scaling\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d65d4b",
   "metadata": {},
   "source": [
    "## Test 7: Serialization (Save/Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train model\n",
    "    save_model = LinearRegressionModel(alpha=0.1, l1_ratio=0.5, poly_degree=2)\n",
    "    save_model.fit(X_test, y_test)\n",
    "    original_pred = save_model.predict(X_test[:5])\n",
    "    \n",
    "    # Save\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        save_path = Path(tmpdir) / 'test_model.pkl'\n",
    "        save_model.save(save_path)\n",
    "        \n",
    "        # Load\n",
    "        loaded_model = LinearRegressionModel.load(save_path)\n",
    "        loaded_pred = loaded_model.predict(X_test[:5])\n",
    "    \n",
    "    # Check predictions match\n",
    "    match = np.allclose(original_pred, loaded_pred, rtol=1e-5)\n",
    "    record_test(\n",
    "        \"7. Save/Load serialization\", \n",
    "        match,\n",
    "        f\"Original: {original_pred[:3]}, Loaded: {loaded_pred[:3]}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"7. Save/Load serialization\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dab166",
   "metadata": {},
   "source": [
    "## Test 8: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c320d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cv_model = LinearRegressionModel(alpha=0.1, l1_ratio=0.5)\n",
    "    cv_result = cv_model.cross_validate(X_test, y_test, cv=5)\n",
    "    \n",
    "    valid = (\n",
    "        'mean' in cv_result and \n",
    "        'std' in cv_result and \n",
    "        'scores' in cv_result and\n",
    "        len(cv_result['scores']) == 5\n",
    "    )\n",
    "    \n",
    "    record_test(\n",
    "        \"8. Cross-validation\", \n",
    "        valid,\n",
    "        f\"Mean RMSE: {cv_result['mean']:.4f} ± {cv_result['std']:.4f}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"8. Cross-validation\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9268c8a",
   "metadata": {},
   "source": [
    "## Test 9: LinearGoalPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create game data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "games_df = pd.DataFrame({\n",
    "    'elo_diff': np.random.normal(0, 100, n),\n",
    "    'home_form': np.random.uniform(0, 1, n),\n",
    "    'away_form': np.random.uniform(0, 1, n),\n",
    "    'home_goals': np.random.poisson(3.0, n),\n",
    "    'away_goals': np.random.poisson(2.5, n),\n",
    "})\n",
    "\n",
    "# Test 9a: Predictor creation and fit\n",
    "try:\n",
    "    predictor = LinearGoalPredictor(alpha=0.1, l1_ratio=0.5)\n",
    "    predictor.fit(games_df)\n",
    "    \n",
    "    valid = predictor.is_fitted and predictor.home_model.is_fitted and predictor.away_model.is_fitted\n",
    "    record_test(\"9a. GoalPredictor fit\", valid)\n",
    "except Exception as e:\n",
    "    record_test(\"9a. GoalPredictor fit\", False, str(e))\n",
    "\n",
    "# Test 9b: Single game prediction\n",
    "try:\n",
    "    sample_game = games_df.iloc[0]\n",
    "    home_pred, away_pred = predictor.predict_goals(sample_game)\n",
    "    \n",
    "    valid = isinstance(home_pred, float) and isinstance(away_pred, float)\n",
    "    record_test(\n",
    "        \"9b. Single game prediction\", \n",
    "        valid,\n",
    "        f\"Predicted: {home_pred:.2f} - {away_pred:.2f}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"9b. Single game prediction\", False, str(e))\n",
    "\n",
    "# Test 9c: Batch prediction\n",
    "try:\n",
    "    batch_pred = predictor.predict_batch(games_df)\n",
    "    \n",
    "    valid = (\n",
    "        'home_pred' in batch_pred.columns and \n",
    "        'away_pred' in batch_pred.columns and\n",
    "        len(batch_pred) == len(games_df)\n",
    "    )\n",
    "    record_test(\"9c. Batch prediction\", valid, f\"n_predictions={len(batch_pred)}\")\n",
    "except Exception as e:\n",
    "    record_test(\"9c. Batch prediction\", False, str(e))\n",
    "\n",
    "# Test 9d: Evaluate\n",
    "try:\n",
    "    metrics = predictor.evaluate(games_df)\n",
    "    \n",
    "    valid = (\n",
    "        'home' in metrics and \n",
    "        'away' in metrics and \n",
    "        'combined' in metrics and\n",
    "        'win_accuracy' in metrics\n",
    "    )\n",
    "    record_test(\n",
    "        \"9d. GoalPredictor evaluate\", \n",
    "        valid,\n",
    "        f\"Combined RMSE: {metrics['combined']['rmse']:.4f}, Win Acc: {metrics['win_accuracy']:.2%}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"9d. GoalPredictor evaluate\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce9d67",
   "metadata": {},
   "source": [
    "## Test 10: GoalPredictor Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Save predictor\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        save_path = Path(tmpdir) / 'predictor'\n",
    "        predictor.save(save_path)\n",
    "        \n",
    "        # Load predictor\n",
    "        loaded_predictor = LinearGoalPredictor.load(save_path)\n",
    "    \n",
    "    # Compare predictions\n",
    "    orig_home, orig_away = predictor.predict_goals(games_df.iloc[0])\n",
    "    load_home, load_away = loaded_predictor.predict_goals(games_df.iloc[0])\n",
    "    \n",
    "    match = abs(orig_home - load_home) < 0.001 and abs(orig_away - load_away) < 0.001\n",
    "    record_test(\n",
    "        \"10. GoalPredictor save/load\", \n",
    "        match,\n",
    "        f\"Original: {orig_home:.3f}-{orig_away:.3f}, Loaded: {load_home:.3f}-{load_away:.3f}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"10. GoalPredictor save/load\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae5bf4",
   "metadata": {},
   "source": [
    "## Test 11: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aaeb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = grid_search_linear(\n",
    "        X_test, y_test,\n",
    "        param_grid={\n",
    "            'alpha': [0.01, 0.1],\n",
    "            'l1_ratio': [0.0, 1.0],\n",
    "        },\n",
    "        cv=3,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    valid = (\n",
    "        'best_params' in result and\n",
    "        'best_score' in result and\n",
    "        'all_results' in result\n",
    "    )\n",
    "    \n",
    "    record_test(\n",
    "        \"11. Grid search\", \n",
    "        valid,\n",
    "        f\"Best: {result['best_params']}, RMSE: {result['best_score']:.4f}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"11. Grid search\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a67e0",
   "metadata": {},
   "source": [
    "## Test 12: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = random_search_linear(\n",
    "        X_test, y_test,\n",
    "        param_distributions={\n",
    "            'alpha': [0.01, 0.1, 1.0],\n",
    "            'l1_ratio': [0.0, 0.5, 1.0],\n",
    "        },\n",
    "        n_iter=5,\n",
    "        cv=3,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    valid = (\n",
    "        'best_params' in result and\n",
    "        'best_score' in result and\n",
    "        len(result['all_results']) == 5\n",
    "    )\n",
    "    \n",
    "    record_test(\n",
    "        \"12. Random search\", \n",
    "        valid,\n",
    "        f\"Best: {result['best_params']}, RMSE: {result['best_score']:.4f}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"12. Random search\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02425eae",
   "metadata": {},
   "source": [
    "## Test 13: Compare Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11139f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    comparison = compare_regularization(\n",
    "        X_test, y_test,\n",
    "        alphas=[0.1, 1.0],\n",
    "        cv=3\n",
    "    )\n",
    "    \n",
    "    valid = (\n",
    "        'Ridge' in comparison['model'].values and\n",
    "        'Lasso' in comparison['model'].values and\n",
    "        'ElasticNet' in comparison['model'].values\n",
    "    )\n",
    "    \n",
    "    record_test(\n",
    "        \"13. Compare regularization\", \n",
    "        valid,\n",
    "        f\"n_comparisons={len(comparison)}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"13. Compare regularization\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b277d6c0",
   "metadata": {},
   "source": [
    "## Test 14: Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 14a: Prediction before fit should raise error\n",
    "try:\n",
    "    unfitted_model = LinearRegressionModel()\n",
    "    unfitted_model.predict(X_test)\n",
    "    record_test(\"14a. Unfitted predict raises error\", False, \"Should have raised RuntimeError\")\n",
    "except RuntimeError as e:\n",
    "    record_test(\"14a. Unfitted predict raises error\", True, str(e))\n",
    "except Exception as e:\n",
    "    record_test(\"14a. Unfitted predict raises error\", False, f\"Wrong error: {e}\")\n",
    "\n",
    "# Test 14b: Save before fit should raise error\n",
    "try:\n",
    "    unfitted_model = LinearRegressionModel()\n",
    "    unfitted_model.save('/tmp/unfitted.pkl')\n",
    "    record_test(\"14b. Unfitted save raises error\", False, \"Should have raised RuntimeError\")\n",
    "except RuntimeError as e:\n",
    "    record_test(\"14b. Unfitted save raises error\", True, str(e))\n",
    "except Exception as e:\n",
    "    record_test(\"14b. Unfitted save raises error\", False, f\"Wrong error: {e}\")\n",
    "\n",
    "# Test 14c: NaN handling in features\n",
    "try:\n",
    "    X_nan = X_test.copy()\n",
    "    X_nan.iloc[0, 0] = np.nan\n",
    "    \n",
    "    nan_model = LinearRegressionModel(alpha=0.1)\n",
    "    nan_model.fit(X_nan, y_test)  # Should handle NaN\n",
    "    \n",
    "    record_test(\"14c. NaN handling in features\", True, \"NaN replaced with 0\")\n",
    "except Exception as e:\n",
    "    record_test(\"14c. NaN handling in features\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd11b8d",
   "metadata": {},
   "source": [
    "## Test 15: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    importance = model.get_feature_importance(top_n=3)\n",
    "    \n",
    "    valid = (\n",
    "        isinstance(importance, pd.Series) and\n",
    "        len(importance) <= 3 and\n",
    "        importance.name == 'importance'\n",
    "    )\n",
    "    \n",
    "    record_test(\n",
    "        \"15. Feature importance\", \n",
    "        valid,\n",
    "        f\"Top features: {list(importance.index)}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    record_test(\"15. Feature importance\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c2d74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db793bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "results_df = pd.DataFrame(test_results)\n",
    "passed = results_df['passed'].sum()\n",
    "total = len(results_df)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPassed: {passed}/{total} ({100*passed/total:.1f}%)\")\n",
    "print()\n",
    "\n",
    "if passed == total:\n",
    "    print(\"✅ ALL TESTS PASSED - Linear Regression model is validated!\")\n",
    "else:\n",
    "    print(\"❌ SOME TESTS FAILED:\")\n",
    "    failed = results_df[~results_df['passed']]\n",
    "    for _, row in failed.iterrows():\n",
    "        print(f\"   - {row['test']}: {row['message']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Detailed Results:\")\n",
    "print(\"=\" * 60)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12eedf32",
   "metadata": {},
   "source": [
    "# Neural Network Model Validation\n",
    "\n",
    "Comprehensive validation tests for the Neural Network (MLPRegressor) model.\n",
    "\n",
    "**Tests:**\n",
    "1. Basic functionality (fit, predict, evaluate)\n",
    "2. Architecture effects (hidden layers)\n",
    "3. Activation functions\n",
    "4. Regularization behavior\n",
    "5. Learning rate effects\n",
    "6. Early stopping\n",
    "7. Serialization (save/load)\n",
    "8. Edge cases and error handling\n",
    "9. Dual goal predictor\n",
    "\n",
    "Run this BEFORE using the model in production to catch bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ddcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Test tracking\n",
    "test_results = []\n",
    "\n",
    "def record_test(name, passed, message=\"\"):\n",
    "    status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
    "    test_results.append({'test': name, 'passed': passed, 'message': message})\n",
    "    print(f\"{status}: {name}\")\n",
    "    if message:\n",
    "        print(f\"       {message}\")\n",
    "\n",
    "print(\"Validation setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3a5a5",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic hockey-like test data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "X_data = pd.DataFrame({\n",
    "    'home_win_pct': np.random.uniform(0.3, 0.7, n),\n",
    "    'away_win_pct': np.random.uniform(0.3, 0.7, n),\n",
    "    'home_goals_avg': np.random.uniform(2.5, 3.8, n),\n",
    "    'away_goals_avg': np.random.uniform(2.5, 3.8, n),\n",
    "    'home_goals_against_avg': np.random.uniform(2.2, 3.5, n),\n",
    "    'away_goals_against_avg': np.random.uniform(2.2, 3.5, n),\n",
    "    'home_pp_pct': np.random.uniform(0.15, 0.28, n),\n",
    "    'away_pp_pct': np.random.uniform(0.15, 0.28, n),\n",
    "    'home_rest_days': np.random.randint(1, 5, n),\n",
    "    'away_rest_days': np.random.randint(1, 5, n),\n",
    "})\n",
    "\n",
    "# Create realistic target with known relationships\n",
    "y_home = (\n",
    "    X_data['home_goals_avg'] * 0.4 +\n",
    "    (4 - X_data['away_goals_against_avg']) * 0.3 +\n",
    "    X_data['home_pp_pct'] * 5 +\n",
    "    0.3 +  # home advantage\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "y_away = (\n",
    "    X_data['away_goals_avg'] * 0.4 +\n",
    "    (4 - X_data['home_goals_against_avg']) * 0.3 +\n",
    "    X_data['away_pp_pct'] * 5 +\n",
    "    np.random.normal(0, 0.5, n)\n",
    ").clip(0, 8).round().astype(int)\n",
    "\n",
    "# Train/test split\n",
    "split_idx = int(n * 0.8)\n",
    "X_train, X_val = X_data.iloc[:split_idx], X_data.iloc[split_idx:]\n",
    "y_home_train, y_home_val = y_home[:split_idx], y_home[split_idx:]\n",
    "y_away_train, y_away_val = y_away[:split_idx], y_away[split_idx:]\n",
    "\n",
    "# Scale features (CRITICAL for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}\")\n",
    "print(f\"Home goals range: {y_home.min()} - {y_home.max()}\")\n",
    "print(f\"Scaled mean: {X_train_scaled.mean():.4f}, std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38fe15b",
   "metadata": {},
   "source": [
    "## Test 1: Basic Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1a: Model creation\n",
    "try:\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )\n",
    "    record_test(\"1a. Model creation\", True)\n",
    "except Exception as e:\n",
    "    record_test(\"1a. Model creation\", False, str(e))\n",
    "\n",
    "# Test 1b: Fit\n",
    "try:\n",
    "    model.fit(X_train_scaled, y_home_train)\n",
    "    record_test(\"1b. Model fit\", True, f\"Iterations: {model.n_iter_}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1b. Model fit\", False, str(e))\n",
    "\n",
    "# Test 1c: Predict\n",
    "try:\n",
    "    predictions = model.predict(X_val_scaled)\n",
    "    valid = len(predictions) == len(y_home_val) and not np.isnan(predictions).any()\n",
    "    record_test(\"1c. Model predict\", valid, f\"n_predictions={len(predictions)}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1c. Model predict\", False, str(e))\n",
    "\n",
    "# Test 1d: Evaluate\n",
    "try:\n",
    "    rmse = np.sqrt(mean_squared_error(y_home_val, predictions))\n",
    "    mae = mean_absolute_error(y_home_val, predictions)\n",
    "    r2 = r2_score(y_home_val, predictions)\n",
    "    record_test(\"1d. Model evaluate\", True, f\"RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"1d. Model evaluate\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704fd2a",
   "metadata": {},
   "source": [
    "## Test 2: Architecture Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bac527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different architectures\n",
    "architectures = [\n",
    "    (50,),           # Single layer\n",
    "    (100, 50),       # Two layers\n",
    "    (100, 50, 25),   # Three layers\n",
    "    (200,),          # Wide single layer\n",
    "]\n",
    "\n",
    "arch_results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    try:\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes=arch,\n",
    "            max_iter=200,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_home_train)\n",
    "        val_pred = model.predict(X_val_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_home_val, val_pred))\n",
    "        arch_results.append({'arch': str(arch), 'rmse': rmse, 'n_iter': model.n_iter_})\n",
    "    except Exception as e:\n",
    "        arch_results.append({'arch': str(arch), 'rmse': None, 'error': str(e)})\n",
    "\n",
    "arch_df = pd.DataFrame(arch_results)\n",
    "print(\"Architecture Comparison:\")\n",
    "print(arch_df.to_string(index=False))\n",
    "\n",
    "# All architectures should train successfully\n",
    "all_trained = all(r.get('rmse') is not None for r in arch_results)\n",
    "record_test(\"2a. All architectures train\", all_trained)\n",
    "\n",
    "# Performance should be reasonably similar (within 50%)\n",
    "rmses = [r['rmse'] for r in arch_results if r['rmse'] is not None]\n",
    "if rmses:\n",
    "    rmse_range = max(rmses) / min(rmses)\n",
    "    record_test(\"2b. Architecture performance variation\", rmse_range < 2.0,\n",
    "               f\"Range: {min(rmses):.4f} - {max(rmses):.4f} (ratio: {rmse_range:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472c10b",
   "metadata": {},
   "source": [
    "## Test 3: Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26bca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['relu', 'tanh', 'logistic', 'identity']\n",
    "activation_results = []\n",
    "\n",
    "for act in activations:\n",
    "    try:\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes=(100, 50),\n",
    "            activation=act,\n",
    "            max_iter=200,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_home_train)\n",
    "        val_pred = model.predict(X_val_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_home_val, val_pred))\n",
    "        activation_results.append({'activation': act, 'rmse': rmse})\n",
    "    except Exception as e:\n",
    "        activation_results.append({'activation': act, 'rmse': None, 'error': str(e)})\n",
    "\n",
    "act_df = pd.DataFrame(activation_results)\n",
    "print(\"Activation Function Comparison:\")\n",
    "print(act_df.to_string(index=False))\n",
    "\n",
    "all_trained = all(r.get('rmse') is not None for r in activation_results)\n",
    "record_test(\"3a. All activations train\", all_trained)\n",
    "\n",
    "# ReLU should be among the best\n",
    "relu_rmse = next(r['rmse'] for r in activation_results if r['activation'] == 'relu')\n",
    "best_rmse = min(r['rmse'] for r in activation_results if r['rmse'] is not None)\n",
    "relu_competitive = relu_rmse <= best_rmse * 1.2\n",
    "record_test(\"3b. ReLU is competitive\", relu_competitive,\n",
    "           f\"ReLU: {relu_rmse:.4f}, Best: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0bcfa",
   "metadata": {},
   "source": [
    "## Test 4: Regularization (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "alpha_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    try:\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes=(100, 50),\n",
    "            alpha=alpha,\n",
    "            max_iter=200,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_home_train)\n",
    "        \n",
    "        train_pred = model.predict(X_train_scaled)\n",
    "        val_pred = model.predict(X_val_scaled)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_home_train, train_pred))\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_home_val, val_pred))\n",
    "        \n",
    "        alpha_results.append({\n",
    "            'alpha': alpha,\n",
    "            'train_rmse': train_rmse,\n",
    "            'val_rmse': val_rmse,\n",
    "            'overfit': train_rmse / val_rmse if val_rmse > 0 else 0\n",
    "        })\n",
    "    except Exception as e:\n",
    "        alpha_results.append({'alpha': alpha, 'error': str(e)})\n",
    "\n",
    "alpha_df = pd.DataFrame(alpha_results)\n",
    "print(\"Alpha (L2 Regularization) Comparison:\")\n",
    "print(alpha_df.to_string(index=False))\n",
    "\n",
    "# Higher alpha should increase training error (less overfitting)\n",
    "try:\n",
    "    low_alpha = next(r for r in alpha_results if r['alpha'] == 0.0001)\n",
    "    high_alpha = next(r for r in alpha_results if r['alpha'] == 1.0)\n",
    "    \n",
    "    more_regularized = high_alpha['train_rmse'] >= low_alpha['train_rmse']\n",
    "    record_test(\"4a. Alpha increases training error\", more_regularized,\n",
    "               f\"α=0.0001: {low_alpha['train_rmse']:.4f}, α=1.0: {high_alpha['train_rmse']:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"4a. Alpha increases training error\", False, str(e))\n",
    "\n",
    "# Moderate alpha should have best validation performance\n",
    "best_val_alpha = min(alpha_results, key=lambda x: x.get('val_rmse', float('inf')))\n",
    "record_test(\"4b. Best validation alpha\", True,\n",
    "           f\"Best α={best_val_alpha['alpha']}: val RMSE={best_val_alpha['val_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47c23f",
   "metadata": {},
   "source": [
    "## Test 5: Learning Rate Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    try:\n",
    "        model = MLPRegressor(\n",
    "            hidden_layer_sizes=(100, 50),\n",
    "            learning_rate_init=lr,\n",
    "            max_iter=200,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train_scaled, y_home_train)\n",
    "        val_pred = model.predict(X_val_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_home_val, val_pred))\n",
    "        \n",
    "        lr_results.append({\n",
    "            'learning_rate': lr,\n",
    "            'rmse': rmse,\n",
    "            'n_iter': model.n_iter_,\n",
    "            'final_loss': model.loss_\n",
    "        })\n",
    "    except Exception as e:\n",
    "        lr_results.append({'learning_rate': lr, 'error': str(e)})\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"Learning Rate Comparison:\")\n",
    "print(lr_df.to_string(index=False))\n",
    "\n",
    "# All should train\n",
    "all_trained = all(r.get('rmse') is not None for r in lr_results)\n",
    "record_test(\"5a. All learning rates train\", all_trained)\n",
    "\n",
    "# Higher LR should converge faster (fewer iterations) or have lower loss\n",
    "try:\n",
    "    low_lr = next(r for r in lr_results if r['learning_rate'] == 0.0001)\n",
    "    high_lr = next(r for r in lr_results if r['learning_rate'] == 0.01)\n",
    "    \n",
    "    # Higher LR should either converge faster or have similar loss\n",
    "    faster = high_lr['n_iter'] <= low_lr['n_iter'] or high_lr['final_loss'] <= low_lr['final_loss'] * 1.5\n",
    "    record_test(\"5b. Higher LR converges faster or similar\", faster,\n",
    "               f\"LR=0.0001: {low_lr['n_iter']} iters, LR=0.01: {high_lr['n_iter']} iters\")\n",
    "except Exception as e:\n",
    "    record_test(\"5b. Higher LR converges faster\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863a818",
   "metadata": {},
   "source": [
    "## Test 6: Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a42a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Model with early stopping\n",
    "    model_es = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model_es.fit(X_train_scaled, y_home_train)\n",
    "    \n",
    "    # Model without early stopping\n",
    "    model_no_es = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        early_stopping=False,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model_no_es.fit(X_train_scaled, y_home_train)\n",
    "    \n",
    "    record_test(\"6a. Early stopping trains\", model_es.n_iter_ > 0,\n",
    "               f\"Stopped at iteration {model_es.n_iter_}\")\n",
    "    \n",
    "    # Early stopping should stop before max_iter (usually)\n",
    "    stopped_early = model_es.n_iter_ < 500\n",
    "    record_test(\"6b. Early stopping before max_iter\", stopped_early,\n",
    "               f\"ES: {model_es.n_iter_}, No ES: {model_no_es.n_iter_}\")\n",
    "    \n",
    "    # Validation score should be available\n",
    "    has_val_score = hasattr(model_es, 'validation_scores_')\n",
    "    record_test(\"6c. Validation scores tracked\", has_val_score,\n",
    "               f\"N validation scores: {len(model_es.validation_scores_) if has_val_score else 0}\")\n",
    "    \n",
    "    # Best validation score should be stored\n",
    "    has_best = hasattr(model_es, 'best_validation_score_')\n",
    "    record_test(\"6d. Best validation score stored\", has_best,\n",
    "               f\"Best: {model_es.best_validation_score_:.4f}\" if has_best else \"\")\n",
    "except Exception as e:\n",
    "    record_test(\"6. Early stopping\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3e5fa",
   "metadata": {},
   "source": [
    "## Test 7: Serialization (Save/Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b561fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train model\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_home_train)\n",
    "    original_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Save with pickle\n",
    "    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\n",
    "        pickle.dump(model, f)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    # Load\n",
    "    with open(temp_path, 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "    \n",
    "    loaded_pred = loaded_model.predict(X_val_scaled)\n",
    "    \n",
    "    # Predictions should match exactly\n",
    "    match = np.allclose(original_pred, loaded_pred)\n",
    "    record_test(\"7a. Pickle save/load predictions match\", match,\n",
    "               f\"Max diff: {np.abs(original_pred - loaded_pred).max():.6f}\")\n",
    "    \n",
    "    # Model attributes preserved\n",
    "    attrs_match = (\n",
    "        loaded_model.hidden_layer_sizes == model.hidden_layer_sizes and\n",
    "        loaded_model.n_iter_ == model.n_iter_\n",
    "    )\n",
    "    record_test(\"7b. Attributes preserved\", attrs_match,\n",
    "               f\"Layers: {loaded_model.hidden_layer_sizes}, Iterations: {loaded_model.n_iter_}\")\n",
    "    \n",
    "    # Clean up\n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "    # Save scaler too (important for neural nets)\n",
    "    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        scaler_path = f.name\n",
    "    \n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        loaded_scaler = pickle.load(f)\n",
    "    \n",
    "    scaled_match = np.allclose(\n",
    "        scaler.transform(X_val),\n",
    "        loaded_scaler.transform(X_val)\n",
    "    )\n",
    "    record_test(\"7c. Scaler save/load\", scaled_match)\n",
    "    \n",
    "    os.unlink(scaler_path)\n",
    "except Exception as e:\n",
    "    record_test(\"7. Serialization\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f12305",
   "metadata": {},
   "source": [
    "## Test 8: Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8a: Single sample prediction\n",
    "try:\n",
    "    model = MLPRegressor(hidden_layer_sizes=(50,), max_iter=200, random_state=42)\n",
    "    model.fit(X_train_scaled, y_home_train)\n",
    "    \n",
    "    single_pred = model.predict(X_val_scaled[[0]])\n",
    "    record_test(\"8a. Single sample prediction\", len(single_pred) == 1,\n",
    "               f\"Prediction: {single_pred[0]:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"8a. Single sample prediction\", False, str(e))\n",
    "\n",
    "# Test 8b: Very small network\n",
    "try:\n",
    "    tiny_model = MLPRegressor(hidden_layer_sizes=(5,), max_iter=200, random_state=42)\n",
    "    tiny_model.fit(X_train_scaled, y_home_train)\n",
    "    tiny_pred = tiny_model.predict(X_val_scaled)\n",
    "    tiny_rmse = np.sqrt(mean_squared_error(y_home_val, tiny_pred))\n",
    "    record_test(\"8b. Tiny network trains\", True, f\"RMSE: {tiny_rmse:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"8b. Tiny network trains\", False, str(e))\n",
    "\n",
    "# Test 8c: Predictions should be in reasonable range\n",
    "try:\n",
    "    all_pred = model.predict(scaler.transform(X_data))\n",
    "    min_pred, max_pred = all_pred.min(), all_pred.max()\n",
    "    \n",
    "    # Goals should be roughly in 0-10 range\n",
    "    reasonable = min_pred >= -2 and max_pred <= 12\n",
    "    record_test(\"8c. Reasonable prediction range\", reasonable,\n",
    "               f\"Range: [{min_pred:.2f}, {max_pred:.2f}]\")\n",
    "except Exception as e:\n",
    "    record_test(\"8c. Reasonable prediction range\", False, str(e))\n",
    "\n",
    "# Test 8d: Scaling matters - unscaled data should perform worse\n",
    "try:\n",
    "    model_scaled = MLPRegressor(hidden_layer_sizes=(100,), max_iter=200, random_state=42)\n",
    "    model_scaled.fit(X_train_scaled, y_home_train)\n",
    "    scaled_rmse = np.sqrt(mean_squared_error(y_home_val, model_scaled.predict(X_val_scaled)))\n",
    "    \n",
    "    model_unscaled = MLPRegressor(hidden_layer_sizes=(100,), max_iter=200, random_state=42)\n",
    "    model_unscaled.fit(X_train, y_home_train)\n",
    "    unscaled_rmse = np.sqrt(mean_squared_error(y_home_val, model_unscaled.predict(X_val)))\n",
    "    \n",
    "    scaling_helps = scaled_rmse <= unscaled_rmse\n",
    "    record_test(\"8d. Scaling improves performance\", scaling_helps,\n",
    "               f\"Scaled: {scaled_rmse:.4f}, Unscaled: {unscaled_rmse:.4f}\")\n",
    "except Exception as e:\n",
    "    record_test(\"8d. Scaling improves performance\", False, str(e))\n",
    "\n",
    "# Test 8e: Loss curve available\n",
    "try:\n",
    "    has_loss = hasattr(model, 'loss_curve_') and len(model.loss_curve_) > 0\n",
    "    record_test(\"8e. Loss curve available\", has_loss,\n",
    "               f\"Length: {len(model.loss_curve_) if has_loss else 0}\")\n",
    "except Exception as e:\n",
    "    record_test(\"8e. Loss curve available\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7e5d6",
   "metadata": {},
   "source": [
    "## Test 9: Dual Goal Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Train separate models for home and away goals\n",
    "    home_model = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )\n",
    "    away_model = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    home_model.fit(X_train_scaled, y_home_train)\n",
    "    away_model.fit(X_train_scaled, y_away_train)\n",
    "    \n",
    "    home_pred = home_model.predict(X_val_scaled)\n",
    "    away_pred = away_model.predict(X_val_scaled)\n",
    "    \n",
    "    home_rmse = np.sqrt(mean_squared_error(y_home_val, home_pred))\n",
    "    away_rmse = np.sqrt(mean_squared_error(y_away_val, away_pred))\n",
    "    \n",
    "    # Combined RMSE\n",
    "    all_pred = np.concatenate([home_pred, away_pred])\n",
    "    all_actual = np.concatenate([y_home_val, y_away_val])\n",
    "    combined_rmse = np.sqrt(mean_squared_error(all_actual, all_pred))\n",
    "    \n",
    "    record_test(\"9a. Dual model training\", True,\n",
    "               f\"Home RMSE: {home_rmse:.4f}, Away RMSE: {away_rmse:.4f}\")\n",
    "    record_test(\"9b. Combined performance\", combined_rmse < 2.0,\n",
    "               f\"Combined RMSE: {combined_rmse:.4f}\")\n",
    "    \n",
    "    # Models should have learned different weights\n",
    "    # (even with same architecture, different targets)\n",
    "    home_weights = home_model.coefs_[0].flatten()[:10]  # First 10 weights\n",
    "    away_weights = away_model.coefs_[0].flatten()[:10]\n",
    "    weights_differ = not np.allclose(home_weights, away_weights, rtol=0.1)\n",
    "    record_test(\"9c. Models learned different weights\", weights_differ)\n",
    "except Exception as e:\n",
    "    record_test(\"9. Dual goal predictor\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ba978",
   "metadata": {},
   "source": [
    "## Test 10: Cross-Validation Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6866cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale all data\n",
    "    X_all_scaled = scaler.fit_transform(X_data)\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_all_scaled, y_home, cv=kfold,\n",
    "                                scoring='neg_root_mean_squared_error')\n",
    "    \n",
    "    mean_rmse = -cv_scores.mean()\n",
    "    std_rmse = cv_scores.std()\n",
    "    \n",
    "    cv_ratio = std_rmse / mean_rmse\n",
    "    is_stable = cv_ratio < 0.3\n",
    "    \n",
    "    record_test(\"10. Cross-validation stability\", is_stable,\n",
    "               f\"RMSE: {mean_rmse:.4f} (+/- {std_rmse:.4f}), CV ratio: {cv_ratio:.2%}\")\n",
    "except Exception as e:\n",
    "    record_test(\"10. Cross-validation stability\", False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b334900",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" NEURAL NETWORK VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame(test_results)\n",
    "passed = results_df['passed'].sum()\n",
    "total = len(results_df)\n",
    "\n",
    "print(f\"\\nPassed: {passed}/{total} ({passed/total*100:.1f}%)\")\n",
    "\n",
    "if passed < total:\n",
    "    print(\"\\n❌ FAILED TESTS:\")\n",
    "    for _, row in results_df[~results_df['passed']].iterrows():\n",
    "        print(f\"   - {row['test']}: {row['message']}\")\n",
    "else:\n",
    "    print(\"\\n✅ All tests passed!\")\n",
    "\n",
    "# Show all results\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Elo Validation\n",
        "\n",
        "Tests for BaselineEloModel: synthetic data, sanity checks, fit/predict/evaluate.\n",
        "\n",
        "Run all cells. All tests should print PASS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, sys, pathlib\n",
        "_cwd = pathlib.Path('.').resolve()\n",
        "if _cwd.name == 'validation':\n",
        "    _python_dir = _cwd.parent\n",
        "elif (_cwd / 'python').is_dir():\n",
        "    _python_dir = _cwd / 'python'\n",
        "else:\n",
        "    _python_dir = _cwd\n",
        "os.chdir(_python_dir)\n",
        "sys.path.insert(0, str(_python_dir))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils.baseline_elo import BaselineEloModel\n",
        "\n",
        "passed = 0\n",
        "failed = 0\n",
        "\n",
        "def check(name, condition):\n",
        "    global passed, failed\n",
        "    if condition:\n",
        "        print(f'  PASS: {name}')\n",
        "        passed += 1\n",
        "    else:\n",
        "        print(f'  FAIL: {name}')\n",
        "        failed += 1\n",
        "\n",
        "print('BaselineEloModel loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Dominant team (synthetic)\n",
        "\n",
        "Team A always beats Team B. Expected: A rating rises, B falls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "synthetic = pd.DataFrame({\n",
        "    'home_team': ['Team_A'] * 10 + ['Team_B'] * 10,\n",
        "    'away_team': ['Team_B'] * 10 + ['Team_A'] * 10,\n",
        "    'home_goals': [4, 3, 5, 2, 4, 3, 5, 4, 3, 6] + [1, 2, 0, 1, 2, 1, 0, 2, 1, 0],\n",
        "    'away_goals': [1, 2, 0, 1, 2, 1, 0, 2, 1, 0] + [4, 3, 5, 2, 4, 3, 5, 4, 3, 6],\n",
        "})\n",
        "\n",
        "model = BaselineEloModel({'k_factor': 32, 'initial_rating': 1200})\n",
        "model.fit(synthetic)\n",
        "\n",
        "rA = model.elo.ratings['Team_A']\n",
        "rB = model.elo.ratings['Team_B']\n",
        "check('Dominant team A > 1300', rA > 1300)\n",
        "check('Losing team B < 1100', rB < 1100)\n",
        "check('A > B', rA > rB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Equal teams (50/50)\n",
        "\n",
        "Alternating wins. Ratings should stay near initial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "equal = pd.DataFrame({\n",
        "    'home_team': ['C'] * 10 + ['D'] * 10,\n",
        "    'away_team': ['D'] * 10 + ['C'] * 10,\n",
        "    'home_goals': [3, 2, 4, 2, 3, 2, 3, 2, 4, 3] + [1, 2, 0, 2, 1, 2, 1, 2, 0, 1],\n",
        "    'away_goals': [1, 2, 0, 2, 1, 2, 1, 2, 0, 1] + [3, 2, 4, 2, 3, 2, 3, 2, 4, 3],\n",
        "})\n",
        "\n",
        "m2 = BaselineEloModel({'k_factor': 20, 'initial_rating': 1200})\n",
        "m2.fit(equal)\n",
        "\n",
        "rC = m2.elo.ratings['C']\n",
        "rD = m2.elo.ratings['D']\n",
        "check('Equal teams near 1200', 1100 < rC < 1300 and 1100 < rD < 1300)\n",
        "check('Equal teams close', abs(rC - rD) < 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Sanity (finite predictions, fit required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "m3 = BaselineEloModel({'k_factor': 5, 'initial_rating': 1200})\n",
        "m3.fit(synthetic)\n",
        "\n",
        "h, a = m3.predict_goals({'home_team': 'Team_A', 'away_team': 'Team_B'})\n",
        "check('predict_goals finite', np.isfinite(h) and np.isfinite(a))\n",
        "check('predict_goals non-negative', h >= 0 and a >= 0)\n",
        "check('predict_goals sum reasonable', 1 < h + a < 10)\n",
        "\n",
        "winner, conf = m3.predict_winner({'home_team': 'Team_A', 'away_team': 'Team_B'})\n",
        "check('predict_winner returns tuple', isinstance(winner, str) and isinstance(conf, (int, float)))\n",
        "check('confidence in [0,1]', 0 <= conf <= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: evaluate() and get_rankings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "m4 = BaselineEloModel({'k_factor': 32, 'initial_rating': 1200})\n",
        "m4.fit(synthetic)\n",
        "\n",
        "metrics = m4.evaluate(synthetic)\n",
        "check('evaluate returns dict', isinstance(metrics, dict))\n",
        "check('evaluate has combined_rmse', 'combined_rmse' in metrics)\n",
        "check('evaluate has win_accuracy', 'win_accuracy' in metrics)\n",
        "check('RMSE finite', np.isfinite(metrics['combined_rmse']))\n",
        "check('win_accuracy in [0,1]', 0 <= metrics['win_accuracy'] <= 1)\n",
        "\n",
        "ranks = m4.get_rankings()\n",
        "check('get_rankings returns list', isinstance(ranks, list))\n",
        "check('get_rankings (team, rating) tuples', all(len(x) == 2 and isinstance(x[0], str) and isinstance(x[1], (int, float)) for x in ranks))\n",
        "check('get_rankings sorted', ranks == sorted(ranks, key=lambda x: x[1], reverse=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Unknown team (uses base rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 6: Edge cases (ties, empty, single game, formula constants)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single game\n",
        "m6a = BaselineEloModel({'k_factor': 32, 'initial_rating': 1200})\n",
        "m6a.fit(pd.DataFrame([{'home_team': 'A', 'away_team': 'B', 'home_goals': 4, 'away_goals': 1}]))\n",
        "rA, rB = m6a.elo.ratings['A'], m6a.elo.ratings['B']\n",
        "check('Single game: winner higher', rA > rB)\n",
        "\n",
        "# Empty DataFrame - should not crash\n",
        "m6b = BaselineEloModel({'k_factor': 5, 'initial_rating': 1200})\n",
        "m6b.fit(pd.DataFrame(columns=['home_team', 'away_team', 'home_goals', 'away_goals']))\n",
        "h, a = m6b.predict_goals({'home_team': 'X', 'away_team': 'Y'})\n",
        "check('Empty fit: predict ~3-3', abs(h - 3) < 1 and abs(a - 3) < 1)\n",
        "\n",
        "# Formula constants - goals clipped non-negative\n",
        "m6c = BaselineEloModel({'k_factor': 32, 'league_avg_goals': 2, 'goal_diff_half_range': 10})\n",
        "df_weak = pd.DataFrame({'home_team': ['Weak']*5, 'away_team': ['Strong']*5, 'home_goals': [0]*5, 'away_goals': [5]*5})\n",
        "m6c.fit(df_weak)\n",
        "h, a = m6c.predict_goals({'home_team': 'Weak', 'away_team': 'Strong'})\n",
        "check('Goals non-negative (extreme)', h >= 0 and a >= 0)\n",
        "\n",
        "# elo_scale=0 fallback\n",
        "m6d = BaselineEloModel({'k_factor': 32, 'elo_scale': 0})\n",
        "m6d.fit(synthetic)\n",
        "h, a = m6d.predict_goals({'home_team': 'Team_A', 'away_team': 'Team_B'})\n",
        "check('elo_scale=0 returns 50/50', abs(h - 3) < 0.5 and abs(a - 3) < 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "m5 = BaselineEloModel({'k_factor': 32, 'initial_rating': 1200})\n",
        "m5.fit(synthetic)\n",
        "\n",
        "winner, conf = m5.predict_winner({'home_team': 'Unknown_Team', 'away_team': 'Team_A'})\n",
        "check('Unknown team handled', winner in ('Unknown_Team', 'Team_A'))\n",
        "check('Confidence valid', 0 <= conf <= 1)\n",
        "\n",
        "h, a = m5.predict_goals({'home_team': 'Unknown_Team', 'away_team': 'Unknown_Team2'})\n",
        "check('Two unknowns predict ~3-3', abs(h - 3) < 1 and abs(a - 3) < 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f'\\nTotal: {passed} passed, {failed} failed')\n",
        "if failed == 0:\n",
        "    print('[OK] All Baseline Elo validation tests passed')\n",
        "else:\n",
        "    print('[FAIL] Some tests failed')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}